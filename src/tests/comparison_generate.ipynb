{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name : sdf_chd8bar\n",
      "batch_size : 16\n",
      "max_epoch : 100\n",
      "learning_rate : 5e-05\n",
      "max_grad_norm : 10\n",
      "fp16 : True\n",
      "num_workers : 4\n",
      "pin_memory : True\n",
      "in_channels : 2\n",
      "out_channels : 2\n",
      "channels : 64\n",
      "attention_levels : [2, 3]\n",
      "n_res_blocks : 2\n",
      "channel_multipliers : [1, 2, 4, 4]\n",
      "n_heads : 4\n",
      "tf_layers : 1\n",
      "d_cond : 512\n",
      "linear_start : 0.00085\n",
      "linear_end : 0.012\n",
      "n_steps : 1000\n",
      "latent_scaling_factor : 0.18215\n",
      "img_h : 128\n",
      "img_w : 128\n",
      "cond_type : chord\n",
      "cond_mode : mix\n",
      "use_enc : True\n",
      "chd_n_step : 32\n",
      "chd_input_dim : 36\n",
      "chd_z_input_dim : 512\n",
      "chd_hidden_dim : 512\n",
      "chd_z_dim : 512\n",
      "---------------loading polyffusion weights-------------------\n",
      "input_blocks.10.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.10.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.10.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.10.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.10.2.attention.layers.0.linear1.weight\n",
      "input_blocks.10.2.attention.layers.0.linear1.bias\n",
      "input_blocks.10.2.attention.layers.0.linear2.weight\n",
      "input_blocks.10.2.attention.layers.0.linear2.bias\n",
      "input_blocks.10.2.attention.layers.0.norm1.weight\n",
      "input_blocks.10.2.attention.layers.0.norm1.bias\n",
      "input_blocks.10.2.attention.layers.0.norm2.weight\n",
      "input_blocks.10.2.attention.layers.0.norm2.bias\n",
      "input_blocks.10.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.10.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.10.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.10.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.10.2.attention.layers.1.linear1.weight\n",
      "input_blocks.10.2.attention.layers.1.linear1.bias\n",
      "input_blocks.10.2.attention.layers.1.linear2.weight\n",
      "input_blocks.10.2.attention.layers.1.linear2.bias\n",
      "input_blocks.10.2.attention.layers.1.norm1.weight\n",
      "input_blocks.10.2.attention.layers.1.norm1.bias\n",
      "input_blocks.10.2.attention.layers.1.norm2.weight\n",
      "input_blocks.10.2.attention.layers.1.norm2.bias\n",
      "input_blocks.11.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.11.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.11.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.11.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.11.2.attention.layers.0.linear1.weight\n",
      "input_blocks.11.2.attention.layers.0.linear1.bias\n",
      "input_blocks.11.2.attention.layers.0.linear2.weight\n",
      "input_blocks.11.2.attention.layers.0.linear2.bias\n",
      "input_blocks.11.2.attention.layers.0.norm1.weight\n",
      "input_blocks.11.2.attention.layers.0.norm1.bias\n",
      "input_blocks.11.2.attention.layers.0.norm2.weight\n",
      "input_blocks.11.2.attention.layers.0.norm2.bias\n",
      "input_blocks.11.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.11.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.11.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.11.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.11.2.attention.layers.1.linear1.weight\n",
      "input_blocks.11.2.attention.layers.1.linear1.bias\n",
      "input_blocks.11.2.attention.layers.1.linear2.weight\n",
      "input_blocks.11.2.attention.layers.1.linear2.bias\n",
      "input_blocks.11.2.attention.layers.1.norm1.weight\n",
      "input_blocks.11.2.attention.layers.1.norm1.bias\n",
      "input_blocks.11.2.attention.layers.1.norm2.weight\n",
      "input_blocks.11.2.attention.layers.1.norm2.bias\n",
      "output_blocks.0.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.0.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.0.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.0.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.0.2.attention.layers.0.linear1.weight\n",
      "output_blocks.0.2.attention.layers.0.linear1.bias\n",
      "output_blocks.0.2.attention.layers.0.linear2.weight\n",
      "output_blocks.0.2.attention.layers.0.linear2.bias\n",
      "output_blocks.0.2.attention.layers.0.norm1.weight\n",
      "output_blocks.0.2.attention.layers.0.norm1.bias\n",
      "output_blocks.0.2.attention.layers.0.norm2.weight\n",
      "output_blocks.0.2.attention.layers.0.norm2.bias\n",
      "output_blocks.0.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.0.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.0.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.0.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.0.2.attention.layers.1.linear1.weight\n",
      "output_blocks.0.2.attention.layers.1.linear1.bias\n",
      "output_blocks.0.2.attention.layers.1.linear2.weight\n",
      "output_blocks.0.2.attention.layers.1.linear2.bias\n",
      "output_blocks.0.2.attention.layers.1.norm1.weight\n",
      "output_blocks.0.2.attention.layers.1.norm1.bias\n",
      "output_blocks.0.2.attention.layers.1.norm2.weight\n",
      "output_blocks.0.2.attention.layers.1.norm2.bias\n",
      "output_blocks.1.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.1.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.1.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.1.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.1.2.attention.layers.0.linear1.weight\n",
      "output_blocks.1.2.attention.layers.0.linear1.bias\n",
      "output_blocks.1.2.attention.layers.0.linear2.weight\n",
      "output_blocks.1.2.attention.layers.0.linear2.bias\n",
      "output_blocks.1.2.attention.layers.0.norm1.weight\n",
      "output_blocks.1.2.attention.layers.0.norm1.bias\n",
      "output_blocks.1.2.attention.layers.0.norm2.weight\n",
      "output_blocks.1.2.attention.layers.0.norm2.bias\n",
      "output_blocks.1.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.1.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.1.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.1.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.1.2.attention.layers.1.linear1.weight\n",
      "output_blocks.1.2.attention.layers.1.linear1.bias\n",
      "output_blocks.1.2.attention.layers.1.linear2.weight\n",
      "output_blocks.1.2.attention.layers.1.linear2.bias\n",
      "output_blocks.1.2.attention.layers.1.norm1.weight\n",
      "output_blocks.1.2.attention.layers.1.norm1.bias\n",
      "output_blocks.1.2.attention.layers.1.norm2.weight\n",
      "output_blocks.1.2.attention.layers.1.norm2.bias\n",
      "output_blocks.2.3.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.2.3.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.2.3.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.2.3.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.2.3.attention.layers.0.linear1.weight\n",
      "output_blocks.2.3.attention.layers.0.linear1.bias\n",
      "output_blocks.2.3.attention.layers.0.linear2.weight\n",
      "output_blocks.2.3.attention.layers.0.linear2.bias\n",
      "output_blocks.2.3.attention.layers.0.norm1.weight\n",
      "output_blocks.2.3.attention.layers.0.norm1.bias\n",
      "output_blocks.2.3.attention.layers.0.norm2.weight\n",
      "output_blocks.2.3.attention.layers.0.norm2.bias\n",
      "output_blocks.2.3.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.2.3.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.2.3.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.2.3.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.2.3.attention.layers.1.linear1.weight\n",
      "output_blocks.2.3.attention.layers.1.linear1.bias\n",
      "output_blocks.2.3.attention.layers.1.linear2.weight\n",
      "output_blocks.2.3.attention.layers.1.linear2.bias\n",
      "output_blocks.2.3.attention.layers.1.norm1.weight\n",
      "output_blocks.2.3.attention.layers.1.norm1.bias\n",
      "output_blocks.2.3.attention.layers.1.norm2.weight\n",
      "output_blocks.2.3.attention.layers.1.norm2.bias\n",
      "---------------polyffusion weights loaded with the above missing keys-------------------\n",
      "It is expected that missing keys are all intertrack attention modules\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "MULTIPOLY_FOLDER = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "POLYFFUSION_CKPT_PATH = os.path.join(MULTIPOLY_FOLDER, r\"polyffusion_ckpts\\ldm_chd8bar\\sdf+pop909wm_mix16_chd8bar\\01-11_102022\\chkpts\\weights_best.pt\")\n",
    "POLYFFUSION_PARAMS_PATH = os.path.join(MULTIPOLY_FOLDER, r\"polyffusion_ckpts\\ldm_chd8bar\\sdf+pop909wm_mix16_chd8bar\\01-11_102022\\params.yaml\")\n",
    "CHORD_CKPT_PATH = os.path.join(MULTIPOLY_FOLDER, r\"pretrained\\chd8bar\\weights.pt\")\n",
    "\n",
    "with open(POLYFFUSION_PARAMS_PATH, 'r') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "for key,value in params.items():\n",
    "    print(key,\":\",value)\n",
    "\n",
    "\n",
    "polyffusion_checkpoint = torch.load(POLYFFUSION_CKPT_PATH)[\"model\"]\n",
    "chord_checkpoint = torch.load(CHORD_CKPT_PATH)[\"model\"]\n",
    "\n",
    "sys.path.append(MULTIPOLY_FOLDER)\n",
    "# Define models according to the settings in `polyffusion_ckpts\\...\\params.yaml`\n",
    "from polyffusion.dl_modules import ChordEncoder, ChordDecoder\n",
    "from polyffusion.stable_diffusion.model.unet import UNetModel as PolyffusionUNet\n",
    "from src.models.unet import UNetModel as MultipolyUNet\n",
    "\n",
    "import inspect\n",
    "\n",
    "chord_enc_params = inspect.signature(ChordEncoder.__init__).parameters\n",
    "chord_enc_params_dict = {key.removeprefix(\"chd_\"):params[key] for key in params if key.removeprefix(\"chd_\") in chord_enc_params}\n",
    "chord_encoder = ChordEncoder(**chord_enc_params_dict)\n",
    "CHORD_ENC_PREFIX = \"chord_enc.\"\n",
    "chord_enc_state_dict = {key.removeprefix(CHORD_ENC_PREFIX):value for key,value in chord_checkpoint.items() if key.startswith(CHORD_ENC_PREFIX)}\n",
    "chord_encoder.load_state_dict(chord_enc_state_dict)\n",
    "\n",
    "chord_dec_params = inspect.signature(ChordDecoder.__init__).parameters\n",
    "chord_dec_params_dict = {key.removeprefix(\"chd_\"):params[key] for key in params if key.removeprefix(\"chd_\") in chord_dec_params}\n",
    "chord_decoder = ChordDecoder(**chord_dec_params_dict)\n",
    "CHORD_DEC_PREFIX = \"chord_dec.\"\n",
    "chord_dec_state_dict = {key.removeprefix(CHORD_DEC_PREFIX):value for key,value in chord_checkpoint.items() if key.startswith(CHORD_DEC_PREFIX)}\n",
    "chord_decoder.load_state_dict(chord_dec_state_dict)\n",
    "\n",
    "\n",
    "\n",
    "polyffusion_unet_params = inspect.signature(PolyffusionUNet.__init__).parameters\n",
    "polyffusion_unet_params_dict = {key:params[key] for key in params if key in polyffusion_unet_params}\n",
    "polyffusion_unet = PolyffusionUNet(**polyffusion_unet_params_dict)\n",
    "UNET_PREFIX = \"ldm.eps_model.\"\n",
    "polyffusion_unet_state_dict = {key.removeprefix(UNET_PREFIX):value for key,value in polyffusion_checkpoint.items() if key.startswith(UNET_PREFIX)}\n",
    "polyffusion_unet.load_state_dict(polyffusion_unet_state_dict)\n",
    "\n",
    "multipoly_unet_params = inspect.signature(MultipolyUNet.__init__).parameters\n",
    "multipoly_unet_params_dict = {key:params[key] for key in params if key in multipoly_unet_params}\n",
    "multipoly_unet_params_dict[\"n_intertrack_head\"] = 4\n",
    "multipoly_unet_params_dict[\"num_intertrack_encoder_layers\"] = 2\n",
    "multipoly_unet_params_dict[\"intertrack_attention_levels\"] = [3]\n",
    "multipoly_unet = MultipolyUNet(**multipoly_unet_params_dict)\n",
    "\n",
    "multipoly_unet.load_polyffusion_checkpoints(polyffusion_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gather(consts: torch.Tensor, t: torch.Tensor):\n",
    "    \"\"\"Gather consts for $t$ and reshape to feature map shape\"\"\"\n",
    "    c = consts.gather(-1, t)\n",
    "    return c.reshape(-1, 1, 1, 1)\n",
    "\n",
    "\n",
    "class Diffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet_model: PolyffusionUNet,\n",
    "        n_steps: int,\n",
    "        linear_start: float,\n",
    "        linear_end: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.eps_model = unet_model\n",
    "        self.n_steps = n_steps\n",
    "        beta = (\n",
    "            torch.linspace(\n",
    "                linear_start**0.5, linear_end**0.5, n_steps, dtype=torch.float64\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "        alpha = 1.0 - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "        self.alpha = nn.Parameter(alpha.to(torch.float32), requires_grad=False)\n",
    "        self.beta = nn.Parameter(beta.to(torch.float32), requires_grad=False)\n",
    "        self.alpha_bar = nn.Parameter(alpha_bar.to(torch.float32), requires_grad=False)\n",
    "        self.sigma2 = self.beta\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\"\n",
    "        ### Get model device\n",
    "        \"\"\"\n",
    "        return next(iter(self.eps_model.parameters())).device\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, context: torch.Tensor):\n",
    "        return self.eps_model(x, t, context)\n",
    "\n",
    "\n",
    "    def p_sample(self, xt: torch.Tensor, t: torch.Tensor):\n",
    "        eps_theta = self.eps_model(xt, t)\n",
    "        alpha_bar = gather(self.alpha_bar, t)\n",
    "        alpha = gather(self.alpha, t)\n",
    "        eps_coef = (1 - alpha) / (1 - alpha_bar) ** 0.5\n",
    "        mean = 1 / (alpha**0.5) * (xt - eps_coef * eps_theta)\n",
    "        var = gather(self.sigma2, t)\n",
    "        eps = torch.randn(xt.shape, device=xt.device)\n",
    "        return mean + (var**0.5) * eps\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "from labml import monit\n",
    "\n",
    "class DiffusionSampler:\n",
    "    model: Diffusion\n",
    "\n",
    "    def __init__(self, model: Diffusion):\n",
    "        \"\"\"\n",
    "        :param model: is the model to predict noise $\\epsilon_\\text{cond}(x_t, c)$\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Set the model $\\epsilon_\\text{cond}(x_t, c)$\n",
    "        self.model = model\n",
    "        # Get number of steps the model was trained with $T$\n",
    "        self.n_steps = model.n_steps        \n",
    "        # Sampling steps $1, 2, \\dots, T$\n",
    "        self.time_steps = np.asarray(list(range(self.n_steps)), dtype=np.int32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # $\\bar\\alpha_t$\n",
    "            alpha_bar = self.model.alpha_bar\n",
    "            # $\\beta_t$ schedule\n",
    "            beta = self.model.beta\n",
    "            #  $\\bar\\alpha_{t-1}$\n",
    "            alpha_bar_prev = torch.cat([alpha_bar.new_tensor([1.0]), alpha_bar[:-1]])\n",
    "\n",
    "            # $\\sqrt{\\bar\\alpha}$\n",
    "            self.sqrt_alpha_bar = alpha_bar**0.5\n",
    "            # $\\sqrt{1 - \\bar\\alpha}$\n",
    "            self.sqrt_1m_alpha_bar = (1.0 - alpha_bar) ** 0.5\n",
    "            # $\\frac{1}{\\sqrt{\\bar\\alpha_t}}$\n",
    "            self.sqrt_recip_alpha_bar = alpha_bar**-0.5\n",
    "            # $\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}$\n",
    "            self.sqrt_recip_m1_alpha_bar = (1 / alpha_bar - 1) ** 0.5\n",
    "\n",
    "            # $\\frac{1 - \\bar\\alpha_{t-1}}{1 - \\bar\\alpha_t} \\beta_t$\n",
    "            variance = beta * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar)\n",
    "            # Clamped log of $\\tilde\\beta_t$\n",
    "            self.log_var = torch.log(torch.clamp(variance, min=1e-20))\n",
    "            # $\\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}$\n",
    "            self.mean_x0_coef = beta * (alpha_bar_prev**0.5) / (1.0 - alpha_bar)\n",
    "            # $\\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}$\n",
    "            self.mean_xt_coef = (\n",
    "                (1.0 - alpha_bar_prev) * ((1 - beta) ** 0.5) / (1.0 - alpha_bar)\n",
    "            )\n",
    "\n",
    "    def get_eps(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        c: torch.Tensor,\n",
    "        *,\n",
    "        uncond_scale: float,\n",
    "        uncond_cond: Optional[torch.Tensor],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ## Get $\\epsilon(x_t, c)$\n",
    "\n",
    "        :param x: is $x_t$ of shape `[batch_size, channels, height, width]`\n",
    "        :param t: is $t$ of shape `[batch_size]`\n",
    "        :param c: is the conditional embeddings $c$ of shape `[batch_size, emb_size]`\n",
    "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
    "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
    "        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n",
    "        \"\"\"\n",
    "        # When the scale $s = 1$\n",
    "        # $$\\epsilon_\\theta(x_t, c) = \\epsilon_\\text{cond}(x_t, c)$$\n",
    "        if uncond_cond is None or uncond_scale == 1.0:\n",
    "            return self.model(x, t, c)\n",
    "        elif uncond_scale == 0.0:  # unconditional\n",
    "            return self.model(x, t, uncond_cond)\n",
    "\n",
    "        # Duplicate $x_t$ and $t$\n",
    "        x_in = torch.cat([x] * 2)\n",
    "        t_in = torch.cat([t] * 2)\n",
    "        # Concatenated $c$ and $c_u$\n",
    "        c_in = torch.cat([uncond_cond, c])\n",
    "        # Get $\\epsilon_\\text{cond}(x_t, c)$ and $\\epsilon_\\text{cond}(x_t, c_u)$\n",
    "        e_t_uncond, e_t_cond = self.model(x_in, t_in, c_in).chunk(2)\n",
    "        # Calculate\n",
    "        # $$\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$$\n",
    "        e_t = e_t_uncond + uncond_scale * (e_t_cond - e_t_uncond)\n",
    "        return e_t\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        c: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        step: int,\n",
    "        repeat_noise: bool = False,\n",
    "        temperature: float = 1.0,\n",
    "        uncond_scale: float = 1.0,\n",
    "        uncond_cond: Optional[torch.Tensor] = None,\n",
    "        \n",
    "    ):\n",
    "        \"\"\"\n",
    "        ### Sample $x_{t-1}$ from $p_\\theta(x_{t-1} | x_t)$\n",
    "\n",
    "        :param x: is $x_t$ of shape `[batch_size, channels, height, width]`\n",
    "        :param c: is the conditional embeddings $c$ of shape `[batch_size, emb_size]`\n",
    "        :param t: is $t$ of shape `[batch_size]`\n",
    "        :param step: is the step $t$ as an integer\n",
    "        :repeat_noise: specified whether the noise should be same for all samples in the batch\n",
    "        :param temperature: is the noise temperature (random noise gets multiplied by this)\n",
    "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
    "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
    "        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n",
    "        \"\"\"\n",
    "\n",
    "        # Get $\\epsilon_\\theta$\n",
    "        \n",
    "        e_t = self.get_eps(\n",
    "                x, t, c, uncond_scale=uncond_scale, uncond_cond=uncond_cond\n",
    "            )\n",
    "\n",
    "        # Get batch size\n",
    "        bs = x.shape[0]\n",
    "\n",
    "        # $\\frac{1}{\\sqrt{\\bar\\alpha_t}}$\n",
    "        sqrt_recip_alpha_bar = x.new_full(\n",
    "            (bs, 1, 1, 1), self.sqrt_recip_alpha_bar[step]\n",
    "        )\n",
    "        # $\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}$\n",
    "        sqrt_recip_m1_alpha_bar = x.new_full(\n",
    "            (bs, 1, 1, 1), self.sqrt_recip_m1_alpha_bar[step]\n",
    "        )\n",
    "\n",
    "        # Calculate $x_0$ with current $\\epsilon_\\theta$\n",
    "        #\n",
    "        # $$x_0 = \\frac{1}{\\sqrt{\\bar\\alpha_t}} x_t -  \\Big(\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}\\Big)\\epsilon_\\theta$$\n",
    "        x0 = sqrt_recip_alpha_bar * x - sqrt_recip_m1_alpha_bar * e_t\n",
    "\n",
    "        # $\\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}$\n",
    "        mean_x0_coef = x.new_full((bs, 1, 1, 1), self.mean_x0_coef[step])\n",
    "        # $\\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}$\n",
    "        mean_xt_coef = x.new_full((bs, 1, 1, 1), self.mean_xt_coef[step])\n",
    "\n",
    "        # Calculate $\\mu_t(x_t, t)$\n",
    "        #\n",
    "        # $$\\mu_t(x_t, t) = \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}x_0\n",
    "        #    + \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}x_t$$\n",
    "        mean = mean_x0_coef * x0 + mean_xt_coef * x\n",
    "        # $\\log \\tilde\\beta_t$\n",
    "        log_var = x.new_full((bs, 1, 1, 1), self.log_var[step])\n",
    "\n",
    "        # Do not add noise when $t = 1$ (final step sampling process).\n",
    "        # Note that `step` is `0` when $t = 1$)\n",
    "        if step == 0:\n",
    "            noise = 0\n",
    "        # If same noise is used for all samples in the batch\n",
    "        elif repeat_noise:\n",
    "            noise = torch.randn((1, *x.shape[1:]), device=x.device)\n",
    "        # Different noise for each sample\n",
    "        else:\n",
    "            noise = torch.randn(x.shape, device=x.device)\n",
    "\n",
    "        # Multiply noise by the temperature\n",
    "        noise = noise * temperature\n",
    "\n",
    "        # Sample from,\n",
    "        #\n",
    "        # $$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}\\big(x_{t-1}; \\mu_\\theta(x_t, t), \\tilde\\beta_t \\mathbf{I} \\big)$$\n",
    "        x_prev = mean + (0.5 * log_var).exp() * noise\n",
    "        return x_prev, x0, e_t\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def q_sample(\n",
    "        self, x0: torch.Tensor, index: int, noise: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ### Sample from $q(x_t|x_0)$\n",
    "\n",
    "        $$q(x_t|x_0) = \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)$$\n",
    "\n",
    "        :param x0: is $x_0$ of shape `[batch_size, channels, height, width]`\n",
    "        :param index: is the time step $t$ index\n",
    "        :param noise: is the noise, $\\epsilon$\n",
    "        \"\"\"\n",
    "\n",
    "        # Random noise, if noise is not specified\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0, device=x0.device)\n",
    "\n",
    "        # Sample from $\\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)$\n",
    "        return self.sqrt_alpha_bar[index] * x0 + self.sqrt_1m_alpha_bar[index] * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        shape: List[int],\n",
    "        cond: torch.Tensor,\n",
    "        repeat_noise: bool = False,\n",
    "        temperature: float = 1.0,\n",
    "        x_last: Optional[torch.Tensor] = None,\n",
    "        uncond_scale: float = 1.0,\n",
    "        uncond_cond: Optional[torch.Tensor] = None,\n",
    "        t_start: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ### Sampling Loop\n",
    "\n",
    "        :param shape: is the shape of the generated images in the\n",
    "            form `[batch_size, channels, height, width]`\n",
    "        :param cond: is the conditional embeddings $c$\n",
    "        :param temperature: is the noise temperature (random noise gets multiplied by this)\n",
    "        :param x_last: is $x_T$. If not provided random noise will be used.\n",
    "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
    "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
    "        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n",
    "        :param skip_steps: is the number of time steps to skip $t'$. We start sampling from $T - t'$.\n",
    "            And `x_last` is then $x_{T - t'}$.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get device and batch size\n",
    "        bs = shape[0]\n",
    "\n",
    "        # Get $x_T$\n",
    "        x = x_last if x_last is not None else torch.randn(shape, device=cond.device)\n",
    "\n",
    "        # Time steps to sample at $T - t', T - t' - 1, \\dots, 1$\n",
    "        time_steps = np.flip(self.time_steps)[t_start:]\n",
    "\n",
    "        # Sampling loop\n",
    "        from tqdm import tqdm\n",
    "        for step in tqdm( time_steps):\n",
    "            # Time step $t$\n",
    "            ts = x.new_full((bs,), step, dtype=torch.long)\n",
    "\n",
    "            # Sample $x_{t-1}$\n",
    "            x, pred_x0, e_t = self.p_sample(\n",
    "                x,\n",
    "                cond,\n",
    "                ts,\n",
    "                step,\n",
    "                repeat_noise=repeat_noise,\n",
    "                temperature=temperature,\n",
    "                uncond_scale=uncond_scale,\n",
    "                uncond_cond=uncond_cond,\n",
    "            )\n",
    "            \n",
    "        return x\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "diffusion = Diffusion(\n",
    "    unet_model=polyffusion_unet,n_steps=params[\"n_steps\"],linear_start=params[\"linear_start\"],linear_end=params[\"linear_end\"]\n",
    ").to(device)\n",
    "sampler = DiffusionSampler(diffusion)\n",
    "\n",
    "uncond_cond = -torch.ones([10, 1, 512]).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\19941\\anaconda3\\envs\\music\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'notes': array([[  14,   66,    2,  121,    0],\n",
      "       [  16,   47,    6,   65,    0],\n",
      "       [  16,   75,    2,  121,    0],\n",
      "       ...,\n",
      "       [1152,   58,   11,  104,    0],\n",
      "       [1153,   61,   10,   87,    0],\n",
      "       [1153,   66,   10,   73,    0]]), 'start_table': array({0: 0, 16: 1, 32: 16, 48: 28, 64: 44, 80: 61, 96: 81, 112: 100, 128: 116, 144: 134, 160: 146, 176: 158, 192: 168, 208: 197, 224: 209, 240: 222, 256: 231, 272: 261, 288: 281, 304: 300, 320: 316, 336: 345, 352: 365, 368: 384, 384: 400, 400: 422, 416: 437, 432: 454, 448: 478, 464: 493, 480: 505, 496: 517, 512: 527, 528: 556, 544: 568, 560: 581, 576: 590, 592: 620, 608: 640, 624: 659, 640: 675, 656: 704, 672: 724, 688: 743, 704: 759, 720: 781, 736: 799, 752: 823, 768: 844, 784: 870, 800: 888, 816: 912, 832: 933, 848: 948, 864: 962, 880: 978, 896: 992, 912: 1014, 928: 1028, 944: 1044, 960: 1060, 976: 1082, 992: 1100, 1008: 1124, 1024: 1147, 1040: 1165, 1056: 1179, 1072: 1195, 1088: 1211, 1104: 1230, 1120: 1250, 1136: 1267, 1152: 1287},\n",
      "      dtype=object), 'db_pos': array([   0,   16,   32,   48,   64,   80,   96,  112,  128,  144,  160,\n",
      "        176,  192,  208,  224,  240,  256,  272,  288,  304,  320,  336,\n",
      "        352,  368,  384,  400,  416,  432,  448,  464,  480,  496,  512,\n",
      "        528,  544,  560,  576,  592,  608,  624,  640,  656,  672,  688,\n",
      "        704,  720,  736,  752,  768,  784,  800,  816,  832,  848,  864,\n",
      "        880,  896,  912,  928,  944,  960,  976,  992, 1008, 1024, 1040,\n",
      "       1056, 1072, 1088, 1104, 1120, 1136, 1152]), 'db_pos_filter': array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True]), 'chord': array([[-1,  0,  0, ...,  0,  0, 10],\n",
      "       [-1,  0,  0, ...,  0,  0, 10],\n",
      "       [-1,  0,  0, ...,  0,  0, 10],\n",
      "       ...,\n",
      "       [ 6,  0,  1, ...,  1,  0,  6],\n",
      "       [ 6,  0,  1, ...,  1,  0,  6],\n",
      "       [ 6,  0,  1, ...,  1,  0,  6]])}\n",
      "prmat2c : (10, 2, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "from polyffusion.data.midi_to_data import get_data_for_single_midi\n",
    "from polyffusion.data.datasample import DataSample\n",
    "from polyffusion.inference_sdf import get_data_preprocessed\n",
    "data = get_data_for_single_midi(\"test_data/pop909.mid\",\"test_data/chord.out\")\n",
    "data_sample = DataSample(data)\n",
    "print(data)\n",
    "\n",
    "prmat2c, pnotree, chd, prmat = get_data_preprocessed(\n",
    "                    data_sample, \"cond\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 36])\n",
      "torch.Size([10, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(chd.shape)\n",
    "chord_encoder = chord_encoder.to(device)\n",
    "z = chord_encoder(chd).mean\n",
    "z = z.unsqueeze(1)\n",
    "print(z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cond = z\n",
    "\n",
    "# gen = sampler.sample([cond.shape[0],2,128,128],cond,uncond_scale=2.0,uncond_cond=uncond_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi as pm\n",
    "\n",
    "def custom_round(x):\n",
    "    if x > 0.95 and x < 1.05:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def prmat2c_to_midi_file(\n",
    "    prmat2c, fpath, is_custom_round=True\n",
    "):\n",
    "    print(f\"prmat2c : {prmat2c.shape}\")\n",
    "    midi = pm.PrettyMIDI()\n",
    "    piano_program = pm.instrument_name_to_program(\"Acoustic Grand Piano\")\n",
    "    origin = pm.Instrument(program=piano_program)\n",
    "    t = 0\n",
    "    n_step = prmat2c.shape[2]\n",
    "    t_bar = int(n_step / 8)\n",
    "    for bar_ind, bars in enumerate(prmat2c):\n",
    "        onset = bars[0]\n",
    "        sustain = bars[1]\n",
    "        for step_ind, step in enumerate(onset):\n",
    "            for key, on in enumerate(step):\n",
    "                if is_custom_round:\n",
    "                    on = int(custom_round(on))\n",
    "                else:\n",
    "                    on = int(round(on))\n",
    "                if on > 0:\n",
    "                    dur = 1\n",
    "                    while step_ind + dur < n_step:\n",
    "                        if not (int(round(sustain[step_ind + dur, key])) > 0):\n",
    "                            break\n",
    "                        dur += 1\n",
    "                    note = pm.Note(\n",
    "                        velocity=80,\n",
    "                        pitch=key,\n",
    "                        start=t + step_ind * 1 / 8,\n",
    "                        end=min(t + (step_ind + dur) * 1 / 8, t + t_bar),\n",
    "                    )\n",
    "                    \n",
    "                    origin.notes.append(note)\n",
    "        t += t_bar\n",
    "    midi.instruments.append(origin)\n",
    "    midi.write(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prmat2c = gen.cpu().numpy()\n",
    "# prmat2c_to_midi_file(prmat2c, \"exp/test1.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MultiDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet_model: MultipolyUNet,\n",
    "        n_steps: int,\n",
    "        linear_start: float,\n",
    "        linear_end: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.eps_model = unet_model\n",
    "        self.n_steps = n_steps\n",
    "        beta = (\n",
    "            torch.linspace(\n",
    "                linear_start**0.5, linear_end**0.5, n_steps, dtype=torch.float64\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "        alpha = 1.0 - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "        self.alpha = nn.Parameter(alpha.to(torch.float32), requires_grad=False)\n",
    "        self.beta = nn.Parameter(beta.to(torch.float32), requires_grad=False)\n",
    "        self.alpha_bar = nn.Parameter(alpha_bar.to(torch.float32), requires_grad=False)\n",
    "        self.sigma2 = self.beta\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\"\n",
    "        ### Get model device\n",
    "        \"\"\"\n",
    "        return next(iter(self.eps_model.parameters())).device\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, context: torch.Tensor):\n",
    "        return self.eps_model(x, t, context)\n",
    "\n",
    "\n",
    "    def p_sample(self, xt: torch.Tensor, t: torch.Tensor):\n",
    "        eps_theta = self.eps_model(xt, t)\n",
    "        alpha_bar = gather(self.alpha_bar, t)\n",
    "        alpha = gather(self.alpha, t)\n",
    "        eps_coef = (1 - alpha) / (1 - alpha_bar) ** 0.5\n",
    "        mean = 1 / (alpha**0.5) * (xt - eps_coef * eps_theta)\n",
    "        var = gather(self.sigma2, t)\n",
    "        eps = torch.randn(xt.shape, device=xt.device)\n",
    "        return mean + (var**0.5) * eps\n",
    "\n",
    "\n",
    "\n",
    "class MultiDiffusionSampler:\n",
    "    model: MultiDiffusion\n",
    "\n",
    "    def __init__(self, model: MultiDiffusion):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.n_steps = model.n_steps\n",
    "        self.time_steps = np.asarray(list(range(self.n_steps)), dtype=np.int32)\n",
    "        with torch.no_grad():\n",
    "            alpha_bar = self.model.alpha_bar\n",
    "            beta = self.model.beta\n",
    "            alpha_bar_prev = torch.cat([alpha_bar.new_tensor([1.0]), alpha_bar[:-1]])\n",
    "            self.sqrt_alpha_bar = alpha_bar**0.5\n",
    "            self.sqrt_1m_alpha_bar = (1.0 - alpha_bar) ** 0.5\n",
    "            self.sqrt_recip_alpha_bar = alpha_bar**-0.5\n",
    "            self.sqrt_recip_m1_alpha_bar = (1 / alpha_bar - 1) ** 0.5\n",
    "            variance = beta * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar)\n",
    "            self.log_var = torch.log(torch.clamp(variance, min=1e-20))\n",
    "            self.mean_x0_coef = beta * (alpha_bar_prev**0.5) / (1.0 - alpha_bar)\n",
    "            self.mean_xt_coef = (\n",
    "                (1.0 - alpha_bar_prev) * ((1 - beta) ** 0.5) / (1.0 - alpha_bar)\n",
    "            )\n",
    "\n",
    "    def get_eps(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        c: torch.Tensor,\n",
    "        *,\n",
    "        uncond_scale: float,\n",
    "        uncond_cond: Optional[torch.Tensor],\n",
    "    ):\n",
    "        if uncond_cond is None or uncond_scale == 1.0:\n",
    "            return self.model(x, t, c)\n",
    "        elif uncond_scale == 0.0:  # unconditional\n",
    "            return self.model(x, t, uncond_cond)\n",
    "\n",
    "        x_in = torch.cat([x] * 2)\n",
    "        t_in = torch.cat([t] * 2)\n",
    "        c_in = torch.cat([uncond_cond, c])\n",
    "        e_t_uncond, e_t_cond = self.model(x_in, t_in, c_in).chunk(2)\n",
    "        e_t = e_t_uncond + uncond_scale * (e_t_cond - e_t_uncond)\n",
    "        return e_t\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        c: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        step: int,\n",
    "        repeat_noise: bool = False,\n",
    "        temperature: float = 1.0,\n",
    "        uncond_scale: float = 1.0,\n",
    "        uncond_cond: Optional[torch.Tensor] = None,\n",
    "        \n",
    "    ):\n",
    "        e_t = self.get_eps(\n",
    "                x, t, c, uncond_scale=uncond_scale, uncond_cond=uncond_cond\n",
    "            )\n",
    "\n",
    "        bs,track_num = x.shape[0],x.shape[1]\n",
    "\n",
    "        sqrt_recip_alpha_bar = x.new_full(\n",
    "            (bs,track_num, 1, 1, 1), self.sqrt_recip_alpha_bar[step]\n",
    "        )\n",
    "        sqrt_recip_m1_alpha_bar = x.new_full(\n",
    "            (bs,track_num, 1, 1, 1), self.sqrt_recip_m1_alpha_bar[step]\n",
    "        )\n",
    "\n",
    "        x0 = sqrt_recip_alpha_bar * x - sqrt_recip_m1_alpha_bar * e_t\n",
    "\n",
    "        mean_x0_coef = x.new_full((bs,track_num, 1, 1, 1), self.mean_x0_coef[step])\n",
    "        mean_xt_coef = x.new_full((bs,track_num, 1, 1, 1), self.mean_xt_coef[step])\n",
    "        mean = mean_x0_coef * x0 + mean_xt_coef * x\n",
    "        log_var = x.new_full((bs,track_num, 1, 1, 1), self.log_var[step])\n",
    "        if step == 0:\n",
    "            noise = 0\n",
    "        elif repeat_noise:\n",
    "            noise = torch.randn((1, *x.shape[1:]), device=x.device)\n",
    "        else:\n",
    "            noise = torch.randn(x.shape, device=x.device)\n",
    "        noise = noise * temperature\n",
    "        x_prev = mean + (0.5 * log_var).exp() * noise\n",
    "        return x_prev, x0, e_t\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def q_sample(\n",
    "        self, x0: torch.Tensor, index: int, noise: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ### Sample from $q(x_t|x_0)$\n",
    "\n",
    "        $$q(x_t|x_0) = \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)$$\n",
    "\n",
    "        :param x0: is $x_0$ of shape `[batch_size, channels, height, width]`\n",
    "        :param index: is the time step $t$ index\n",
    "        :param noise: is the noise, $\\epsilon$\n",
    "        \"\"\"\n",
    "\n",
    "        # Random noise, if noise is not specified\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0, device=x0.device)\n",
    "\n",
    "        # Sample from $\\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)$\n",
    "        return self.sqrt_alpha_bar[index] * x0 + self.sqrt_1m_alpha_bar[index] * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        shape: List[int],\n",
    "        cond: torch.Tensor,\n",
    "        repeat_noise: bool = False,\n",
    "        temperature: float = 1.0,\n",
    "        x_last: Optional[torch.Tensor] = None,\n",
    "        uncond_scale: float = 1.0,\n",
    "        uncond_cond: Optional[torch.Tensor] = None,\n",
    "        t_start: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ### Sampling Loop\n",
    "\n",
    "        :param shape: is the shape of the generated images in the\n",
    "            form `[batch_size, channels, height, width]`\n",
    "        :param cond: is the conditional embeddings $c$\n",
    "        :param temperature: is the noise temperature (random noise gets multiplied by this)\n",
    "        :param x_last: is $x_T$. If not provided random noise will be used.\n",
    "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
    "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
    "        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n",
    "        :param skip_steps: is the number of time steps to skip $t'$. We start sampling from $T - t'$.\n",
    "            And `x_last` is then $x_{T - t'}$.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get device and batch size\n",
    "        bs, track_num = shape[0],shape[1]\n",
    "\n",
    "        # Get $x_T$\n",
    "        x = x_last if x_last is not None else torch.randn(shape, device=cond.device)\n",
    "\n",
    "        # Time steps to sample at $T - t', T - t' - 1, \\dots, 1$\n",
    "        time_steps = np.flip(self.time_steps)[t_start:]\n",
    "\n",
    "        # Sampling loop\n",
    "        from tqdm import tqdm\n",
    "        for step in tqdm( time_steps):\n",
    "            # Time step $t$\n",
    "            ts = x.new_full((bs*track_num,), step, dtype=torch.long)\n",
    "\n",
    "            # Sample $x_{t-1}$\n",
    "            x, pred_x0, e_t = self.p_sample(\n",
    "                x,\n",
    "                cond,\n",
    "                ts,\n",
    "                step,\n",
    "                repeat_noise=repeat_noise,\n",
    "                temperature=temperature,\n",
    "                uncond_scale=uncond_scale,\n",
    "                uncond_cond=uncond_cond,\n",
    "            )\n",
    "            \n",
    "        return x\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "multidiffusion = MultiDiffusion(\n",
    "    unet_model=multipoly_unet,n_steps=params[\"n_steps\"],linear_start=params[\"linear_start\"],linear_end=params[\"linear_end\"]\n",
    ").to(device)\n",
    "multisampler = MultiDiffusionSampler(multidiffusion)\n",
    "multidiffusion.eval()\n",
    "uncond_cond = -torch.ones([10*4, 1, 512]).to(device)\n",
    "multi_cond = torch.cat([cond]*4, dim=1)\n",
    "print(multi_cond.shape)\n",
    "multi_cond = multi_cond.reshape(40,1,512).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [41:18<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    gen = multisampler.sample([10, 4, 2, 128, 128], multi_cond, uncond_cond=uncond_cond, uncond_scale=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4, 2, 128, 128])\n",
      "prmat2c : (10, 2, 128, 128)\n",
      "prmat2c : (10, 2, 128, 128)\n",
      "prmat2c : (10, 2, 128, 128)\n",
      "prmat2c : (10, 2, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(gen.shape)\n",
    "for i in range(4):\n",
    "    track_gen = gen[:,i].cpu().numpy()\n",
    "    prmat2c_to_midi_file(track_gen, f\"exp/track{i}.mid\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
