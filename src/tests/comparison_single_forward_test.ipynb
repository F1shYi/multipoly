{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name : sdf_chd8bar\n",
      "batch_size : 16\n",
      "max_epoch : 100\n",
      "learning_rate : 5e-05\n",
      "max_grad_norm : 10\n",
      "fp16 : True\n",
      "num_workers : 4\n",
      "pin_memory : True\n",
      "in_channels : 2\n",
      "out_channels : 2\n",
      "channels : 64\n",
      "attention_levels : [2, 3]\n",
      "n_res_blocks : 2\n",
      "channel_multipliers : [1, 2, 4, 4]\n",
      "n_heads : 4\n",
      "tf_layers : 1\n",
      "d_cond : 512\n",
      "linear_start : 0.00085\n",
      "linear_end : 0.012\n",
      "n_steps : 1000\n",
      "latent_scaling_factor : 0.18215\n",
      "img_h : 128\n",
      "img_w : 128\n",
      "cond_type : chord\n",
      "cond_mode : mix\n",
      "use_enc : True\n",
      "chd_n_step : 32\n",
      "chd_input_dim : 36\n",
      "chd_z_input_dim : 512\n",
      "chd_hidden_dim : 512\n",
      "chd_z_dim : 512\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "MULTIPOLY_FOLDER = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "POLYFFUSION_CKPT_PATH = os.path.join(MULTIPOLY_FOLDER, r\"polyffusion_ckpts\\ldm_chd8bar\\sdf+pop909wm_mix16_chd8bar\\01-11_102022\\chkpts\\weights_best.pt\")\n",
    "POLYFFUSION_PARAMS_PATH = os.path.join(MULTIPOLY_FOLDER, r\"polyffusion_ckpts\\ldm_chd8bar\\sdf+pop909wm_mix16_chd8bar\\01-11_102022\\params.yaml\")\n",
    "CHORD_CKPT_PATH = os.path.join(MULTIPOLY_FOLDER, r\"pretrained\\chd8bar\\weights.pt\")\n",
    "\n",
    "with open(POLYFFUSION_PARAMS_PATH, 'r') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "for key,value in params.items():\n",
    "    print(key,\":\",value)\n",
    "\n",
    "\n",
    "polyffusion_checkpoint = torch.load(POLYFFUSION_CKPT_PATH)[\"model\"]\n",
    "chord_checkpoint = torch.load(CHORD_CKPT_PATH)[\"model\"]\n",
    "\n",
    "sys.path.append(MULTIPOLY_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define models according to the settings in `polyffusion_ckpts\\...\\params.yaml`\n",
    "from polyffusion.dl_modules import ChordEncoder, ChordDecoder\n",
    "from polyffusion.stable_diffusion.model.unet import UNetModel as PolyffusionUNet\n",
    "from src.models.unet import UNetModel as MultipolyUNet\n",
    "\n",
    "import inspect\n",
    "\n",
    "chord_enc_params = inspect.signature(ChordEncoder.__init__).parameters\n",
    "chord_enc_params_dict = {key.removeprefix(\"chd_\"):params[key] for key in params if key.removeprefix(\"chd_\") in chord_enc_params}\n",
    "chord_encoder = ChordEncoder(**chord_enc_params_dict)\n",
    "CHORD_ENC_PREFIX = \"chord_enc.\"\n",
    "chord_enc_state_dict = {key.removeprefix(CHORD_ENC_PREFIX):value for key,value in chord_checkpoint.items() if key.startswith(CHORD_ENC_PREFIX)}\n",
    "chord_encoder.load_state_dict(chord_enc_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chord_dec_params = inspect.signature(ChordDecoder.__init__).parameters\n",
    "chord_dec_params_dict = {key.removeprefix(\"chd_\"):params[key] for key in params if key.removeprefix(\"chd_\") in chord_dec_params}\n",
    "chord_decoder = ChordDecoder(**chord_dec_params_dict)\n",
    "CHORD_DEC_PREFIX = \"chord_dec.\"\n",
    "chord_dec_state_dict = {key.removeprefix(CHORD_DEC_PREFIX):value for key,value in chord_checkpoint.items() if key.startswith(CHORD_DEC_PREFIX)}\n",
    "chord_decoder.load_state_dict(chord_dec_state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_embed.0.weight\n",
      "time_embed.0.bias\n",
      "time_embed.2.weight\n",
      "time_embed.2.bias\n",
      "input_blocks.0.0.weight\n",
      "input_blocks.0.0.bias\n",
      "input_blocks.1.0.in_layers.0.weight\n",
      "input_blocks.1.0.in_layers.0.bias\n",
      "input_blocks.1.0.in_layers.2.weight\n",
      "input_blocks.1.0.in_layers.2.bias\n",
      "input_blocks.1.0.emb_layers.1.weight\n",
      "input_blocks.1.0.emb_layers.1.bias\n",
      "input_blocks.1.0.out_layers.0.weight\n",
      "input_blocks.1.0.out_layers.0.bias\n",
      "input_blocks.1.0.out_layers.3.weight\n",
      "input_blocks.1.0.out_layers.3.bias\n",
      "input_blocks.2.0.in_layers.0.weight\n",
      "input_blocks.2.0.in_layers.0.bias\n",
      "input_blocks.2.0.in_layers.2.weight\n",
      "input_blocks.2.0.in_layers.2.bias\n",
      "input_blocks.2.0.emb_layers.1.weight\n",
      "input_blocks.2.0.emb_layers.1.bias\n",
      "input_blocks.2.0.out_layers.0.weight\n",
      "input_blocks.2.0.out_layers.0.bias\n",
      "input_blocks.2.0.out_layers.3.weight\n",
      "input_blocks.2.0.out_layers.3.bias\n",
      "input_blocks.3.0.op.weight\n",
      "input_blocks.3.0.op.bias\n",
      "input_blocks.4.0.in_layers.0.weight\n",
      "input_blocks.4.0.in_layers.0.bias\n",
      "input_blocks.4.0.in_layers.2.weight\n",
      "input_blocks.4.0.in_layers.2.bias\n",
      "input_blocks.4.0.emb_layers.1.weight\n",
      "input_blocks.4.0.emb_layers.1.bias\n",
      "input_blocks.4.0.out_layers.0.weight\n",
      "input_blocks.4.0.out_layers.0.bias\n",
      "input_blocks.4.0.out_layers.3.weight\n",
      "input_blocks.4.0.out_layers.3.bias\n",
      "input_blocks.4.0.skip_connection.weight\n",
      "input_blocks.4.0.skip_connection.bias\n",
      "input_blocks.5.0.in_layers.0.weight\n",
      "input_blocks.5.0.in_layers.0.bias\n",
      "input_blocks.5.0.in_layers.2.weight\n",
      "input_blocks.5.0.in_layers.2.bias\n",
      "input_blocks.5.0.emb_layers.1.weight\n",
      "input_blocks.5.0.emb_layers.1.bias\n",
      "input_blocks.5.0.out_layers.0.weight\n",
      "input_blocks.5.0.out_layers.0.bias\n",
      "input_blocks.5.0.out_layers.3.weight\n",
      "input_blocks.5.0.out_layers.3.bias\n",
      "input_blocks.6.0.op.weight\n",
      "input_blocks.6.0.op.bias\n",
      "input_blocks.7.0.in_layers.0.weight\n",
      "input_blocks.7.0.in_layers.0.bias\n",
      "input_blocks.7.0.in_layers.2.weight\n",
      "input_blocks.7.0.in_layers.2.bias\n",
      "input_blocks.7.0.emb_layers.1.weight\n",
      "input_blocks.7.0.emb_layers.1.bias\n",
      "input_blocks.7.0.out_layers.0.weight\n",
      "input_blocks.7.0.out_layers.0.bias\n",
      "input_blocks.7.0.out_layers.3.weight\n",
      "input_blocks.7.0.out_layers.3.bias\n",
      "input_blocks.7.0.skip_connection.weight\n",
      "input_blocks.7.0.skip_connection.bias\n",
      "input_blocks.7.1.norm.weight\n",
      "input_blocks.7.1.norm.bias\n",
      "input_blocks.7.1.proj_in.weight\n",
      "input_blocks.7.1.proj_in.bias\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.7.1.proj_out.weight\n",
      "input_blocks.7.1.proj_out.bias\n",
      "input_blocks.8.0.in_layers.0.weight\n",
      "input_blocks.8.0.in_layers.0.bias\n",
      "input_blocks.8.0.in_layers.2.weight\n",
      "input_blocks.8.0.in_layers.2.bias\n",
      "input_blocks.8.0.emb_layers.1.weight\n",
      "input_blocks.8.0.emb_layers.1.bias\n",
      "input_blocks.8.0.out_layers.0.weight\n",
      "input_blocks.8.0.out_layers.0.bias\n",
      "input_blocks.8.0.out_layers.3.weight\n",
      "input_blocks.8.0.out_layers.3.bias\n",
      "input_blocks.8.1.norm.weight\n",
      "input_blocks.8.1.norm.bias\n",
      "input_blocks.8.1.proj_in.weight\n",
      "input_blocks.8.1.proj_in.bias\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.8.1.proj_out.weight\n",
      "input_blocks.8.1.proj_out.bias\n",
      "input_blocks.9.0.op.weight\n",
      "input_blocks.9.0.op.bias\n",
      "input_blocks.10.0.in_layers.0.weight\n",
      "input_blocks.10.0.in_layers.0.bias\n",
      "input_blocks.10.0.in_layers.2.weight\n",
      "input_blocks.10.0.in_layers.2.bias\n",
      "input_blocks.10.0.emb_layers.1.weight\n",
      "input_blocks.10.0.emb_layers.1.bias\n",
      "input_blocks.10.0.out_layers.0.weight\n",
      "input_blocks.10.0.out_layers.0.bias\n",
      "input_blocks.10.0.out_layers.3.weight\n",
      "input_blocks.10.0.out_layers.3.bias\n",
      "input_blocks.10.1.norm.weight\n",
      "input_blocks.10.1.norm.bias\n",
      "input_blocks.10.1.proj_in.weight\n",
      "input_blocks.10.1.proj_in.bias\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.10.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.10.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.10.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.10.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.10.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.10.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.10.1.proj_out.weight\n",
      "input_blocks.10.1.proj_out.bias\n",
      "input_blocks.11.0.in_layers.0.weight\n",
      "input_blocks.11.0.in_layers.0.bias\n",
      "input_blocks.11.0.in_layers.2.weight\n",
      "input_blocks.11.0.in_layers.2.bias\n",
      "input_blocks.11.0.emb_layers.1.weight\n",
      "input_blocks.11.0.emb_layers.1.bias\n",
      "input_blocks.11.0.out_layers.0.weight\n",
      "input_blocks.11.0.out_layers.0.bias\n",
      "input_blocks.11.0.out_layers.3.weight\n",
      "input_blocks.11.0.out_layers.3.bias\n",
      "input_blocks.11.1.norm.weight\n",
      "input_blocks.11.1.norm.bias\n",
      "input_blocks.11.1.proj_in.weight\n",
      "input_blocks.11.1.proj_in.bias\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.11.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.11.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.11.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.11.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.11.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.11.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.11.1.proj_out.weight\n",
      "input_blocks.11.1.proj_out.bias\n",
      "middle_block.0.in_layers.0.weight\n",
      "middle_block.0.in_layers.0.bias\n",
      "middle_block.0.in_layers.2.weight\n",
      "middle_block.0.in_layers.2.bias\n",
      "middle_block.0.emb_layers.1.weight\n",
      "middle_block.0.emb_layers.1.bias\n",
      "middle_block.0.out_layers.0.weight\n",
      "middle_block.0.out_layers.0.bias\n",
      "middle_block.0.out_layers.3.weight\n",
      "middle_block.0.out_layers.3.bias\n",
      "middle_block.1.norm.weight\n",
      "middle_block.1.norm.bias\n",
      "middle_block.1.proj_in.weight\n",
      "middle_block.1.proj_in.bias\n",
      "middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "middle_block.1.transformer_blocks.0.norm1.weight\n",
      "middle_block.1.transformer_blocks.0.norm1.bias\n",
      "middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "middle_block.1.transformer_blocks.0.norm2.weight\n",
      "middle_block.1.transformer_blocks.0.norm2.bias\n",
      "middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "middle_block.1.transformer_blocks.0.norm3.weight\n",
      "middle_block.1.transformer_blocks.0.norm3.bias\n",
      "middle_block.1.proj_out.weight\n",
      "middle_block.1.proj_out.bias\n",
      "middle_block.2.in_layers.0.weight\n",
      "middle_block.2.in_layers.0.bias\n",
      "middle_block.2.in_layers.2.weight\n",
      "middle_block.2.in_layers.2.bias\n",
      "middle_block.2.emb_layers.1.weight\n",
      "middle_block.2.emb_layers.1.bias\n",
      "middle_block.2.out_layers.0.weight\n",
      "middle_block.2.out_layers.0.bias\n",
      "middle_block.2.out_layers.3.weight\n",
      "middle_block.2.out_layers.3.bias\n",
      "output_blocks.0.0.in_layers.0.weight\n",
      "output_blocks.0.0.in_layers.0.bias\n",
      "output_blocks.0.0.in_layers.2.weight\n",
      "output_blocks.0.0.in_layers.2.bias\n",
      "output_blocks.0.0.emb_layers.1.weight\n",
      "output_blocks.0.0.emb_layers.1.bias\n",
      "output_blocks.0.0.out_layers.0.weight\n",
      "output_blocks.0.0.out_layers.0.bias\n",
      "output_blocks.0.0.out_layers.3.weight\n",
      "output_blocks.0.0.out_layers.3.bias\n",
      "output_blocks.0.0.skip_connection.weight\n",
      "output_blocks.0.0.skip_connection.bias\n",
      "output_blocks.0.1.norm.weight\n",
      "output_blocks.0.1.norm.bias\n",
      "output_blocks.0.1.proj_in.weight\n",
      "output_blocks.0.1.proj_in.bias\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.0.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.0.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.0.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.0.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.0.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.0.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.0.1.proj_out.weight\n",
      "output_blocks.0.1.proj_out.bias\n",
      "output_blocks.1.0.in_layers.0.weight\n",
      "output_blocks.1.0.in_layers.0.bias\n",
      "output_blocks.1.0.in_layers.2.weight\n",
      "output_blocks.1.0.in_layers.2.bias\n",
      "output_blocks.1.0.emb_layers.1.weight\n",
      "output_blocks.1.0.emb_layers.1.bias\n",
      "output_blocks.1.0.out_layers.0.weight\n",
      "output_blocks.1.0.out_layers.0.bias\n",
      "output_blocks.1.0.out_layers.3.weight\n",
      "output_blocks.1.0.out_layers.3.bias\n",
      "output_blocks.1.0.skip_connection.weight\n",
      "output_blocks.1.0.skip_connection.bias\n",
      "output_blocks.1.1.norm.weight\n",
      "output_blocks.1.1.norm.bias\n",
      "output_blocks.1.1.proj_in.weight\n",
      "output_blocks.1.1.proj_in.bias\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.1.1.proj_out.weight\n",
      "output_blocks.1.1.proj_out.bias\n",
      "output_blocks.2.0.in_layers.0.weight\n",
      "output_blocks.2.0.in_layers.0.bias\n",
      "output_blocks.2.0.in_layers.2.weight\n",
      "output_blocks.2.0.in_layers.2.bias\n",
      "output_blocks.2.0.emb_layers.1.weight\n",
      "output_blocks.2.0.emb_layers.1.bias\n",
      "output_blocks.2.0.out_layers.0.weight\n",
      "output_blocks.2.0.out_layers.0.bias\n",
      "output_blocks.2.0.out_layers.3.weight\n",
      "output_blocks.2.0.out_layers.3.bias\n",
      "output_blocks.2.0.skip_connection.weight\n",
      "output_blocks.2.0.skip_connection.bias\n",
      "output_blocks.2.1.norm.weight\n",
      "output_blocks.2.1.norm.bias\n",
      "output_blocks.2.1.proj_in.weight\n",
      "output_blocks.2.1.proj_in.bias\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.2.1.proj_out.weight\n",
      "output_blocks.2.1.proj_out.bias\n",
      "output_blocks.2.2.conv.weight\n",
      "output_blocks.2.2.conv.bias\n",
      "output_blocks.3.0.in_layers.0.weight\n",
      "output_blocks.3.0.in_layers.0.bias\n",
      "output_blocks.3.0.in_layers.2.weight\n",
      "output_blocks.3.0.in_layers.2.bias\n",
      "output_blocks.3.0.emb_layers.1.weight\n",
      "output_blocks.3.0.emb_layers.1.bias\n",
      "output_blocks.3.0.out_layers.0.weight\n",
      "output_blocks.3.0.out_layers.0.bias\n",
      "output_blocks.3.0.out_layers.3.weight\n",
      "output_blocks.3.0.out_layers.3.bias\n",
      "output_blocks.3.0.skip_connection.weight\n",
      "output_blocks.3.0.skip_connection.bias\n",
      "output_blocks.3.1.norm.weight\n",
      "output_blocks.3.1.norm.bias\n",
      "output_blocks.3.1.proj_in.weight\n",
      "output_blocks.3.1.proj_in.bias\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.3.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.3.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.3.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.3.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.3.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.3.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.3.1.proj_out.weight\n",
      "output_blocks.3.1.proj_out.bias\n",
      "output_blocks.4.0.in_layers.0.weight\n",
      "output_blocks.4.0.in_layers.0.bias\n",
      "output_blocks.4.0.in_layers.2.weight\n",
      "output_blocks.4.0.in_layers.2.bias\n",
      "output_blocks.4.0.emb_layers.1.weight\n",
      "output_blocks.4.0.emb_layers.1.bias\n",
      "output_blocks.4.0.out_layers.0.weight\n",
      "output_blocks.4.0.out_layers.0.bias\n",
      "output_blocks.4.0.out_layers.3.weight\n",
      "output_blocks.4.0.out_layers.3.bias\n",
      "output_blocks.4.0.skip_connection.weight\n",
      "output_blocks.4.0.skip_connection.bias\n",
      "output_blocks.4.1.norm.weight\n",
      "output_blocks.4.1.norm.bias\n",
      "output_blocks.4.1.proj_in.weight\n",
      "output_blocks.4.1.proj_in.bias\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.4.1.proj_out.weight\n",
      "output_blocks.4.1.proj_out.bias\n",
      "output_blocks.5.0.in_layers.0.weight\n",
      "output_blocks.5.0.in_layers.0.bias\n",
      "output_blocks.5.0.in_layers.2.weight\n",
      "output_blocks.5.0.in_layers.2.bias\n",
      "output_blocks.5.0.emb_layers.1.weight\n",
      "output_blocks.5.0.emb_layers.1.bias\n",
      "output_blocks.5.0.out_layers.0.weight\n",
      "output_blocks.5.0.out_layers.0.bias\n",
      "output_blocks.5.0.out_layers.3.weight\n",
      "output_blocks.5.0.out_layers.3.bias\n",
      "output_blocks.5.0.skip_connection.weight\n",
      "output_blocks.5.0.skip_connection.bias\n",
      "output_blocks.5.1.norm.weight\n",
      "output_blocks.5.1.norm.bias\n",
      "output_blocks.5.1.proj_in.weight\n",
      "output_blocks.5.1.proj_in.bias\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.5.1.proj_out.weight\n",
      "output_blocks.5.1.proj_out.bias\n",
      "output_blocks.5.2.conv.weight\n",
      "output_blocks.5.2.conv.bias\n",
      "output_blocks.6.0.in_layers.0.weight\n",
      "output_blocks.6.0.in_layers.0.bias\n",
      "output_blocks.6.0.in_layers.2.weight\n",
      "output_blocks.6.0.in_layers.2.bias\n",
      "output_blocks.6.0.emb_layers.1.weight\n",
      "output_blocks.6.0.emb_layers.1.bias\n",
      "output_blocks.6.0.out_layers.0.weight\n",
      "output_blocks.6.0.out_layers.0.bias\n",
      "output_blocks.6.0.out_layers.3.weight\n",
      "output_blocks.6.0.out_layers.3.bias\n",
      "output_blocks.6.0.skip_connection.weight\n",
      "output_blocks.6.0.skip_connection.bias\n",
      "output_blocks.7.0.in_layers.0.weight\n",
      "output_blocks.7.0.in_layers.0.bias\n",
      "output_blocks.7.0.in_layers.2.weight\n",
      "output_blocks.7.0.in_layers.2.bias\n",
      "output_blocks.7.0.emb_layers.1.weight\n",
      "output_blocks.7.0.emb_layers.1.bias\n",
      "output_blocks.7.0.out_layers.0.weight\n",
      "output_blocks.7.0.out_layers.0.bias\n",
      "output_blocks.7.0.out_layers.3.weight\n",
      "output_blocks.7.0.out_layers.3.bias\n",
      "output_blocks.7.0.skip_connection.weight\n",
      "output_blocks.7.0.skip_connection.bias\n",
      "output_blocks.8.0.in_layers.0.weight\n",
      "output_blocks.8.0.in_layers.0.bias\n",
      "output_blocks.8.0.in_layers.2.weight\n",
      "output_blocks.8.0.in_layers.2.bias\n",
      "output_blocks.8.0.emb_layers.1.weight\n",
      "output_blocks.8.0.emb_layers.1.bias\n",
      "output_blocks.8.0.out_layers.0.weight\n",
      "output_blocks.8.0.out_layers.0.bias\n",
      "output_blocks.8.0.out_layers.3.weight\n",
      "output_blocks.8.0.out_layers.3.bias\n",
      "output_blocks.8.0.skip_connection.weight\n",
      "output_blocks.8.0.skip_connection.bias\n",
      "output_blocks.8.1.conv.weight\n",
      "output_blocks.8.1.conv.bias\n",
      "output_blocks.9.0.in_layers.0.weight\n",
      "output_blocks.9.0.in_layers.0.bias\n",
      "output_blocks.9.0.in_layers.2.weight\n",
      "output_blocks.9.0.in_layers.2.bias\n",
      "output_blocks.9.0.emb_layers.1.weight\n",
      "output_blocks.9.0.emb_layers.1.bias\n",
      "output_blocks.9.0.out_layers.0.weight\n",
      "output_blocks.9.0.out_layers.0.bias\n",
      "output_blocks.9.0.out_layers.3.weight\n",
      "output_blocks.9.0.out_layers.3.bias\n",
      "output_blocks.9.0.skip_connection.weight\n",
      "output_blocks.9.0.skip_connection.bias\n",
      "output_blocks.10.0.in_layers.0.weight\n",
      "output_blocks.10.0.in_layers.0.bias\n",
      "output_blocks.10.0.in_layers.2.weight\n",
      "output_blocks.10.0.in_layers.2.bias\n",
      "output_blocks.10.0.emb_layers.1.weight\n",
      "output_blocks.10.0.emb_layers.1.bias\n",
      "output_blocks.10.0.out_layers.0.weight\n",
      "output_blocks.10.0.out_layers.0.bias\n",
      "output_blocks.10.0.out_layers.3.weight\n",
      "output_blocks.10.0.out_layers.3.bias\n",
      "output_blocks.10.0.skip_connection.weight\n",
      "output_blocks.10.0.skip_connection.bias\n",
      "output_blocks.11.0.in_layers.0.weight\n",
      "output_blocks.11.0.in_layers.0.bias\n",
      "output_blocks.11.0.in_layers.2.weight\n",
      "output_blocks.11.0.in_layers.2.bias\n",
      "output_blocks.11.0.emb_layers.1.weight\n",
      "output_blocks.11.0.emb_layers.1.bias\n",
      "output_blocks.11.0.out_layers.0.weight\n",
      "output_blocks.11.0.out_layers.0.bias\n",
      "output_blocks.11.0.out_layers.3.weight\n",
      "output_blocks.11.0.out_layers.3.bias\n",
      "output_blocks.11.0.skip_connection.weight\n",
      "output_blocks.11.0.skip_connection.bias\n",
      "out.0.weight\n",
      "out.0.bias\n",
      "out.2.weight\n",
      "out.2.bias\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "polyffusion_unet_params = inspect.signature(PolyffusionUNet.__init__).parameters\n",
    "polyffusion_unet_params_dict = {key:params[key] for key in params if key in polyffusion_unet_params}\n",
    "polyffusion_unet = PolyffusionUNet(**polyffusion_unet_params_dict)\n",
    "UNET_PREFIX = \"ldm.eps_model.\"\n",
    "polyffusion_unet_state_dict = {key.removeprefix(UNET_PREFIX):value for key,value in polyffusion_checkpoint.items() if key.startswith(UNET_PREFIX)}\n",
    "polyffusion_unet.load_state_dict(polyffusion_unet_state_dict)\n",
    "\n",
    "for key in polyffusion_unet.state_dict().keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_embed.0.weight\n",
      "time_embed.0.bias\n",
      "time_embed.2.weight\n",
      "time_embed.2.bias\n",
      "input_blocks.0.0.weight\n",
      "input_blocks.0.0.bias\n",
      "input_blocks.1.0.in_layers.0.weight\n",
      "input_blocks.1.0.in_layers.0.bias\n",
      "input_blocks.1.0.in_layers.2.weight\n",
      "input_blocks.1.0.in_layers.2.bias\n",
      "input_blocks.1.0.emb_layers.1.weight\n",
      "input_blocks.1.0.emb_layers.1.bias\n",
      "input_blocks.1.0.out_layers.0.weight\n",
      "input_blocks.1.0.out_layers.0.bias\n",
      "input_blocks.1.0.out_layers.3.weight\n",
      "input_blocks.1.0.out_layers.3.bias\n",
      "input_blocks.2.0.in_layers.0.weight\n",
      "input_blocks.2.0.in_layers.0.bias\n",
      "input_blocks.2.0.in_layers.2.weight\n",
      "input_blocks.2.0.in_layers.2.bias\n",
      "input_blocks.2.0.emb_layers.1.weight\n",
      "input_blocks.2.0.emb_layers.1.bias\n",
      "input_blocks.2.0.out_layers.0.weight\n",
      "input_blocks.2.0.out_layers.0.bias\n",
      "input_blocks.2.0.out_layers.3.weight\n",
      "input_blocks.2.0.out_layers.3.bias\n",
      "input_blocks.3.0.op.weight\n",
      "input_blocks.3.0.op.bias\n",
      "input_blocks.4.0.in_layers.0.weight\n",
      "input_blocks.4.0.in_layers.0.bias\n",
      "input_blocks.4.0.in_layers.2.weight\n",
      "input_blocks.4.0.in_layers.2.bias\n",
      "input_blocks.4.0.emb_layers.1.weight\n",
      "input_blocks.4.0.emb_layers.1.bias\n",
      "input_blocks.4.0.out_layers.0.weight\n",
      "input_blocks.4.0.out_layers.0.bias\n",
      "input_blocks.4.0.out_layers.3.weight\n",
      "input_blocks.4.0.out_layers.3.bias\n",
      "input_blocks.4.0.skip_connection.weight\n",
      "input_blocks.4.0.skip_connection.bias\n",
      "input_blocks.5.0.in_layers.0.weight\n",
      "input_blocks.5.0.in_layers.0.bias\n",
      "input_blocks.5.0.in_layers.2.weight\n",
      "input_blocks.5.0.in_layers.2.bias\n",
      "input_blocks.5.0.emb_layers.1.weight\n",
      "input_blocks.5.0.emb_layers.1.bias\n",
      "input_blocks.5.0.out_layers.0.weight\n",
      "input_blocks.5.0.out_layers.0.bias\n",
      "input_blocks.5.0.out_layers.3.weight\n",
      "input_blocks.5.0.out_layers.3.bias\n",
      "input_blocks.6.0.op.weight\n",
      "input_blocks.6.0.op.bias\n",
      "input_blocks.7.0.in_layers.0.weight\n",
      "input_blocks.7.0.in_layers.0.bias\n",
      "input_blocks.7.0.in_layers.2.weight\n",
      "input_blocks.7.0.in_layers.2.bias\n",
      "input_blocks.7.0.emb_layers.1.weight\n",
      "input_blocks.7.0.emb_layers.1.bias\n",
      "input_blocks.7.0.out_layers.0.weight\n",
      "input_blocks.7.0.out_layers.0.bias\n",
      "input_blocks.7.0.out_layers.3.weight\n",
      "input_blocks.7.0.out_layers.3.bias\n",
      "input_blocks.7.0.skip_connection.weight\n",
      "input_blocks.7.0.skip_connection.bias\n",
      "input_blocks.7.1.norm.weight\n",
      "input_blocks.7.1.norm.bias\n",
      "input_blocks.7.1.proj_in.weight\n",
      "input_blocks.7.1.proj_in.bias\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.7.1.proj_out.weight\n",
      "input_blocks.7.1.proj_out.bias\n",
      "input_blocks.7.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.7.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.7.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.7.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.7.2.attention.layers.0.linear1.weight\n",
      "input_blocks.7.2.attention.layers.0.linear1.bias\n",
      "input_blocks.7.2.attention.layers.0.linear2.weight\n",
      "input_blocks.7.2.attention.layers.0.linear2.bias\n",
      "input_blocks.7.2.attention.layers.0.norm1.weight\n",
      "input_blocks.7.2.attention.layers.0.norm1.bias\n",
      "input_blocks.7.2.attention.layers.0.norm2.weight\n",
      "input_blocks.7.2.attention.layers.0.norm2.bias\n",
      "input_blocks.7.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.7.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.7.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.7.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.7.2.attention.layers.1.linear1.weight\n",
      "input_blocks.7.2.attention.layers.1.linear1.bias\n",
      "input_blocks.7.2.attention.layers.1.linear2.weight\n",
      "input_blocks.7.2.attention.layers.1.linear2.bias\n",
      "input_blocks.7.2.attention.layers.1.norm1.weight\n",
      "input_blocks.7.2.attention.layers.1.norm1.bias\n",
      "input_blocks.7.2.attention.layers.1.norm2.weight\n",
      "input_blocks.7.2.attention.layers.1.norm2.bias\n",
      "input_blocks.8.0.in_layers.0.weight\n",
      "input_blocks.8.0.in_layers.0.bias\n",
      "input_blocks.8.0.in_layers.2.weight\n",
      "input_blocks.8.0.in_layers.2.bias\n",
      "input_blocks.8.0.emb_layers.1.weight\n",
      "input_blocks.8.0.emb_layers.1.bias\n",
      "input_blocks.8.0.out_layers.0.weight\n",
      "input_blocks.8.0.out_layers.0.bias\n",
      "input_blocks.8.0.out_layers.3.weight\n",
      "input_blocks.8.0.out_layers.3.bias\n",
      "input_blocks.8.1.norm.weight\n",
      "input_blocks.8.1.norm.bias\n",
      "input_blocks.8.1.proj_in.weight\n",
      "input_blocks.8.1.proj_in.bias\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.8.1.proj_out.weight\n",
      "input_blocks.8.1.proj_out.bias\n",
      "input_blocks.8.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.8.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.8.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.8.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.8.2.attention.layers.0.linear1.weight\n",
      "input_blocks.8.2.attention.layers.0.linear1.bias\n",
      "input_blocks.8.2.attention.layers.0.linear2.weight\n",
      "input_blocks.8.2.attention.layers.0.linear2.bias\n",
      "input_blocks.8.2.attention.layers.0.norm1.weight\n",
      "input_blocks.8.2.attention.layers.0.norm1.bias\n",
      "input_blocks.8.2.attention.layers.0.norm2.weight\n",
      "input_blocks.8.2.attention.layers.0.norm2.bias\n",
      "input_blocks.8.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.8.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.8.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.8.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.8.2.attention.layers.1.linear1.weight\n",
      "input_blocks.8.2.attention.layers.1.linear1.bias\n",
      "input_blocks.8.2.attention.layers.1.linear2.weight\n",
      "input_blocks.8.2.attention.layers.1.linear2.bias\n",
      "input_blocks.8.2.attention.layers.1.norm1.weight\n",
      "input_blocks.8.2.attention.layers.1.norm1.bias\n",
      "input_blocks.8.2.attention.layers.1.norm2.weight\n",
      "input_blocks.8.2.attention.layers.1.norm2.bias\n",
      "input_blocks.9.0.op.weight\n",
      "input_blocks.9.0.op.bias\n",
      "input_blocks.10.0.in_layers.0.weight\n",
      "input_blocks.10.0.in_layers.0.bias\n",
      "input_blocks.10.0.in_layers.2.weight\n",
      "input_blocks.10.0.in_layers.2.bias\n",
      "input_blocks.10.0.emb_layers.1.weight\n",
      "input_blocks.10.0.emb_layers.1.bias\n",
      "input_blocks.10.0.out_layers.0.weight\n",
      "input_blocks.10.0.out_layers.0.bias\n",
      "input_blocks.10.0.out_layers.3.weight\n",
      "input_blocks.10.0.out_layers.3.bias\n",
      "input_blocks.10.1.norm.weight\n",
      "input_blocks.10.1.norm.bias\n",
      "input_blocks.10.1.proj_in.weight\n",
      "input_blocks.10.1.proj_in.bias\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.10.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.10.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.10.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.10.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.10.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.10.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.10.1.proj_out.weight\n",
      "input_blocks.10.1.proj_out.bias\n",
      "input_blocks.10.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.10.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.10.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.10.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.10.2.attention.layers.0.linear1.weight\n",
      "input_blocks.10.2.attention.layers.0.linear1.bias\n",
      "input_blocks.10.2.attention.layers.0.linear2.weight\n",
      "input_blocks.10.2.attention.layers.0.linear2.bias\n",
      "input_blocks.10.2.attention.layers.0.norm1.weight\n",
      "input_blocks.10.2.attention.layers.0.norm1.bias\n",
      "input_blocks.10.2.attention.layers.0.norm2.weight\n",
      "input_blocks.10.2.attention.layers.0.norm2.bias\n",
      "input_blocks.10.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.10.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.10.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.10.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.10.2.attention.layers.1.linear1.weight\n",
      "input_blocks.10.2.attention.layers.1.linear1.bias\n",
      "input_blocks.10.2.attention.layers.1.linear2.weight\n",
      "input_blocks.10.2.attention.layers.1.linear2.bias\n",
      "input_blocks.10.2.attention.layers.1.norm1.weight\n",
      "input_blocks.10.2.attention.layers.1.norm1.bias\n",
      "input_blocks.10.2.attention.layers.1.norm2.weight\n",
      "input_blocks.10.2.attention.layers.1.norm2.bias\n",
      "input_blocks.11.0.in_layers.0.weight\n",
      "input_blocks.11.0.in_layers.0.bias\n",
      "input_blocks.11.0.in_layers.2.weight\n",
      "input_blocks.11.0.in_layers.2.bias\n",
      "input_blocks.11.0.emb_layers.1.weight\n",
      "input_blocks.11.0.emb_layers.1.bias\n",
      "input_blocks.11.0.out_layers.0.weight\n",
      "input_blocks.11.0.out_layers.0.bias\n",
      "input_blocks.11.0.out_layers.3.weight\n",
      "input_blocks.11.0.out_layers.3.bias\n",
      "input_blocks.11.1.norm.weight\n",
      "input_blocks.11.1.norm.bias\n",
      "input_blocks.11.1.proj_in.weight\n",
      "input_blocks.11.1.proj_in.bias\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.11.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.11.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.11.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.11.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.11.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.11.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.11.1.proj_out.weight\n",
      "input_blocks.11.1.proj_out.bias\n",
      "input_blocks.11.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.11.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.11.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.11.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.11.2.attention.layers.0.linear1.weight\n",
      "input_blocks.11.2.attention.layers.0.linear1.bias\n",
      "input_blocks.11.2.attention.layers.0.linear2.weight\n",
      "input_blocks.11.2.attention.layers.0.linear2.bias\n",
      "input_blocks.11.2.attention.layers.0.norm1.weight\n",
      "input_blocks.11.2.attention.layers.0.norm1.bias\n",
      "input_blocks.11.2.attention.layers.0.norm2.weight\n",
      "input_blocks.11.2.attention.layers.0.norm2.bias\n",
      "input_blocks.11.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.11.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.11.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.11.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.11.2.attention.layers.1.linear1.weight\n",
      "input_blocks.11.2.attention.layers.1.linear1.bias\n",
      "input_blocks.11.2.attention.layers.1.linear2.weight\n",
      "input_blocks.11.2.attention.layers.1.linear2.bias\n",
      "input_blocks.11.2.attention.layers.1.norm1.weight\n",
      "input_blocks.11.2.attention.layers.1.norm1.bias\n",
      "input_blocks.11.2.attention.layers.1.norm2.weight\n",
      "input_blocks.11.2.attention.layers.1.norm2.bias\n",
      "middle_block.0.in_layers.0.weight\n",
      "middle_block.0.in_layers.0.bias\n",
      "middle_block.0.in_layers.2.weight\n",
      "middle_block.0.in_layers.2.bias\n",
      "middle_block.0.emb_layers.1.weight\n",
      "middle_block.0.emb_layers.1.bias\n",
      "middle_block.0.out_layers.0.weight\n",
      "middle_block.0.out_layers.0.bias\n",
      "middle_block.0.out_layers.3.weight\n",
      "middle_block.0.out_layers.3.bias\n",
      "middle_block.1.norm.weight\n",
      "middle_block.1.norm.bias\n",
      "middle_block.1.proj_in.weight\n",
      "middle_block.1.proj_in.bias\n",
      "middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "middle_block.1.transformer_blocks.0.norm1.weight\n",
      "middle_block.1.transformer_blocks.0.norm1.bias\n",
      "middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "middle_block.1.transformer_blocks.0.norm2.weight\n",
      "middle_block.1.transformer_blocks.0.norm2.bias\n",
      "middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "middle_block.1.transformer_blocks.0.norm3.weight\n",
      "middle_block.1.transformer_blocks.0.norm3.bias\n",
      "middle_block.1.proj_out.weight\n",
      "middle_block.1.proj_out.bias\n",
      "middle_block.2.in_layers.0.weight\n",
      "middle_block.2.in_layers.0.bias\n",
      "middle_block.2.in_layers.2.weight\n",
      "middle_block.2.in_layers.2.bias\n",
      "middle_block.2.emb_layers.1.weight\n",
      "middle_block.2.emb_layers.1.bias\n",
      "middle_block.2.out_layers.0.weight\n",
      "middle_block.2.out_layers.0.bias\n",
      "middle_block.2.out_layers.3.weight\n",
      "middle_block.2.out_layers.3.bias\n",
      "output_blocks.0.0.in_layers.0.weight\n",
      "output_blocks.0.0.in_layers.0.bias\n",
      "output_blocks.0.0.in_layers.2.weight\n",
      "output_blocks.0.0.in_layers.2.bias\n",
      "output_blocks.0.0.emb_layers.1.weight\n",
      "output_blocks.0.0.emb_layers.1.bias\n",
      "output_blocks.0.0.out_layers.0.weight\n",
      "output_blocks.0.0.out_layers.0.bias\n",
      "output_blocks.0.0.out_layers.3.weight\n",
      "output_blocks.0.0.out_layers.3.bias\n",
      "output_blocks.0.0.skip_connection.weight\n",
      "output_blocks.0.0.skip_connection.bias\n",
      "output_blocks.0.1.norm.weight\n",
      "output_blocks.0.1.norm.bias\n",
      "output_blocks.0.1.proj_in.weight\n",
      "output_blocks.0.1.proj_in.bias\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.0.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.0.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.0.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.0.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.0.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.0.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.0.1.proj_out.weight\n",
      "output_blocks.0.1.proj_out.bias\n",
      "output_blocks.0.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.0.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.0.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.0.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.0.2.attention.layers.0.linear1.weight\n",
      "output_blocks.0.2.attention.layers.0.linear1.bias\n",
      "output_blocks.0.2.attention.layers.0.linear2.weight\n",
      "output_blocks.0.2.attention.layers.0.linear2.bias\n",
      "output_blocks.0.2.attention.layers.0.norm1.weight\n",
      "output_blocks.0.2.attention.layers.0.norm1.bias\n",
      "output_blocks.0.2.attention.layers.0.norm2.weight\n",
      "output_blocks.0.2.attention.layers.0.norm2.bias\n",
      "output_blocks.0.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.0.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.0.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.0.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.0.2.attention.layers.1.linear1.weight\n",
      "output_blocks.0.2.attention.layers.1.linear1.bias\n",
      "output_blocks.0.2.attention.layers.1.linear2.weight\n",
      "output_blocks.0.2.attention.layers.1.linear2.bias\n",
      "output_blocks.0.2.attention.layers.1.norm1.weight\n",
      "output_blocks.0.2.attention.layers.1.norm1.bias\n",
      "output_blocks.0.2.attention.layers.1.norm2.weight\n",
      "output_blocks.0.2.attention.layers.1.norm2.bias\n",
      "output_blocks.1.0.in_layers.0.weight\n",
      "output_blocks.1.0.in_layers.0.bias\n",
      "output_blocks.1.0.in_layers.2.weight\n",
      "output_blocks.1.0.in_layers.2.bias\n",
      "output_blocks.1.0.emb_layers.1.weight\n",
      "output_blocks.1.0.emb_layers.1.bias\n",
      "output_blocks.1.0.out_layers.0.weight\n",
      "output_blocks.1.0.out_layers.0.bias\n",
      "output_blocks.1.0.out_layers.3.weight\n",
      "output_blocks.1.0.out_layers.3.bias\n",
      "output_blocks.1.0.skip_connection.weight\n",
      "output_blocks.1.0.skip_connection.bias\n",
      "output_blocks.1.1.norm.weight\n",
      "output_blocks.1.1.norm.bias\n",
      "output_blocks.1.1.proj_in.weight\n",
      "output_blocks.1.1.proj_in.bias\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.1.1.proj_out.weight\n",
      "output_blocks.1.1.proj_out.bias\n",
      "output_blocks.1.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.1.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.1.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.1.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.1.2.attention.layers.0.linear1.weight\n",
      "output_blocks.1.2.attention.layers.0.linear1.bias\n",
      "output_blocks.1.2.attention.layers.0.linear2.weight\n",
      "output_blocks.1.2.attention.layers.0.linear2.bias\n",
      "output_blocks.1.2.attention.layers.0.norm1.weight\n",
      "output_blocks.1.2.attention.layers.0.norm1.bias\n",
      "output_blocks.1.2.attention.layers.0.norm2.weight\n",
      "output_blocks.1.2.attention.layers.0.norm2.bias\n",
      "output_blocks.1.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.1.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.1.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.1.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.1.2.attention.layers.1.linear1.weight\n",
      "output_blocks.1.2.attention.layers.1.linear1.bias\n",
      "output_blocks.1.2.attention.layers.1.linear2.weight\n",
      "output_blocks.1.2.attention.layers.1.linear2.bias\n",
      "output_blocks.1.2.attention.layers.1.norm1.weight\n",
      "output_blocks.1.2.attention.layers.1.norm1.bias\n",
      "output_blocks.1.2.attention.layers.1.norm2.weight\n",
      "output_blocks.1.2.attention.layers.1.norm2.bias\n",
      "output_blocks.2.0.in_layers.0.weight\n",
      "output_blocks.2.0.in_layers.0.bias\n",
      "output_blocks.2.0.in_layers.2.weight\n",
      "output_blocks.2.0.in_layers.2.bias\n",
      "output_blocks.2.0.emb_layers.1.weight\n",
      "output_blocks.2.0.emb_layers.1.bias\n",
      "output_blocks.2.0.out_layers.0.weight\n",
      "output_blocks.2.0.out_layers.0.bias\n",
      "output_blocks.2.0.out_layers.3.weight\n",
      "output_blocks.2.0.out_layers.3.bias\n",
      "output_blocks.2.0.skip_connection.weight\n",
      "output_blocks.2.0.skip_connection.bias\n",
      "output_blocks.2.1.norm.weight\n",
      "output_blocks.2.1.norm.bias\n",
      "output_blocks.2.1.proj_in.weight\n",
      "output_blocks.2.1.proj_in.bias\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.2.1.proj_out.weight\n",
      "output_blocks.2.1.proj_out.bias\n",
      "output_blocks.2.2.conv.weight\n",
      "output_blocks.2.2.conv.bias\n",
      "output_blocks.2.3.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.2.3.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.2.3.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.2.3.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.2.3.attention.layers.0.linear1.weight\n",
      "output_blocks.2.3.attention.layers.0.linear1.bias\n",
      "output_blocks.2.3.attention.layers.0.linear2.weight\n",
      "output_blocks.2.3.attention.layers.0.linear2.bias\n",
      "output_blocks.2.3.attention.layers.0.norm1.weight\n",
      "output_blocks.2.3.attention.layers.0.norm1.bias\n",
      "output_blocks.2.3.attention.layers.0.norm2.weight\n",
      "output_blocks.2.3.attention.layers.0.norm2.bias\n",
      "output_blocks.2.3.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.2.3.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.2.3.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.2.3.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.2.3.attention.layers.1.linear1.weight\n",
      "output_blocks.2.3.attention.layers.1.linear1.bias\n",
      "output_blocks.2.3.attention.layers.1.linear2.weight\n",
      "output_blocks.2.3.attention.layers.1.linear2.bias\n",
      "output_blocks.2.3.attention.layers.1.norm1.weight\n",
      "output_blocks.2.3.attention.layers.1.norm1.bias\n",
      "output_blocks.2.3.attention.layers.1.norm2.weight\n",
      "output_blocks.2.3.attention.layers.1.norm2.bias\n",
      "output_blocks.3.0.in_layers.0.weight\n",
      "output_blocks.3.0.in_layers.0.bias\n",
      "output_blocks.3.0.in_layers.2.weight\n",
      "output_blocks.3.0.in_layers.2.bias\n",
      "output_blocks.3.0.emb_layers.1.weight\n",
      "output_blocks.3.0.emb_layers.1.bias\n",
      "output_blocks.3.0.out_layers.0.weight\n",
      "output_blocks.3.0.out_layers.0.bias\n",
      "output_blocks.3.0.out_layers.3.weight\n",
      "output_blocks.3.0.out_layers.3.bias\n",
      "output_blocks.3.0.skip_connection.weight\n",
      "output_blocks.3.0.skip_connection.bias\n",
      "output_blocks.3.1.norm.weight\n",
      "output_blocks.3.1.norm.bias\n",
      "output_blocks.3.1.proj_in.weight\n",
      "output_blocks.3.1.proj_in.bias\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.3.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.3.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.3.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.3.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.3.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.3.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.3.1.proj_out.weight\n",
      "output_blocks.3.1.proj_out.bias\n",
      "output_blocks.3.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.3.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.3.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.3.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.3.2.attention.layers.0.linear1.weight\n",
      "output_blocks.3.2.attention.layers.0.linear1.bias\n",
      "output_blocks.3.2.attention.layers.0.linear2.weight\n",
      "output_blocks.3.2.attention.layers.0.linear2.bias\n",
      "output_blocks.3.2.attention.layers.0.norm1.weight\n",
      "output_blocks.3.2.attention.layers.0.norm1.bias\n",
      "output_blocks.3.2.attention.layers.0.norm2.weight\n",
      "output_blocks.3.2.attention.layers.0.norm2.bias\n",
      "output_blocks.3.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.3.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.3.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.3.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.3.2.attention.layers.1.linear1.weight\n",
      "output_blocks.3.2.attention.layers.1.linear1.bias\n",
      "output_blocks.3.2.attention.layers.1.linear2.weight\n",
      "output_blocks.3.2.attention.layers.1.linear2.bias\n",
      "output_blocks.3.2.attention.layers.1.norm1.weight\n",
      "output_blocks.3.2.attention.layers.1.norm1.bias\n",
      "output_blocks.3.2.attention.layers.1.norm2.weight\n",
      "output_blocks.3.2.attention.layers.1.norm2.bias\n",
      "output_blocks.4.0.in_layers.0.weight\n",
      "output_blocks.4.0.in_layers.0.bias\n",
      "output_blocks.4.0.in_layers.2.weight\n",
      "output_blocks.4.0.in_layers.2.bias\n",
      "output_blocks.4.0.emb_layers.1.weight\n",
      "output_blocks.4.0.emb_layers.1.bias\n",
      "output_blocks.4.0.out_layers.0.weight\n",
      "output_blocks.4.0.out_layers.0.bias\n",
      "output_blocks.4.0.out_layers.3.weight\n",
      "output_blocks.4.0.out_layers.3.bias\n",
      "output_blocks.4.0.skip_connection.weight\n",
      "output_blocks.4.0.skip_connection.bias\n",
      "output_blocks.4.1.norm.weight\n",
      "output_blocks.4.1.norm.bias\n",
      "output_blocks.4.1.proj_in.weight\n",
      "output_blocks.4.1.proj_in.bias\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.4.1.proj_out.weight\n",
      "output_blocks.4.1.proj_out.bias\n",
      "output_blocks.4.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.4.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.4.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.4.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.4.2.attention.layers.0.linear1.weight\n",
      "output_blocks.4.2.attention.layers.0.linear1.bias\n",
      "output_blocks.4.2.attention.layers.0.linear2.weight\n",
      "output_blocks.4.2.attention.layers.0.linear2.bias\n",
      "output_blocks.4.2.attention.layers.0.norm1.weight\n",
      "output_blocks.4.2.attention.layers.0.norm1.bias\n",
      "output_blocks.4.2.attention.layers.0.norm2.weight\n",
      "output_blocks.4.2.attention.layers.0.norm2.bias\n",
      "output_blocks.4.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.4.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.4.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.4.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.4.2.attention.layers.1.linear1.weight\n",
      "output_blocks.4.2.attention.layers.1.linear1.bias\n",
      "output_blocks.4.2.attention.layers.1.linear2.weight\n",
      "output_blocks.4.2.attention.layers.1.linear2.bias\n",
      "output_blocks.4.2.attention.layers.1.norm1.weight\n",
      "output_blocks.4.2.attention.layers.1.norm1.bias\n",
      "output_blocks.4.2.attention.layers.1.norm2.weight\n",
      "output_blocks.4.2.attention.layers.1.norm2.bias\n",
      "output_blocks.5.0.in_layers.0.weight\n",
      "output_blocks.5.0.in_layers.0.bias\n",
      "output_blocks.5.0.in_layers.2.weight\n",
      "output_blocks.5.0.in_layers.2.bias\n",
      "output_blocks.5.0.emb_layers.1.weight\n",
      "output_blocks.5.0.emb_layers.1.bias\n",
      "output_blocks.5.0.out_layers.0.weight\n",
      "output_blocks.5.0.out_layers.0.bias\n",
      "output_blocks.5.0.out_layers.3.weight\n",
      "output_blocks.5.0.out_layers.3.bias\n",
      "output_blocks.5.0.skip_connection.weight\n",
      "output_blocks.5.0.skip_connection.bias\n",
      "output_blocks.5.1.norm.weight\n",
      "output_blocks.5.1.norm.bias\n",
      "output_blocks.5.1.proj_in.weight\n",
      "output_blocks.5.1.proj_in.bias\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.5.1.proj_out.weight\n",
      "output_blocks.5.1.proj_out.bias\n",
      "output_blocks.5.2.conv.weight\n",
      "output_blocks.5.2.conv.bias\n",
      "output_blocks.5.3.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.5.3.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.5.3.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.5.3.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.5.3.attention.layers.0.linear1.weight\n",
      "output_blocks.5.3.attention.layers.0.linear1.bias\n",
      "output_blocks.5.3.attention.layers.0.linear2.weight\n",
      "output_blocks.5.3.attention.layers.0.linear2.bias\n",
      "output_blocks.5.3.attention.layers.0.norm1.weight\n",
      "output_blocks.5.3.attention.layers.0.norm1.bias\n",
      "output_blocks.5.3.attention.layers.0.norm2.weight\n",
      "output_blocks.5.3.attention.layers.0.norm2.bias\n",
      "output_blocks.5.3.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.5.3.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.5.3.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.5.3.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.5.3.attention.layers.1.linear1.weight\n",
      "output_blocks.5.3.attention.layers.1.linear1.bias\n",
      "output_blocks.5.3.attention.layers.1.linear2.weight\n",
      "output_blocks.5.3.attention.layers.1.linear2.bias\n",
      "output_blocks.5.3.attention.layers.1.norm1.weight\n",
      "output_blocks.5.3.attention.layers.1.norm1.bias\n",
      "output_blocks.5.3.attention.layers.1.norm2.weight\n",
      "output_blocks.5.3.attention.layers.1.norm2.bias\n",
      "output_blocks.6.0.in_layers.0.weight\n",
      "output_blocks.6.0.in_layers.0.bias\n",
      "output_blocks.6.0.in_layers.2.weight\n",
      "output_blocks.6.0.in_layers.2.bias\n",
      "output_blocks.6.0.emb_layers.1.weight\n",
      "output_blocks.6.0.emb_layers.1.bias\n",
      "output_blocks.6.0.out_layers.0.weight\n",
      "output_blocks.6.0.out_layers.0.bias\n",
      "output_blocks.6.0.out_layers.3.weight\n",
      "output_blocks.6.0.out_layers.3.bias\n",
      "output_blocks.6.0.skip_connection.weight\n",
      "output_blocks.6.0.skip_connection.bias\n",
      "output_blocks.7.0.in_layers.0.weight\n",
      "output_blocks.7.0.in_layers.0.bias\n",
      "output_blocks.7.0.in_layers.2.weight\n",
      "output_blocks.7.0.in_layers.2.bias\n",
      "output_blocks.7.0.emb_layers.1.weight\n",
      "output_blocks.7.0.emb_layers.1.bias\n",
      "output_blocks.7.0.out_layers.0.weight\n",
      "output_blocks.7.0.out_layers.0.bias\n",
      "output_blocks.7.0.out_layers.3.weight\n",
      "output_blocks.7.0.out_layers.3.bias\n",
      "output_blocks.7.0.skip_connection.weight\n",
      "output_blocks.7.0.skip_connection.bias\n",
      "output_blocks.8.0.in_layers.0.weight\n",
      "output_blocks.8.0.in_layers.0.bias\n",
      "output_blocks.8.0.in_layers.2.weight\n",
      "output_blocks.8.0.in_layers.2.bias\n",
      "output_blocks.8.0.emb_layers.1.weight\n",
      "output_blocks.8.0.emb_layers.1.bias\n",
      "output_blocks.8.0.out_layers.0.weight\n",
      "output_blocks.8.0.out_layers.0.bias\n",
      "output_blocks.8.0.out_layers.3.weight\n",
      "output_blocks.8.0.out_layers.3.bias\n",
      "output_blocks.8.0.skip_connection.weight\n",
      "output_blocks.8.0.skip_connection.bias\n",
      "output_blocks.8.1.conv.weight\n",
      "output_blocks.8.1.conv.bias\n",
      "output_blocks.9.0.in_layers.0.weight\n",
      "output_blocks.9.0.in_layers.0.bias\n",
      "output_blocks.9.0.in_layers.2.weight\n",
      "output_blocks.9.0.in_layers.2.bias\n",
      "output_blocks.9.0.emb_layers.1.weight\n",
      "output_blocks.9.0.emb_layers.1.bias\n",
      "output_blocks.9.0.out_layers.0.weight\n",
      "output_blocks.9.0.out_layers.0.bias\n",
      "output_blocks.9.0.out_layers.3.weight\n",
      "output_blocks.9.0.out_layers.3.bias\n",
      "output_blocks.9.0.skip_connection.weight\n",
      "output_blocks.9.0.skip_connection.bias\n",
      "output_blocks.10.0.in_layers.0.weight\n",
      "output_blocks.10.0.in_layers.0.bias\n",
      "output_blocks.10.0.in_layers.2.weight\n",
      "output_blocks.10.0.in_layers.2.bias\n",
      "output_blocks.10.0.emb_layers.1.weight\n",
      "output_blocks.10.0.emb_layers.1.bias\n",
      "output_blocks.10.0.out_layers.0.weight\n",
      "output_blocks.10.0.out_layers.0.bias\n",
      "output_blocks.10.0.out_layers.3.weight\n",
      "output_blocks.10.0.out_layers.3.bias\n",
      "output_blocks.10.0.skip_connection.weight\n",
      "output_blocks.10.0.skip_connection.bias\n",
      "output_blocks.11.0.in_layers.0.weight\n",
      "output_blocks.11.0.in_layers.0.bias\n",
      "output_blocks.11.0.in_layers.2.weight\n",
      "output_blocks.11.0.in_layers.2.bias\n",
      "output_blocks.11.0.emb_layers.1.weight\n",
      "output_blocks.11.0.emb_layers.1.bias\n",
      "output_blocks.11.0.out_layers.0.weight\n",
      "output_blocks.11.0.out_layers.0.bias\n",
      "output_blocks.11.0.out_layers.3.weight\n",
      "output_blocks.11.0.out_layers.3.bias\n",
      "output_blocks.11.0.skip_connection.weight\n",
      "output_blocks.11.0.skip_connection.bias\n",
      "out.0.weight\n",
      "out.0.bias\n",
      "out.2.weight\n",
      "out.2.bias\n"
     ]
    }
   ],
   "source": [
    "multipoly_unet_params = inspect.signature(MultipolyUNet.__init__).parameters\n",
    "multipoly_unet_params_dict = {key:params[key] for key in params if key in multipoly_unet_params}\n",
    "multipoly_unet_params_dict[\"n_intertrack_head\"] = 4\n",
    "multipoly_unet_params_dict[\"num_intertrack_encoder_layers\"] = 2\n",
    "multipoly_unet_params_dict[\"intertrack_attention_levels\"] = [2,3]\n",
    "multipoly_unet = MultipolyUNet(**multipoly_unet_params_dict)\n",
    "\n",
    "for key in multipoly_unet.state_dict().keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------loading polyffusion weights-------------------\n",
      "input_blocks.7.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.7.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.7.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.7.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.7.2.attention.layers.0.linear1.weight\n",
      "input_blocks.7.2.attention.layers.0.linear1.bias\n",
      "input_blocks.7.2.attention.layers.0.linear2.weight\n",
      "input_blocks.7.2.attention.layers.0.linear2.bias\n",
      "input_blocks.7.2.attention.layers.0.norm1.weight\n",
      "input_blocks.7.2.attention.layers.0.norm1.bias\n",
      "input_blocks.7.2.attention.layers.0.norm2.weight\n",
      "input_blocks.7.2.attention.layers.0.norm2.bias\n",
      "input_blocks.7.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.7.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.7.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.7.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.7.2.attention.layers.1.linear1.weight\n",
      "input_blocks.7.2.attention.layers.1.linear1.bias\n",
      "input_blocks.7.2.attention.layers.1.linear2.weight\n",
      "input_blocks.7.2.attention.layers.1.linear2.bias\n",
      "input_blocks.7.2.attention.layers.1.norm1.weight\n",
      "input_blocks.7.2.attention.layers.1.norm1.bias\n",
      "input_blocks.7.2.attention.layers.1.norm2.weight\n",
      "input_blocks.7.2.attention.layers.1.norm2.bias\n",
      "input_blocks.8.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.8.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.8.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.8.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.8.2.attention.layers.0.linear1.weight\n",
      "input_blocks.8.2.attention.layers.0.linear1.bias\n",
      "input_blocks.8.2.attention.layers.0.linear2.weight\n",
      "input_blocks.8.2.attention.layers.0.linear2.bias\n",
      "input_blocks.8.2.attention.layers.0.norm1.weight\n",
      "input_blocks.8.2.attention.layers.0.norm1.bias\n",
      "input_blocks.8.2.attention.layers.0.norm2.weight\n",
      "input_blocks.8.2.attention.layers.0.norm2.bias\n",
      "input_blocks.8.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.8.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.8.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.8.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.8.2.attention.layers.1.linear1.weight\n",
      "input_blocks.8.2.attention.layers.1.linear1.bias\n",
      "input_blocks.8.2.attention.layers.1.linear2.weight\n",
      "input_blocks.8.2.attention.layers.1.linear2.bias\n",
      "input_blocks.8.2.attention.layers.1.norm1.weight\n",
      "input_blocks.8.2.attention.layers.1.norm1.bias\n",
      "input_blocks.8.2.attention.layers.1.norm2.weight\n",
      "input_blocks.8.2.attention.layers.1.norm2.bias\n",
      "input_blocks.10.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.10.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.10.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.10.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.10.2.attention.layers.0.linear1.weight\n",
      "input_blocks.10.2.attention.layers.0.linear1.bias\n",
      "input_blocks.10.2.attention.layers.0.linear2.weight\n",
      "input_blocks.10.2.attention.layers.0.linear2.bias\n",
      "input_blocks.10.2.attention.layers.0.norm1.weight\n",
      "input_blocks.10.2.attention.layers.0.norm1.bias\n",
      "input_blocks.10.2.attention.layers.0.norm2.weight\n",
      "input_blocks.10.2.attention.layers.0.norm2.bias\n",
      "input_blocks.10.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.10.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.10.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.10.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.10.2.attention.layers.1.linear1.weight\n",
      "input_blocks.10.2.attention.layers.1.linear1.bias\n",
      "input_blocks.10.2.attention.layers.1.linear2.weight\n",
      "input_blocks.10.2.attention.layers.1.linear2.bias\n",
      "input_blocks.10.2.attention.layers.1.norm1.weight\n",
      "input_blocks.10.2.attention.layers.1.norm1.bias\n",
      "input_blocks.10.2.attention.layers.1.norm2.weight\n",
      "input_blocks.10.2.attention.layers.1.norm2.bias\n",
      "input_blocks.11.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.11.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.11.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.11.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.11.2.attention.layers.0.linear1.weight\n",
      "input_blocks.11.2.attention.layers.0.linear1.bias\n",
      "input_blocks.11.2.attention.layers.0.linear2.weight\n",
      "input_blocks.11.2.attention.layers.0.linear2.bias\n",
      "input_blocks.11.2.attention.layers.0.norm1.weight\n",
      "input_blocks.11.2.attention.layers.0.norm1.bias\n",
      "input_blocks.11.2.attention.layers.0.norm2.weight\n",
      "input_blocks.11.2.attention.layers.0.norm2.bias\n",
      "input_blocks.11.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.11.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.11.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.11.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.11.2.attention.layers.1.linear1.weight\n",
      "input_blocks.11.2.attention.layers.1.linear1.bias\n",
      "input_blocks.11.2.attention.layers.1.linear2.weight\n",
      "input_blocks.11.2.attention.layers.1.linear2.bias\n",
      "input_blocks.11.2.attention.layers.1.norm1.weight\n",
      "input_blocks.11.2.attention.layers.1.norm1.bias\n",
      "input_blocks.11.2.attention.layers.1.norm2.weight\n",
      "input_blocks.11.2.attention.layers.1.norm2.bias\n",
      "output_blocks.0.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.0.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.0.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.0.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.0.2.attention.layers.0.linear1.weight\n",
      "output_blocks.0.2.attention.layers.0.linear1.bias\n",
      "output_blocks.0.2.attention.layers.0.linear2.weight\n",
      "output_blocks.0.2.attention.layers.0.linear2.bias\n",
      "output_blocks.0.2.attention.layers.0.norm1.weight\n",
      "output_blocks.0.2.attention.layers.0.norm1.bias\n",
      "output_blocks.0.2.attention.layers.0.norm2.weight\n",
      "output_blocks.0.2.attention.layers.0.norm2.bias\n",
      "output_blocks.0.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.0.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.0.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.0.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.0.2.attention.layers.1.linear1.weight\n",
      "output_blocks.0.2.attention.layers.1.linear1.bias\n",
      "output_blocks.0.2.attention.layers.1.linear2.weight\n",
      "output_blocks.0.2.attention.layers.1.linear2.bias\n",
      "output_blocks.0.2.attention.layers.1.norm1.weight\n",
      "output_blocks.0.2.attention.layers.1.norm1.bias\n",
      "output_blocks.0.2.attention.layers.1.norm2.weight\n",
      "output_blocks.0.2.attention.layers.1.norm2.bias\n",
      "output_blocks.1.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.1.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.1.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.1.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.1.2.attention.layers.0.linear1.weight\n",
      "output_blocks.1.2.attention.layers.0.linear1.bias\n",
      "output_blocks.1.2.attention.layers.0.linear2.weight\n",
      "output_blocks.1.2.attention.layers.0.linear2.bias\n",
      "output_blocks.1.2.attention.layers.0.norm1.weight\n",
      "output_blocks.1.2.attention.layers.0.norm1.bias\n",
      "output_blocks.1.2.attention.layers.0.norm2.weight\n",
      "output_blocks.1.2.attention.layers.0.norm2.bias\n",
      "output_blocks.1.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.1.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.1.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.1.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.1.2.attention.layers.1.linear1.weight\n",
      "output_blocks.1.2.attention.layers.1.linear1.bias\n",
      "output_blocks.1.2.attention.layers.1.linear2.weight\n",
      "output_blocks.1.2.attention.layers.1.linear2.bias\n",
      "output_blocks.1.2.attention.layers.1.norm1.weight\n",
      "output_blocks.1.2.attention.layers.1.norm1.bias\n",
      "output_blocks.1.2.attention.layers.1.norm2.weight\n",
      "output_blocks.1.2.attention.layers.1.norm2.bias\n",
      "output_blocks.2.3.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.2.3.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.2.3.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.2.3.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.2.3.attention.layers.0.linear1.weight\n",
      "output_blocks.2.3.attention.layers.0.linear1.bias\n",
      "output_blocks.2.3.attention.layers.0.linear2.weight\n",
      "output_blocks.2.3.attention.layers.0.linear2.bias\n",
      "output_blocks.2.3.attention.layers.0.norm1.weight\n",
      "output_blocks.2.3.attention.layers.0.norm1.bias\n",
      "output_blocks.2.3.attention.layers.0.norm2.weight\n",
      "output_blocks.2.3.attention.layers.0.norm2.bias\n",
      "output_blocks.2.3.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.2.3.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.2.3.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.2.3.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.2.3.attention.layers.1.linear1.weight\n",
      "output_blocks.2.3.attention.layers.1.linear1.bias\n",
      "output_blocks.2.3.attention.layers.1.linear2.weight\n",
      "output_blocks.2.3.attention.layers.1.linear2.bias\n",
      "output_blocks.2.3.attention.layers.1.norm1.weight\n",
      "output_blocks.2.3.attention.layers.1.norm1.bias\n",
      "output_blocks.2.3.attention.layers.1.norm2.weight\n",
      "output_blocks.2.3.attention.layers.1.norm2.bias\n",
      "output_blocks.3.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.3.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.3.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.3.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.3.2.attention.layers.0.linear1.weight\n",
      "output_blocks.3.2.attention.layers.0.linear1.bias\n",
      "output_blocks.3.2.attention.layers.0.linear2.weight\n",
      "output_blocks.3.2.attention.layers.0.linear2.bias\n",
      "output_blocks.3.2.attention.layers.0.norm1.weight\n",
      "output_blocks.3.2.attention.layers.0.norm1.bias\n",
      "output_blocks.3.2.attention.layers.0.norm2.weight\n",
      "output_blocks.3.2.attention.layers.0.norm2.bias\n",
      "output_blocks.3.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.3.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.3.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.3.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.3.2.attention.layers.1.linear1.weight\n",
      "output_blocks.3.2.attention.layers.1.linear1.bias\n",
      "output_blocks.3.2.attention.layers.1.linear2.weight\n",
      "output_blocks.3.2.attention.layers.1.linear2.bias\n",
      "output_blocks.3.2.attention.layers.1.norm1.weight\n",
      "output_blocks.3.2.attention.layers.1.norm1.bias\n",
      "output_blocks.3.2.attention.layers.1.norm2.weight\n",
      "output_blocks.3.2.attention.layers.1.norm2.bias\n",
      "output_blocks.4.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.4.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.4.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.4.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.4.2.attention.layers.0.linear1.weight\n",
      "output_blocks.4.2.attention.layers.0.linear1.bias\n",
      "output_blocks.4.2.attention.layers.0.linear2.weight\n",
      "output_blocks.4.2.attention.layers.0.linear2.bias\n",
      "output_blocks.4.2.attention.layers.0.norm1.weight\n",
      "output_blocks.4.2.attention.layers.0.norm1.bias\n",
      "output_blocks.4.2.attention.layers.0.norm2.weight\n",
      "output_blocks.4.2.attention.layers.0.norm2.bias\n",
      "output_blocks.4.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.4.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.4.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.4.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.4.2.attention.layers.1.linear1.weight\n",
      "output_blocks.4.2.attention.layers.1.linear1.bias\n",
      "output_blocks.4.2.attention.layers.1.linear2.weight\n",
      "output_blocks.4.2.attention.layers.1.linear2.bias\n",
      "output_blocks.4.2.attention.layers.1.norm1.weight\n",
      "output_blocks.4.2.attention.layers.1.norm1.bias\n",
      "output_blocks.4.2.attention.layers.1.norm2.weight\n",
      "output_blocks.4.2.attention.layers.1.norm2.bias\n",
      "output_blocks.5.3.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.5.3.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.5.3.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.5.3.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.5.3.attention.layers.0.linear1.weight\n",
      "output_blocks.5.3.attention.layers.0.linear1.bias\n",
      "output_blocks.5.3.attention.layers.0.linear2.weight\n",
      "output_blocks.5.3.attention.layers.0.linear2.bias\n",
      "output_blocks.5.3.attention.layers.0.norm1.weight\n",
      "output_blocks.5.3.attention.layers.0.norm1.bias\n",
      "output_blocks.5.3.attention.layers.0.norm2.weight\n",
      "output_blocks.5.3.attention.layers.0.norm2.bias\n",
      "output_blocks.5.3.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.5.3.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.5.3.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.5.3.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.5.3.attention.layers.1.linear1.weight\n",
      "output_blocks.5.3.attention.layers.1.linear1.bias\n",
      "output_blocks.5.3.attention.layers.1.linear2.weight\n",
      "output_blocks.5.3.attention.layers.1.linear2.bias\n",
      "output_blocks.5.3.attention.layers.1.norm1.weight\n",
      "output_blocks.5.3.attention.layers.1.norm1.bias\n",
      "output_blocks.5.3.attention.layers.1.norm2.weight\n",
      "output_blocks.5.3.attention.layers.1.norm2.bias\n",
      "---------------polyffusion weights loaded with the above missing keys-------------------\n",
      "It is expected that missing keys are all intertrack attention modules\n"
     ]
    }
   ],
   "source": [
    "\n",
    "multipoly_unet.load_polyffusion_checkpoints(polyffusion_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.7392,  2.2486,  2.7797,  ..., -1.5133,  0.3401,  1.0701],\n",
      "          [ 1.1331, -0.0945,  0.6112,  ...,  0.5868,  0.1905, -1.2726],\n",
      "          [ 1.2748,  1.2093,  0.6495,  ...,  0.7117, -1.8169, -0.5004],\n",
      "          ...,\n",
      "          [-0.7669, -0.4255,  0.2116,  ..., -1.8107,  0.5774, -0.0803],\n",
      "          [ 0.2443,  1.1582, -1.0554,  ..., -1.1977,  0.3561, -1.0011],\n",
      "          [ 0.4833, -0.3195,  0.5699,  ...,  0.2710,  0.6038, -0.4301]],\n",
      "\n",
      "         [[ 1.6031, -2.6465,  1.9635,  ...,  1.0061,  0.3744,  0.8301],\n",
      "          [ 1.2215, -1.8282, -1.3217,  ..., -0.7328, -2.2711,  0.5652],\n",
      "          [-0.8916, -0.8180,  1.2046,  ...,  3.6916, -3.4847,  1.3160],\n",
      "          ...,\n",
      "          [ 2.9825,  0.0461,  0.2895,  ..., -0.2570,  0.0445, -0.8112],\n",
      "          [ 0.3294, -1.1261,  0.1702,  ..., -2.1505, -3.0178, -1.6419],\n",
      "          [ 0.3454, -0.5245, -0.0265,  ..., -0.9442, -1.3108,  0.7070]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0909, -2.1719,  1.1893,  ..., -0.6459,  0.2964,  1.1713],\n",
      "          [ 0.6948,  1.9504, -0.3875,  ..., -2.4773,  1.9311, -0.2856],\n",
      "          [-0.9531, -0.5485, -0.4240,  ..., -0.6112,  0.0321,  2.0178],\n",
      "          ...,\n",
      "          [ 0.2142,  0.3616,  0.4397,  ...,  0.1284,  2.1490, -1.8909],\n",
      "          [-0.3978,  0.3588, -0.8329,  ...,  0.3450,  2.5964, -0.2544],\n",
      "          [-1.8020, -0.0525,  2.5362,  ..., -0.2137,  2.4602,  0.4934]],\n",
      "\n",
      "         [[ 2.0723,  0.5248, -2.2895,  ...,  0.6468, -1.5343, -2.4727],\n",
      "          [-2.7932,  2.6042,  0.6483,  ..., -2.3850, -0.7393, -1.2846],\n",
      "          [ 1.1633,  0.2663,  2.6843,  ..., -2.3694, -0.6206, -0.8038],\n",
      "          ...,\n",
      "          [ 0.9943, -0.6022, -1.8940,  ..., -0.5347, -1.2634,  0.1268],\n",
      "          [ 0.2179,  0.5387, -0.7851,  ...,  0.9920,  0.8526,  1.9320],\n",
      "          [ 0.8153,  1.2663, -1.2811,  ...,  0.3762,  0.6823, -0.2070]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3216, -1.0621,  2.7080,  ..., -0.3501, -0.3428, -1.3163],\n",
      "          [ 0.0255, -0.0210,  0.8331,  ..., -0.7944, -0.7690, -1.2038],\n",
      "          [-0.5560, -1.4842, -1.4194,  ..., -0.1757, -1.9805,  1.2359],\n",
      "          ...,\n",
      "          [-1.1356, -0.4470, -1.4749,  ..., -0.0820,  0.8461,  0.0570],\n",
      "          [ 1.6029, -1.1519, -1.3315,  ..., -3.3043,  0.1576,  0.3853],\n",
      "          [ 1.2093, -0.8893,  1.3439,  ...,  0.6209,  0.0637,  0.8495]],\n",
      "\n",
      "         [[ 1.8959, -2.0639,  0.3423,  ...,  0.1199, -1.9438, -0.5513],\n",
      "          [ 1.6375, -2.3059, -1.0905,  ..., -3.1118,  0.0642,  1.7890],\n",
      "          [-1.0931,  1.6873,  1.8849,  ..., -1.5807, -1.8472,  0.3919],\n",
      "          ...,\n",
      "          [ 0.3105, -1.5343,  1.4343,  ...,  1.9579,  0.8775, -0.5872],\n",
      "          [ 1.6794, -1.3102,  2.4592,  ..., -0.5429, -1.9258, -0.6735],\n",
      "          [-1.1933, -1.0687, -1.2727,  ...,  0.9901, -0.1339,  2.3183]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.7728,  0.8426,  1.4837,  ...,  2.1032, -0.3409, -0.4078],\n",
      "          [-0.2515,  2.1193,  0.4573,  ...,  0.0242,  0.7888,  2.7844],\n",
      "          [ 1.7972, -0.2075,  0.9854,  ...,  0.1653, -0.0771,  1.9009],\n",
      "          ...,\n",
      "          [ 1.9011,  1.4414,  0.1368,  ...,  0.6635, -0.2114, -1.7345],\n",
      "          [-1.9338,  0.0209, -1.8628,  ...,  0.3948,  1.1089,  0.4160],\n",
      "          [-0.0872,  0.8976, -1.8457,  ...,  0.9158, -0.4243, -1.2643]],\n",
      "\n",
      "         [[-0.2645, -0.8291, -1.6492,  ...,  0.6066,  1.1590,  0.0298],\n",
      "          [-1.2169, -2.2640, -0.8552,  ..., -1.0216,  3.2204,  0.9323],\n",
      "          [ 2.0550,  0.4525,  0.2114,  ...,  0.3435,  0.6144, -1.1756],\n",
      "          ...,\n",
      "          [-0.2058, -0.9268,  2.1802,  ...,  0.0951,  1.1323, -0.8052],\n",
      "          [-0.5870,  1.9251, -1.3417,  ..., -1.7241, -0.4822, -2.5505],\n",
      "          [-0.8829, -0.4507, -1.6438,  ..., -0.8519, -0.4210,  1.4693]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7044,  1.5853,  1.2201,  ...,  0.1986,  0.4693,  2.9107],\n",
      "          [-0.8903, -0.0453, -0.1417,  ..., -0.2672,  0.8142, -0.5776],\n",
      "          [-1.9797,  0.2807,  0.1484,  ..., -1.5076,  1.0992,  1.4772],\n",
      "          ...,\n",
      "          [-0.2495, -0.9988,  1.3168,  ...,  1.0645, -0.8325, -0.2390],\n",
      "          [-0.9832, -0.1566, -3.8116,  ..., -0.9528, -0.1308,  0.3113],\n",
      "          [ 1.6033,  0.0604,  1.5714,  ..., -1.1959, -0.3560, -1.4224]],\n",
      "\n",
      "         [[ 0.0092, -0.3423, -0.0176,  ...,  0.5749, -0.0272, -0.9951],\n",
      "          [ 0.4991, -0.0780,  0.2214,  ..., -0.3276,  2.1717, -3.7154],\n",
      "          [ 1.4844, -0.1787, -0.7804,  ..., -1.0072,  1.3066,  1.3730],\n",
      "          ...,\n",
      "          [ 0.3591, -0.9558, -0.7268,  ...,  2.6602, -2.0460,  1.2306],\n",
      "          [ 1.0701,  1.8176,  0.3744,  ..., -1.5596,  2.1683, -1.5450],\n",
      "          [-0.1977,  1.2277, -0.7027,  ...,  1.2052,  1.4834, -2.3848]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0276, -0.5303, -0.7982,  ...,  1.7287, -1.0016,  0.8457],\n",
      "          [ 1.2474, -0.0077, -0.0616,  ..., -0.5222, -2.7296, -1.0111],\n",
      "          [-1.1207, -1.4149,  1.2544,  ...,  0.2557, -4.9521, -0.4288],\n",
      "          ...,\n",
      "          [-0.7993,  0.7902,  1.3629,  ...,  0.1361,  0.2384,  0.0357],\n",
      "          [ 1.0939, -0.6607, -2.2002,  ...,  0.4333,  1.6512,  0.4353],\n",
      "          [ 0.4492,  0.4289, -0.3788,  ..., -2.3434, -0.2293,  0.2796]],\n",
      "\n",
      "         [[ 1.5360,  0.2762, -1.0429,  ...,  0.4712, -2.3713, -1.4854],\n",
      "          [ 1.4389, -1.4602,  5.2574,  ..., -0.6276,  1.5287,  2.3134],\n",
      "          [ 2.0364, -2.6190, -0.1947,  ..., -1.6598, -2.4364, -0.2882],\n",
      "          ...,\n",
      "          [-0.9184, -0.5270,  2.4058,  ..., -2.7161,  3.0939, -1.0876],\n",
      "          [ 1.5266,  0.5605,  0.9293,  ..., -0.5936, -2.1011,  0.6861],\n",
      "          [ 0.1531,  0.3567, -0.9928,  ..., -0.9614, -1.2945, -0.0929]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "track_num = 5\n",
    "image_width = 128\n",
    "image_height = 128\n",
    "channels = 2\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "input_tensors = torch.randn(batch_size, track_num, channels, image_width, image_height).to(device)\n",
    "cond = torch.randn(batch_size*track_num, 1, 512).to(device)\n",
    "t = torch.randn(batch_size*track_num, ).to(device)\n",
    "\n",
    "polyffusion_unet = polyffusion_unet.to(device)\n",
    "polyffusion_unet.eval()\n",
    "with torch.no_grad():\n",
    "    poly_output = polyffusion_unet(input_tensors.reshape(batch_size*track_num, channels, image_width, image_height),t,  cond)\n",
    "print(poly_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[-0.7391,  2.2486,  2.7801,  ..., -1.5134,  0.3401,  1.0701],\n",
      "           [ 1.1331, -0.0945,  0.6112,  ...,  0.5868,  0.1905, -1.2726],\n",
      "           [ 1.2747,  1.2093,  0.6495,  ...,  0.7117, -1.8170, -0.5004],\n",
      "           ...,\n",
      "           [-0.7669, -0.4255,  0.2115,  ..., -1.8109,  0.5775, -0.0803],\n",
      "           [ 0.2443,  1.1580, -1.0554,  ..., -1.1976,  0.3561, -1.0011],\n",
      "           [ 0.4834, -0.3196,  0.5699,  ...,  0.2710,  0.6037, -0.4301]],\n",
      "\n",
      "          [[ 1.6031, -2.6464,  1.9635,  ...,  1.0061,  0.3743,  0.8301],\n",
      "           [ 1.2215, -1.8284, -1.3217,  ..., -0.7329, -2.2711,  0.5652],\n",
      "           [-0.8916, -0.8181,  1.2046,  ...,  3.6917, -3.4847,  1.3160],\n",
      "           ...,\n",
      "           [ 2.9826,  0.0460,  0.2895,  ..., -0.2569,  0.0445, -0.8111],\n",
      "           [ 0.3295, -1.1262,  0.1702,  ..., -2.1505, -3.0178, -1.6420],\n",
      "           [ 0.3454, -0.5245, -0.0265,  ..., -0.9442, -1.3108,  0.7070]]],\n",
      "\n",
      "\n",
      "         [[[ 2.0909, -2.1720,  1.1893,  ..., -0.6460,  0.2965,  1.1714],\n",
      "           [ 0.6949,  1.9506, -0.3874,  ..., -2.4773,  1.9314, -0.2856],\n",
      "           [-0.9532, -0.5485, -0.4240,  ..., -0.6112,  0.0321,  2.0180],\n",
      "           ...,\n",
      "           [ 0.2142,  0.3616,  0.4397,  ...,  0.1284,  2.1488, -1.8909],\n",
      "           [-0.3978,  0.3589, -0.8330,  ...,  0.3450,  2.5965, -0.2543],\n",
      "           [-1.8019, -0.0524,  2.5360,  ..., -0.2138,  2.4599,  0.4934]],\n",
      "\n",
      "          [[ 2.0724,  0.5248, -2.2894,  ...,  0.6468, -1.5343, -2.4727],\n",
      "           [-2.7932,  2.6043,  0.6484,  ..., -2.3850, -0.7394, -1.2846],\n",
      "           [ 1.1635,  0.2663,  2.6844,  ..., -2.3694, -0.6208, -0.8036],\n",
      "           ...,\n",
      "           [ 0.9944, -0.6023, -1.8940,  ..., -0.5347, -1.2634,  0.1268],\n",
      "           [ 0.2180,  0.5387, -0.7852,  ...,  0.9920,  0.8527,  1.9320],\n",
      "           [ 0.8153,  1.2663, -1.2811,  ...,  0.3762,  0.6824, -0.2070]]],\n",
      "\n",
      "\n",
      "         [[[ 0.3216, -1.0621,  2.7080,  ..., -0.3501, -0.3429, -1.3162],\n",
      "           [ 0.0255, -0.0211,  0.8332,  ..., -0.7943, -0.7690, -1.2039],\n",
      "           [-0.5559, -1.4841, -1.4193,  ..., -0.1758, -1.9804,  1.2359],\n",
      "           ...,\n",
      "           [-1.1357, -0.4469, -1.4749,  ..., -0.0820,  0.8462,  0.0569],\n",
      "           [ 1.6029, -1.1519, -1.3314,  ..., -3.3043,  0.1576,  0.3852],\n",
      "           [ 1.2093, -0.8894,  1.3439,  ...,  0.6209,  0.0637,  0.8495]],\n",
      "\n",
      "          [[ 1.8959, -2.0638,  0.3423,  ...,  0.1199, -1.9437, -0.5513],\n",
      "           [ 1.6375, -2.3059, -1.0906,  ..., -3.1119,  0.0643,  1.7890],\n",
      "           [-1.0931,  1.6872,  1.8849,  ..., -1.5807, -1.8472,  0.3919],\n",
      "           ...,\n",
      "           [ 0.3104, -1.5344,  1.4343,  ...,  1.9579,  0.8776, -0.5872],\n",
      "           [ 1.6793, -1.3101,  2.4593,  ..., -0.5428, -1.9258, -0.6735],\n",
      "           [-1.1933, -1.0687, -1.2727,  ...,  0.9901, -0.1339,  2.3183]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7725,  1.7087,  0.4178,  ...,  1.7160, -0.4290, -0.4190],\n",
      "           [ 0.1061,  0.7600, -1.1173,  ..., -0.8087,  1.4105,  2.1414],\n",
      "           [-1.9277, -2.8350,  2.4557,  ...,  0.1166, -0.7749, -2.3584],\n",
      "           ...,\n",
      "           [-3.0160,  0.3782, -0.9693,  ..., -2.3489, -2.1741,  0.2811],\n",
      "           [ 0.2207, -1.3607,  1.0580,  ..., -1.1860,  0.7334,  1.7038],\n",
      "           [ 0.2416, -1.4872, -0.3797,  ..., -1.5466, -0.1127,  0.5802]],\n",
      "\n",
      "          [[ 2.0911, -1.7349, -0.4259,  ...,  1.2794,  0.6346,  0.4070],\n",
      "           [-1.2967, -0.3302,  0.0514,  ..., -1.7185, -0.7124, -0.4297],\n",
      "           [ 0.7166, -0.7599,  1.1006,  ...,  1.5870, -0.1527, -2.8418],\n",
      "           ...,\n",
      "           [-0.4315,  1.8642, -2.3184,  ...,  0.6268, -1.7263,  0.2491],\n",
      "           [-0.4025, -0.7191,  2.2277,  ..., -1.1120,  1.6398,  1.1287],\n",
      "           [-0.1713,  0.7666,  0.0362,  ...,  1.1095, -2.5709, -0.1832]]],\n",
      "\n",
      "\n",
      "         [[[ 1.0431, -0.0559,  0.0870,  ..., -0.5176, -0.1126,  0.8503],\n",
      "           [ 1.5737,  2.3576,  2.2703,  ...,  0.6032, -1.6638,  2.3276],\n",
      "           [-0.8100,  0.9253,  0.7581,  ..., -0.5711,  1.8322, -0.8674],\n",
      "           ...,\n",
      "           [-0.7057, -1.2341,  1.0032,  ...,  0.7716, -1.6471, -0.6107],\n",
      "           [ 1.2543, -2.6063,  1.3647,  ..., -1.1182,  1.4977, -0.5333],\n",
      "           [-1.4225, -1.2298,  4.3401,  ..., -0.2698, -1.8321,  0.1220]],\n",
      "\n",
      "          [[ 0.4782,  0.8484,  0.0077,  ..., -1.1292,  1.5736, -0.2635],\n",
      "           [-0.6329, -0.6765,  0.0221,  ..., -0.4662, -4.3193, -0.2386],\n",
      "           [ 0.1737,  0.9321,  0.6443,  ..., -2.7296, -0.5869, -2.5728],\n",
      "           ...,\n",
      "           [-1.3836,  0.0971,  0.5942,  ...,  0.0941, -0.7957, -1.3535],\n",
      "           [-0.4680,  0.4076, -0.2748,  ...,  3.2888,  1.0509,  0.1179],\n",
      "           [ 0.3992,  3.3436, -3.6165,  ..., -1.2162, -0.5817,  0.2591]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.5065, -1.3524,  1.6372,  ...,  0.2992,  3.1862,  0.5900],\n",
      "           [-0.2599,  2.1798, -2.1230,  ..., -0.6166,  3.8434, -0.8274],\n",
      "           [ 0.8945,  1.4198,  1.1554,  ..., -0.6144, -1.7295,  0.1591],\n",
      "           ...,\n",
      "           [-0.1316,  0.4910,  1.1742,  ..., -1.0926,  0.4765,  1.4114],\n",
      "           [ 0.5808,  0.5457,  0.4412,  ..., -0.2221, -1.8536, -0.6241],\n",
      "           [-0.3519,  0.7226, -0.2509,  ...,  1.8857,  1.0242,  0.8855]],\n",
      "\n",
      "          [[-0.2026,  1.1379, -0.0156,  ..., -0.5036, -2.5945,  0.2053],\n",
      "           [-0.7540, -1.5355,  0.4884,  ..., -1.1288, -3.2966,  0.2359],\n",
      "           [ 2.2927, -3.4164, -3.0828,  ...,  3.9320, -0.9465, -0.6802],\n",
      "           ...,\n",
      "           [-1.8043, -2.9505, -0.5704,  ...,  0.6679,  1.3253, -0.2796],\n",
      "           [-0.5419, -0.4127, -0.5201,  ...,  1.6344, -0.5982, -1.2591],\n",
      "           [-0.0492,  0.3031,  2.0755,  ...,  1.1045, -1.2892,  0.4672]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7581,  2.1366, -1.1929,  ...,  0.1824, -1.8223, -0.1449],\n",
      "           [-2.6232, -2.4061,  0.4913,  ...,  0.4113, -0.9278,  0.8968],\n",
      "           [ 0.3260, -0.4110,  1.7775,  ..., -0.0358, -1.7337,  1.0164],\n",
      "           ...,\n",
      "           [-0.5379, -2.4579,  1.1513,  ..., -1.1383,  1.3735,  0.5356],\n",
      "           [ 1.0330,  0.2183,  2.5274,  ...,  0.7832,  0.3997, -0.2873],\n",
      "           [ 1.5604, -1.2270, -2.0714,  ...,  0.0168,  0.0639,  1.4193]],\n",
      "\n",
      "          [[-0.3953, -0.3239,  0.6358,  ..., -0.7408, -0.3145, -0.5964],\n",
      "           [-2.1967, -1.3199,  0.2000,  ...,  0.3885, -1.0206, -1.4057],\n",
      "           [-2.7958, -0.9400,  1.0921,  ...,  0.5521,  0.3171,  0.0444],\n",
      "           ...,\n",
      "           [-2.1497,  0.8289,  2.9116,  ...,  0.7582, -0.1393,  0.5527],\n",
      "           [-0.6655, -3.1071,  2.7531,  ..., -0.2569,  0.3213,  1.1176],\n",
      "           [-0.6513,  0.7247,  1.5377,  ...,  0.8696,  0.6043, -0.4213]]],\n",
      "\n",
      "\n",
      "         [[[ 1.7727,  0.8427,  1.4836,  ...,  2.1031, -0.3408, -0.4078],\n",
      "           [-0.2515,  2.1193,  0.4575,  ...,  0.0242,  0.7888,  2.7843],\n",
      "           [ 1.7972, -0.2075,  0.9850,  ...,  0.1653, -0.0771,  1.9010],\n",
      "           ...,\n",
      "           [ 1.9014,  1.4414,  0.1368,  ...,  0.6634, -0.2114, -1.7346],\n",
      "           [-1.9338,  0.0208, -1.8628,  ...,  0.3948,  1.1087,  0.4161],\n",
      "           [-0.0872,  0.8977, -1.8458,  ...,  0.9158, -0.4243, -1.2643]],\n",
      "\n",
      "          [[-0.2645, -0.8290, -1.6491,  ...,  0.6066,  1.1590,  0.0298],\n",
      "           [-1.2169, -2.2640, -0.8552,  ..., -1.0216,  3.2206,  0.9322],\n",
      "           [ 2.0550,  0.4525,  0.2115,  ...,  0.3434,  0.6146, -1.1755],\n",
      "           ...,\n",
      "           [-0.2060, -0.9268,  2.1802,  ...,  0.0950,  1.1323, -0.8052],\n",
      "           [-0.5870,  1.9250, -1.3418,  ..., -1.7240, -0.4821, -2.5505],\n",
      "           [-0.8829, -0.4508, -1.6438,  ..., -0.8519, -0.4209,  1.4693]]],\n",
      "\n",
      "\n",
      "         [[[ 1.7043,  1.5853,  1.2201,  ...,  0.1986,  0.4695,  2.9107],\n",
      "           [-0.8903, -0.0454, -0.1417,  ..., -0.2672,  0.8143, -0.5777],\n",
      "           [-1.9797,  0.2805,  0.1484,  ..., -1.5075,  1.0990,  1.4772],\n",
      "           ...,\n",
      "           [-0.2495, -0.9988,  1.3169,  ...,  1.0645, -0.8325, -0.2390],\n",
      "           [-0.9833, -0.1565, -3.8118,  ..., -0.9527, -0.1308,  0.3113],\n",
      "           [ 1.6033,  0.0603,  1.5715,  ..., -1.1959, -0.3559, -1.4224]],\n",
      "\n",
      "          [[ 0.0092, -0.3423, -0.0175,  ...,  0.5750, -0.0273, -0.9951],\n",
      "           [ 0.4991, -0.0780,  0.2213,  ..., -0.3276,  2.1719, -3.7154],\n",
      "           [ 1.4844, -0.1787, -0.7804,  ..., -1.0073,  1.3069,  1.3729],\n",
      "           ...,\n",
      "           [ 0.3591, -0.9558, -0.7268,  ...,  2.6602, -2.0460,  1.2306],\n",
      "           [ 1.0701,  1.8176,  0.3744,  ..., -1.5597,  2.1683, -1.5451],\n",
      "           [-0.1977,  1.2278, -0.7026,  ...,  1.2052,  1.4835, -2.3848]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0276, -0.5303, -0.7982,  ...,  1.7287, -1.0016,  0.8458],\n",
      "           [ 1.2474, -0.0077, -0.0615,  ..., -0.5223, -2.7296, -1.0111],\n",
      "           [-1.1207, -1.4149,  1.2544,  ...,  0.2557, -4.9519, -0.4289],\n",
      "           ...,\n",
      "           [-0.7993,  0.7902,  1.3629,  ...,  0.1360,  0.2384,  0.0357],\n",
      "           [ 1.0939, -0.6607, -2.2004,  ...,  0.4333,  1.6513,  0.4353],\n",
      "           [ 0.4492,  0.4289, -0.3788,  ..., -2.3434, -0.2293,  0.2796]],\n",
      "\n",
      "          [[ 1.5358,  0.2763, -1.0430,  ...,  0.4711, -2.3713, -1.4854],\n",
      "           [ 1.4390, -1.4601,  5.2573,  ..., -0.6276,  1.5288,  2.3135],\n",
      "           [ 2.0363, -2.6191, -0.1946,  ..., -1.6599, -2.4365, -0.2883],\n",
      "           ...,\n",
      "           [-0.9183, -0.5271,  2.4058,  ..., -2.7162,  3.0936, -1.0876],\n",
      "           [ 1.5267,  0.5605,  0.9294,  ..., -0.5936, -2.1010,  0.6860],\n",
      "           [ 0.1531,  0.3567, -0.9928,  ..., -0.9614, -1.2945, -0.0929]]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "multipoly_unet = multipoly_unet.to(device)\n",
    "multipoly_unet.eval()\n",
    "with torch.no_grad():\n",
    "    multi_output = multipoly_unet(input_tensors, t, cond)\n",
    "print(multi_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8938e-05, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "poly_output = poly_output.flatten()\n",
    "multi_output = multi_output.flatten()\n",
    "avg_diff = (poly_output-multi_output).abs().mean()\n",
    "print(avg_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
