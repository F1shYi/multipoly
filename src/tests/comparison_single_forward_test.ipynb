{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name : sdf_chd8bar\n",
      "batch_size : 16\n",
      "max_epoch : 100\n",
      "learning_rate : 5e-05\n",
      "max_grad_norm : 10\n",
      "fp16 : True\n",
      "num_workers : 4\n",
      "pin_memory : True\n",
      "in_channels : 2\n",
      "out_channels : 2\n",
      "channels : 64\n",
      "attention_levels : [2, 3]\n",
      "n_res_blocks : 2\n",
      "channel_multipliers : [1, 2, 4, 4]\n",
      "n_heads : 4\n",
      "tf_layers : 1\n",
      "d_cond : 512\n",
      "linear_start : 0.00085\n",
      "linear_end : 0.012\n",
      "n_steps : 1000\n",
      "latent_scaling_factor : 0.18215\n",
      "img_h : 128\n",
      "img_w : 128\n",
      "cond_type : chord\n",
      "cond_mode : mix\n",
      "use_enc : True\n",
      "chd_n_step : 32\n",
      "chd_input_dim : 36\n",
      "chd_z_input_dim : 512\n",
      "chd_hidden_dim : 512\n",
      "chd_z_dim : 512\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "MULTIPOLY_FOLDER = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "POLYFFUSION_CKPT_PATH = os.path.join(MULTIPOLY_FOLDER, r\"polyffusion_ckpts\\ldm_chd8bar\\sdf+pop909wm_mix16_chd8bar\\01-11_102022\\chkpts\\weights_best.pt\")\n",
    "POLYFFUSION_PARAMS_PATH = os.path.join(MULTIPOLY_FOLDER, r\"polyffusion_ckpts\\ldm_chd8bar\\sdf+pop909wm_mix16_chd8bar\\01-11_102022\\params.yaml\")\n",
    "CHORD_CKPT_PATH = os.path.join(MULTIPOLY_FOLDER, r\"pretrained\\chd8bar\\weights.pt\")\n",
    "\n",
    "with open(POLYFFUSION_PARAMS_PATH, 'r') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "for key,value in params.items():\n",
    "    print(key,\":\",value)\n",
    "\n",
    "\n",
    "polyffusion_checkpoint = torch.load(POLYFFUSION_CKPT_PATH)[\"model\"]\n",
    "chord_checkpoint = torch.load(CHORD_CKPT_PATH)[\"model\"]\n",
    "\n",
    "sys.path.append(MULTIPOLY_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define models according to the settings in `polyffusion_ckpts\\...\\params.yaml`\n",
    "from polyffusion.dl_modules import ChordEncoder, ChordDecoder\n",
    "from polyffusion.stable_diffusion.model.unet import UNetModel as PolyffusionUNet\n",
    "from src.models.unet import UNetModel as MultipolyUNet\n",
    "\n",
    "import inspect\n",
    "\n",
    "chord_enc_params = inspect.signature(ChordEncoder.__init__).parameters\n",
    "chord_enc_params_dict = {key.removeprefix(\"chd_\"):params[key] for key in params if key.removeprefix(\"chd_\") in chord_enc_params}\n",
    "chord_encoder = ChordEncoder(**chord_enc_params_dict)\n",
    "CHORD_ENC_PREFIX = \"chord_enc.\"\n",
    "chord_enc_state_dict = {key.removeprefix(CHORD_ENC_PREFIX):value for key,value in chord_checkpoint.items() if key.startswith(CHORD_ENC_PREFIX)}\n",
    "chord_encoder.load_state_dict(chord_enc_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chord_dec_params = inspect.signature(ChordDecoder.__init__).parameters\n",
    "chord_dec_params_dict = {key.removeprefix(\"chd_\"):params[key] for key in params if key.removeprefix(\"chd_\") in chord_dec_params}\n",
    "chord_decoder = ChordDecoder(**chord_dec_params_dict)\n",
    "CHORD_DEC_PREFIX = \"chord_dec.\"\n",
    "chord_dec_state_dict = {key.removeprefix(CHORD_DEC_PREFIX):value for key,value in chord_checkpoint.items() if key.startswith(CHORD_DEC_PREFIX)}\n",
    "chord_decoder.load_state_dict(chord_dec_state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_embed.0.weight\n",
      "time_embed.0.bias\n",
      "time_embed.2.weight\n",
      "time_embed.2.bias\n",
      "input_blocks.0.0.weight\n",
      "input_blocks.0.0.bias\n",
      "input_blocks.1.0.in_layers.0.weight\n",
      "input_blocks.1.0.in_layers.0.bias\n",
      "input_blocks.1.0.in_layers.2.weight\n",
      "input_blocks.1.0.in_layers.2.bias\n",
      "input_blocks.1.0.emb_layers.1.weight\n",
      "input_blocks.1.0.emb_layers.1.bias\n",
      "input_blocks.1.0.out_layers.0.weight\n",
      "input_blocks.1.0.out_layers.0.bias\n",
      "input_blocks.1.0.out_layers.3.weight\n",
      "input_blocks.1.0.out_layers.3.bias\n",
      "input_blocks.2.0.in_layers.0.weight\n",
      "input_blocks.2.0.in_layers.0.bias\n",
      "input_blocks.2.0.in_layers.2.weight\n",
      "input_blocks.2.0.in_layers.2.bias\n",
      "input_blocks.2.0.emb_layers.1.weight\n",
      "input_blocks.2.0.emb_layers.1.bias\n",
      "input_blocks.2.0.out_layers.0.weight\n",
      "input_blocks.2.0.out_layers.0.bias\n",
      "input_blocks.2.0.out_layers.3.weight\n",
      "input_blocks.2.0.out_layers.3.bias\n",
      "input_blocks.3.0.op.weight\n",
      "input_blocks.3.0.op.bias\n",
      "input_blocks.4.0.in_layers.0.weight\n",
      "input_blocks.4.0.in_layers.0.bias\n",
      "input_blocks.4.0.in_layers.2.weight\n",
      "input_blocks.4.0.in_layers.2.bias\n",
      "input_blocks.4.0.emb_layers.1.weight\n",
      "input_blocks.4.0.emb_layers.1.bias\n",
      "input_blocks.4.0.out_layers.0.weight\n",
      "input_blocks.4.0.out_layers.0.bias\n",
      "input_blocks.4.0.out_layers.3.weight\n",
      "input_blocks.4.0.out_layers.3.bias\n",
      "input_blocks.4.0.skip_connection.weight\n",
      "input_blocks.4.0.skip_connection.bias\n",
      "input_blocks.5.0.in_layers.0.weight\n",
      "input_blocks.5.0.in_layers.0.bias\n",
      "input_blocks.5.0.in_layers.2.weight\n",
      "input_blocks.5.0.in_layers.2.bias\n",
      "input_blocks.5.0.emb_layers.1.weight\n",
      "input_blocks.5.0.emb_layers.1.bias\n",
      "input_blocks.5.0.out_layers.0.weight\n",
      "input_blocks.5.0.out_layers.0.bias\n",
      "input_blocks.5.0.out_layers.3.weight\n",
      "input_blocks.5.0.out_layers.3.bias\n",
      "input_blocks.6.0.op.weight\n",
      "input_blocks.6.0.op.bias\n",
      "input_blocks.7.0.in_layers.0.weight\n",
      "input_blocks.7.0.in_layers.0.bias\n",
      "input_blocks.7.0.in_layers.2.weight\n",
      "input_blocks.7.0.in_layers.2.bias\n",
      "input_blocks.7.0.emb_layers.1.weight\n",
      "input_blocks.7.0.emb_layers.1.bias\n",
      "input_blocks.7.0.out_layers.0.weight\n",
      "input_blocks.7.0.out_layers.0.bias\n",
      "input_blocks.7.0.out_layers.3.weight\n",
      "input_blocks.7.0.out_layers.3.bias\n",
      "input_blocks.7.0.skip_connection.weight\n",
      "input_blocks.7.0.skip_connection.bias\n",
      "input_blocks.7.1.norm.weight\n",
      "input_blocks.7.1.norm.bias\n",
      "input_blocks.7.1.proj_in.weight\n",
      "input_blocks.7.1.proj_in.bias\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.7.1.proj_out.weight\n",
      "input_blocks.7.1.proj_out.bias\n",
      "input_blocks.8.0.in_layers.0.weight\n",
      "input_blocks.8.0.in_layers.0.bias\n",
      "input_blocks.8.0.in_layers.2.weight\n",
      "input_blocks.8.0.in_layers.2.bias\n",
      "input_blocks.8.0.emb_layers.1.weight\n",
      "input_blocks.8.0.emb_layers.1.bias\n",
      "input_blocks.8.0.out_layers.0.weight\n",
      "input_blocks.8.0.out_layers.0.bias\n",
      "input_blocks.8.0.out_layers.3.weight\n",
      "input_blocks.8.0.out_layers.3.bias\n",
      "input_blocks.8.1.norm.weight\n",
      "input_blocks.8.1.norm.bias\n",
      "input_blocks.8.1.proj_in.weight\n",
      "input_blocks.8.1.proj_in.bias\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.8.1.proj_out.weight\n",
      "input_blocks.8.1.proj_out.bias\n",
      "input_blocks.9.0.op.weight\n",
      "input_blocks.9.0.op.bias\n",
      "input_blocks.10.0.in_layers.0.weight\n",
      "input_blocks.10.0.in_layers.0.bias\n",
      "input_blocks.10.0.in_layers.2.weight\n",
      "input_blocks.10.0.in_layers.2.bias\n",
      "input_blocks.10.0.emb_layers.1.weight\n",
      "input_blocks.10.0.emb_layers.1.bias\n",
      "input_blocks.10.0.out_layers.0.weight\n",
      "input_blocks.10.0.out_layers.0.bias\n",
      "input_blocks.10.0.out_layers.3.weight\n",
      "input_blocks.10.0.out_layers.3.bias\n",
      "input_blocks.10.1.norm.weight\n",
      "input_blocks.10.1.norm.bias\n",
      "input_blocks.10.1.proj_in.weight\n",
      "input_blocks.10.1.proj_in.bias\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.10.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.10.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.10.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.10.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.10.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.10.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.10.1.proj_out.weight\n",
      "input_blocks.10.1.proj_out.bias\n",
      "input_blocks.11.0.in_layers.0.weight\n",
      "input_blocks.11.0.in_layers.0.bias\n",
      "input_blocks.11.0.in_layers.2.weight\n",
      "input_blocks.11.0.in_layers.2.bias\n",
      "input_blocks.11.0.emb_layers.1.weight\n",
      "input_blocks.11.0.emb_layers.1.bias\n",
      "input_blocks.11.0.out_layers.0.weight\n",
      "input_blocks.11.0.out_layers.0.bias\n",
      "input_blocks.11.0.out_layers.3.weight\n",
      "input_blocks.11.0.out_layers.3.bias\n",
      "input_blocks.11.1.norm.weight\n",
      "input_blocks.11.1.norm.bias\n",
      "input_blocks.11.1.proj_in.weight\n",
      "input_blocks.11.1.proj_in.bias\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.11.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.11.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.11.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.11.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.11.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.11.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.11.1.proj_out.weight\n",
      "input_blocks.11.1.proj_out.bias\n",
      "middle_block.0.in_layers.0.weight\n",
      "middle_block.0.in_layers.0.bias\n",
      "middle_block.0.in_layers.2.weight\n",
      "middle_block.0.in_layers.2.bias\n",
      "middle_block.0.emb_layers.1.weight\n",
      "middle_block.0.emb_layers.1.bias\n",
      "middle_block.0.out_layers.0.weight\n",
      "middle_block.0.out_layers.0.bias\n",
      "middle_block.0.out_layers.3.weight\n",
      "middle_block.0.out_layers.3.bias\n",
      "middle_block.1.norm.weight\n",
      "middle_block.1.norm.bias\n",
      "middle_block.1.proj_in.weight\n",
      "middle_block.1.proj_in.bias\n",
      "middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "middle_block.1.transformer_blocks.0.norm1.weight\n",
      "middle_block.1.transformer_blocks.0.norm1.bias\n",
      "middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "middle_block.1.transformer_blocks.0.norm2.weight\n",
      "middle_block.1.transformer_blocks.0.norm2.bias\n",
      "middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "middle_block.1.transformer_blocks.0.norm3.weight\n",
      "middle_block.1.transformer_blocks.0.norm3.bias\n",
      "middle_block.1.proj_out.weight\n",
      "middle_block.1.proj_out.bias\n",
      "middle_block.2.in_layers.0.weight\n",
      "middle_block.2.in_layers.0.bias\n",
      "middle_block.2.in_layers.2.weight\n",
      "middle_block.2.in_layers.2.bias\n",
      "middle_block.2.emb_layers.1.weight\n",
      "middle_block.2.emb_layers.1.bias\n",
      "middle_block.2.out_layers.0.weight\n",
      "middle_block.2.out_layers.0.bias\n",
      "middle_block.2.out_layers.3.weight\n",
      "middle_block.2.out_layers.3.bias\n",
      "output_blocks.0.0.in_layers.0.weight\n",
      "output_blocks.0.0.in_layers.0.bias\n",
      "output_blocks.0.0.in_layers.2.weight\n",
      "output_blocks.0.0.in_layers.2.bias\n",
      "output_blocks.0.0.emb_layers.1.weight\n",
      "output_blocks.0.0.emb_layers.1.bias\n",
      "output_blocks.0.0.out_layers.0.weight\n",
      "output_blocks.0.0.out_layers.0.bias\n",
      "output_blocks.0.0.out_layers.3.weight\n",
      "output_blocks.0.0.out_layers.3.bias\n",
      "output_blocks.0.0.skip_connection.weight\n",
      "output_blocks.0.0.skip_connection.bias\n",
      "output_blocks.0.1.norm.weight\n",
      "output_blocks.0.1.norm.bias\n",
      "output_blocks.0.1.proj_in.weight\n",
      "output_blocks.0.1.proj_in.bias\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.0.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.0.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.0.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.0.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.0.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.0.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.0.1.proj_out.weight\n",
      "output_blocks.0.1.proj_out.bias\n",
      "output_blocks.1.0.in_layers.0.weight\n",
      "output_blocks.1.0.in_layers.0.bias\n",
      "output_blocks.1.0.in_layers.2.weight\n",
      "output_blocks.1.0.in_layers.2.bias\n",
      "output_blocks.1.0.emb_layers.1.weight\n",
      "output_blocks.1.0.emb_layers.1.bias\n",
      "output_blocks.1.0.out_layers.0.weight\n",
      "output_blocks.1.0.out_layers.0.bias\n",
      "output_blocks.1.0.out_layers.3.weight\n",
      "output_blocks.1.0.out_layers.3.bias\n",
      "output_blocks.1.0.skip_connection.weight\n",
      "output_blocks.1.0.skip_connection.bias\n",
      "output_blocks.1.1.norm.weight\n",
      "output_blocks.1.1.norm.bias\n",
      "output_blocks.1.1.proj_in.weight\n",
      "output_blocks.1.1.proj_in.bias\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.1.1.proj_out.weight\n",
      "output_blocks.1.1.proj_out.bias\n",
      "output_blocks.2.0.in_layers.0.weight\n",
      "output_blocks.2.0.in_layers.0.bias\n",
      "output_blocks.2.0.in_layers.2.weight\n",
      "output_blocks.2.0.in_layers.2.bias\n",
      "output_blocks.2.0.emb_layers.1.weight\n",
      "output_blocks.2.0.emb_layers.1.bias\n",
      "output_blocks.2.0.out_layers.0.weight\n",
      "output_blocks.2.0.out_layers.0.bias\n",
      "output_blocks.2.0.out_layers.3.weight\n",
      "output_blocks.2.0.out_layers.3.bias\n",
      "output_blocks.2.0.skip_connection.weight\n",
      "output_blocks.2.0.skip_connection.bias\n",
      "output_blocks.2.1.norm.weight\n",
      "output_blocks.2.1.norm.bias\n",
      "output_blocks.2.1.proj_in.weight\n",
      "output_blocks.2.1.proj_in.bias\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.2.1.proj_out.weight\n",
      "output_blocks.2.1.proj_out.bias\n",
      "output_blocks.2.2.conv.weight\n",
      "output_blocks.2.2.conv.bias\n",
      "output_blocks.3.0.in_layers.0.weight\n",
      "output_blocks.3.0.in_layers.0.bias\n",
      "output_blocks.3.0.in_layers.2.weight\n",
      "output_blocks.3.0.in_layers.2.bias\n",
      "output_blocks.3.0.emb_layers.1.weight\n",
      "output_blocks.3.0.emb_layers.1.bias\n",
      "output_blocks.3.0.out_layers.0.weight\n",
      "output_blocks.3.0.out_layers.0.bias\n",
      "output_blocks.3.0.out_layers.3.weight\n",
      "output_blocks.3.0.out_layers.3.bias\n",
      "output_blocks.3.0.skip_connection.weight\n",
      "output_blocks.3.0.skip_connection.bias\n",
      "output_blocks.3.1.norm.weight\n",
      "output_blocks.3.1.norm.bias\n",
      "output_blocks.3.1.proj_in.weight\n",
      "output_blocks.3.1.proj_in.bias\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.3.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.3.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.3.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.3.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.3.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.3.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.3.1.proj_out.weight\n",
      "output_blocks.3.1.proj_out.bias\n",
      "output_blocks.4.0.in_layers.0.weight\n",
      "output_blocks.4.0.in_layers.0.bias\n",
      "output_blocks.4.0.in_layers.2.weight\n",
      "output_blocks.4.0.in_layers.2.bias\n",
      "output_blocks.4.0.emb_layers.1.weight\n",
      "output_blocks.4.0.emb_layers.1.bias\n",
      "output_blocks.4.0.out_layers.0.weight\n",
      "output_blocks.4.0.out_layers.0.bias\n",
      "output_blocks.4.0.out_layers.3.weight\n",
      "output_blocks.4.0.out_layers.3.bias\n",
      "output_blocks.4.0.skip_connection.weight\n",
      "output_blocks.4.0.skip_connection.bias\n",
      "output_blocks.4.1.norm.weight\n",
      "output_blocks.4.1.norm.bias\n",
      "output_blocks.4.1.proj_in.weight\n",
      "output_blocks.4.1.proj_in.bias\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.4.1.proj_out.weight\n",
      "output_blocks.4.1.proj_out.bias\n",
      "output_blocks.5.0.in_layers.0.weight\n",
      "output_blocks.5.0.in_layers.0.bias\n",
      "output_blocks.5.0.in_layers.2.weight\n",
      "output_blocks.5.0.in_layers.2.bias\n",
      "output_blocks.5.0.emb_layers.1.weight\n",
      "output_blocks.5.0.emb_layers.1.bias\n",
      "output_blocks.5.0.out_layers.0.weight\n",
      "output_blocks.5.0.out_layers.0.bias\n",
      "output_blocks.5.0.out_layers.3.weight\n",
      "output_blocks.5.0.out_layers.3.bias\n",
      "output_blocks.5.0.skip_connection.weight\n",
      "output_blocks.5.0.skip_connection.bias\n",
      "output_blocks.5.1.norm.weight\n",
      "output_blocks.5.1.norm.bias\n",
      "output_blocks.5.1.proj_in.weight\n",
      "output_blocks.5.1.proj_in.bias\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.5.1.proj_out.weight\n",
      "output_blocks.5.1.proj_out.bias\n",
      "output_blocks.5.2.conv.weight\n",
      "output_blocks.5.2.conv.bias\n",
      "output_blocks.6.0.in_layers.0.weight\n",
      "output_blocks.6.0.in_layers.0.bias\n",
      "output_blocks.6.0.in_layers.2.weight\n",
      "output_blocks.6.0.in_layers.2.bias\n",
      "output_blocks.6.0.emb_layers.1.weight\n",
      "output_blocks.6.0.emb_layers.1.bias\n",
      "output_blocks.6.0.out_layers.0.weight\n",
      "output_blocks.6.0.out_layers.0.bias\n",
      "output_blocks.6.0.out_layers.3.weight\n",
      "output_blocks.6.0.out_layers.3.bias\n",
      "output_blocks.6.0.skip_connection.weight\n",
      "output_blocks.6.0.skip_connection.bias\n",
      "output_blocks.7.0.in_layers.0.weight\n",
      "output_blocks.7.0.in_layers.0.bias\n",
      "output_blocks.7.0.in_layers.2.weight\n",
      "output_blocks.7.0.in_layers.2.bias\n",
      "output_blocks.7.0.emb_layers.1.weight\n",
      "output_blocks.7.0.emb_layers.1.bias\n",
      "output_blocks.7.0.out_layers.0.weight\n",
      "output_blocks.7.0.out_layers.0.bias\n",
      "output_blocks.7.0.out_layers.3.weight\n",
      "output_blocks.7.0.out_layers.3.bias\n",
      "output_blocks.7.0.skip_connection.weight\n",
      "output_blocks.7.0.skip_connection.bias\n",
      "output_blocks.8.0.in_layers.0.weight\n",
      "output_blocks.8.0.in_layers.0.bias\n",
      "output_blocks.8.0.in_layers.2.weight\n",
      "output_blocks.8.0.in_layers.2.bias\n",
      "output_blocks.8.0.emb_layers.1.weight\n",
      "output_blocks.8.0.emb_layers.1.bias\n",
      "output_blocks.8.0.out_layers.0.weight\n",
      "output_blocks.8.0.out_layers.0.bias\n",
      "output_blocks.8.0.out_layers.3.weight\n",
      "output_blocks.8.0.out_layers.3.bias\n",
      "output_blocks.8.0.skip_connection.weight\n",
      "output_blocks.8.0.skip_connection.bias\n",
      "output_blocks.8.1.conv.weight\n",
      "output_blocks.8.1.conv.bias\n",
      "output_blocks.9.0.in_layers.0.weight\n",
      "output_blocks.9.0.in_layers.0.bias\n",
      "output_blocks.9.0.in_layers.2.weight\n",
      "output_blocks.9.0.in_layers.2.bias\n",
      "output_blocks.9.0.emb_layers.1.weight\n",
      "output_blocks.9.0.emb_layers.1.bias\n",
      "output_blocks.9.0.out_layers.0.weight\n",
      "output_blocks.9.0.out_layers.0.bias\n",
      "output_blocks.9.0.out_layers.3.weight\n",
      "output_blocks.9.0.out_layers.3.bias\n",
      "output_blocks.9.0.skip_connection.weight\n",
      "output_blocks.9.0.skip_connection.bias\n",
      "output_blocks.10.0.in_layers.0.weight\n",
      "output_blocks.10.0.in_layers.0.bias\n",
      "output_blocks.10.0.in_layers.2.weight\n",
      "output_blocks.10.0.in_layers.2.bias\n",
      "output_blocks.10.0.emb_layers.1.weight\n",
      "output_blocks.10.0.emb_layers.1.bias\n",
      "output_blocks.10.0.out_layers.0.weight\n",
      "output_blocks.10.0.out_layers.0.bias\n",
      "output_blocks.10.0.out_layers.3.weight\n",
      "output_blocks.10.0.out_layers.3.bias\n",
      "output_blocks.10.0.skip_connection.weight\n",
      "output_blocks.10.0.skip_connection.bias\n",
      "output_blocks.11.0.in_layers.0.weight\n",
      "output_blocks.11.0.in_layers.0.bias\n",
      "output_blocks.11.0.in_layers.2.weight\n",
      "output_blocks.11.0.in_layers.2.bias\n",
      "output_blocks.11.0.emb_layers.1.weight\n",
      "output_blocks.11.0.emb_layers.1.bias\n",
      "output_blocks.11.0.out_layers.0.weight\n",
      "output_blocks.11.0.out_layers.0.bias\n",
      "output_blocks.11.0.out_layers.3.weight\n",
      "output_blocks.11.0.out_layers.3.bias\n",
      "output_blocks.11.0.skip_connection.weight\n",
      "output_blocks.11.0.skip_connection.bias\n",
      "out.0.weight\n",
      "out.0.bias\n",
      "out.2.weight\n",
      "out.2.bias\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "polyffusion_unet_params = inspect.signature(PolyffusionUNet.__init__).parameters\n",
    "polyffusion_unet_params_dict = {key:params[key] for key in params if key in polyffusion_unet_params}\n",
    "polyffusion_unet = PolyffusionUNet(**polyffusion_unet_params_dict)\n",
    "UNET_PREFIX = \"ldm.eps_model.\"\n",
    "polyffusion_unet_state_dict = {key.removeprefix(UNET_PREFIX):value for key,value in polyffusion_checkpoint.items() if key.startswith(UNET_PREFIX)}\n",
    "polyffusion_unet.load_state_dict(polyffusion_unet_state_dict)\n",
    "\n",
    "for key in polyffusion_unet.state_dict().keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_embed.0.weight\n",
      "time_embed.0.bias\n",
      "time_embed.2.weight\n",
      "time_embed.2.bias\n",
      "input_blocks.0.0.weight\n",
      "input_blocks.0.0.bias\n",
      "input_blocks.1.0.in_layers.0.weight\n",
      "input_blocks.1.0.in_layers.0.bias\n",
      "input_blocks.1.0.in_layers.2.weight\n",
      "input_blocks.1.0.in_layers.2.bias\n",
      "input_blocks.1.0.emb_layers.1.weight\n",
      "input_blocks.1.0.emb_layers.1.bias\n",
      "input_blocks.1.0.out_layers.0.weight\n",
      "input_blocks.1.0.out_layers.0.bias\n",
      "input_blocks.1.0.out_layers.3.weight\n",
      "input_blocks.1.0.out_layers.3.bias\n",
      "input_blocks.2.0.in_layers.0.weight\n",
      "input_blocks.2.0.in_layers.0.bias\n",
      "input_blocks.2.0.in_layers.2.weight\n",
      "input_blocks.2.0.in_layers.2.bias\n",
      "input_blocks.2.0.emb_layers.1.weight\n",
      "input_blocks.2.0.emb_layers.1.bias\n",
      "input_blocks.2.0.out_layers.0.weight\n",
      "input_blocks.2.0.out_layers.0.bias\n",
      "input_blocks.2.0.out_layers.3.weight\n",
      "input_blocks.2.0.out_layers.3.bias\n",
      "input_blocks.3.0.op.weight\n",
      "input_blocks.3.0.op.bias\n",
      "input_blocks.4.0.in_layers.0.weight\n",
      "input_blocks.4.0.in_layers.0.bias\n",
      "input_blocks.4.0.in_layers.2.weight\n",
      "input_blocks.4.0.in_layers.2.bias\n",
      "input_blocks.4.0.emb_layers.1.weight\n",
      "input_blocks.4.0.emb_layers.1.bias\n",
      "input_blocks.4.0.out_layers.0.weight\n",
      "input_blocks.4.0.out_layers.0.bias\n",
      "input_blocks.4.0.out_layers.3.weight\n",
      "input_blocks.4.0.out_layers.3.bias\n",
      "input_blocks.4.0.skip_connection.weight\n",
      "input_blocks.4.0.skip_connection.bias\n",
      "input_blocks.5.0.in_layers.0.weight\n",
      "input_blocks.5.0.in_layers.0.bias\n",
      "input_blocks.5.0.in_layers.2.weight\n",
      "input_blocks.5.0.in_layers.2.bias\n",
      "input_blocks.5.0.emb_layers.1.weight\n",
      "input_blocks.5.0.emb_layers.1.bias\n",
      "input_blocks.5.0.out_layers.0.weight\n",
      "input_blocks.5.0.out_layers.0.bias\n",
      "input_blocks.5.0.out_layers.3.weight\n",
      "input_blocks.5.0.out_layers.3.bias\n",
      "input_blocks.6.0.op.weight\n",
      "input_blocks.6.0.op.bias\n",
      "input_blocks.7.0.in_layers.0.weight\n",
      "input_blocks.7.0.in_layers.0.bias\n",
      "input_blocks.7.0.in_layers.2.weight\n",
      "input_blocks.7.0.in_layers.2.bias\n",
      "input_blocks.7.0.emb_layers.1.weight\n",
      "input_blocks.7.0.emb_layers.1.bias\n",
      "input_blocks.7.0.out_layers.0.weight\n",
      "input_blocks.7.0.out_layers.0.bias\n",
      "input_blocks.7.0.out_layers.3.weight\n",
      "input_blocks.7.0.out_layers.3.bias\n",
      "input_blocks.7.0.skip_connection.weight\n",
      "input_blocks.7.0.skip_connection.bias\n",
      "input_blocks.7.1.norm.weight\n",
      "input_blocks.7.1.norm.bias\n",
      "input_blocks.7.1.proj_in.weight\n",
      "input_blocks.7.1.proj_in.bias\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.7.1.proj_out.weight\n",
      "input_blocks.7.1.proj_out.bias\n",
      "input_blocks.7.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.7.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.7.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.7.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.7.2.attention.layers.0.linear1.weight\n",
      "input_blocks.7.2.attention.layers.0.linear1.bias\n",
      "input_blocks.7.2.attention.layers.0.linear2.weight\n",
      "input_blocks.7.2.attention.layers.0.linear2.bias\n",
      "input_blocks.7.2.attention.layers.0.norm1.weight\n",
      "input_blocks.7.2.attention.layers.0.norm1.bias\n",
      "input_blocks.7.2.attention.layers.0.norm2.weight\n",
      "input_blocks.7.2.attention.layers.0.norm2.bias\n",
      "input_blocks.7.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.7.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.7.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.7.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.7.2.attention.layers.1.linear1.weight\n",
      "input_blocks.7.2.attention.layers.1.linear1.bias\n",
      "input_blocks.7.2.attention.layers.1.linear2.weight\n",
      "input_blocks.7.2.attention.layers.1.linear2.bias\n",
      "input_blocks.7.2.attention.layers.1.norm1.weight\n",
      "input_blocks.7.2.attention.layers.1.norm1.bias\n",
      "input_blocks.7.2.attention.layers.1.norm2.weight\n",
      "input_blocks.7.2.attention.layers.1.norm2.bias\n",
      "input_blocks.8.0.in_layers.0.weight\n",
      "input_blocks.8.0.in_layers.0.bias\n",
      "input_blocks.8.0.in_layers.2.weight\n",
      "input_blocks.8.0.in_layers.2.bias\n",
      "input_blocks.8.0.emb_layers.1.weight\n",
      "input_blocks.8.0.emb_layers.1.bias\n",
      "input_blocks.8.0.out_layers.0.weight\n",
      "input_blocks.8.0.out_layers.0.bias\n",
      "input_blocks.8.0.out_layers.3.weight\n",
      "input_blocks.8.0.out_layers.3.bias\n",
      "input_blocks.8.1.norm.weight\n",
      "input_blocks.8.1.norm.bias\n",
      "input_blocks.8.1.proj_in.weight\n",
      "input_blocks.8.1.proj_in.bias\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.8.1.proj_out.weight\n",
      "input_blocks.8.1.proj_out.bias\n",
      "input_blocks.8.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.8.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.8.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.8.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.8.2.attention.layers.0.linear1.weight\n",
      "input_blocks.8.2.attention.layers.0.linear1.bias\n",
      "input_blocks.8.2.attention.layers.0.linear2.weight\n",
      "input_blocks.8.2.attention.layers.0.linear2.bias\n",
      "input_blocks.8.2.attention.layers.0.norm1.weight\n",
      "input_blocks.8.2.attention.layers.0.norm1.bias\n",
      "input_blocks.8.2.attention.layers.0.norm2.weight\n",
      "input_blocks.8.2.attention.layers.0.norm2.bias\n",
      "input_blocks.8.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.8.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.8.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.8.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.8.2.attention.layers.1.linear1.weight\n",
      "input_blocks.8.2.attention.layers.1.linear1.bias\n",
      "input_blocks.8.2.attention.layers.1.linear2.weight\n",
      "input_blocks.8.2.attention.layers.1.linear2.bias\n",
      "input_blocks.8.2.attention.layers.1.norm1.weight\n",
      "input_blocks.8.2.attention.layers.1.norm1.bias\n",
      "input_blocks.8.2.attention.layers.1.norm2.weight\n",
      "input_blocks.8.2.attention.layers.1.norm2.bias\n",
      "input_blocks.9.0.op.weight\n",
      "input_blocks.9.0.op.bias\n",
      "input_blocks.10.0.in_layers.0.weight\n",
      "input_blocks.10.0.in_layers.0.bias\n",
      "input_blocks.10.0.in_layers.2.weight\n",
      "input_blocks.10.0.in_layers.2.bias\n",
      "input_blocks.10.0.emb_layers.1.weight\n",
      "input_blocks.10.0.emb_layers.1.bias\n",
      "input_blocks.10.0.out_layers.0.weight\n",
      "input_blocks.10.0.out_layers.0.bias\n",
      "input_blocks.10.0.out_layers.3.weight\n",
      "input_blocks.10.0.out_layers.3.bias\n",
      "input_blocks.10.1.norm.weight\n",
      "input_blocks.10.1.norm.bias\n",
      "input_blocks.10.1.proj_in.weight\n",
      "input_blocks.10.1.proj_in.bias\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.10.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.10.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.10.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.10.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.10.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.10.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.10.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.10.1.proj_out.weight\n",
      "input_blocks.10.1.proj_out.bias\n",
      "input_blocks.10.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.10.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.10.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.10.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.10.2.attention.layers.0.linear1.weight\n",
      "input_blocks.10.2.attention.layers.0.linear1.bias\n",
      "input_blocks.10.2.attention.layers.0.linear2.weight\n",
      "input_blocks.10.2.attention.layers.0.linear2.bias\n",
      "input_blocks.10.2.attention.layers.0.norm1.weight\n",
      "input_blocks.10.2.attention.layers.0.norm1.bias\n",
      "input_blocks.10.2.attention.layers.0.norm2.weight\n",
      "input_blocks.10.2.attention.layers.0.norm2.bias\n",
      "input_blocks.10.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.10.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.10.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.10.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.10.2.attention.layers.1.linear1.weight\n",
      "input_blocks.10.2.attention.layers.1.linear1.bias\n",
      "input_blocks.10.2.attention.layers.1.linear2.weight\n",
      "input_blocks.10.2.attention.layers.1.linear2.bias\n",
      "input_blocks.10.2.attention.layers.1.norm1.weight\n",
      "input_blocks.10.2.attention.layers.1.norm1.bias\n",
      "input_blocks.10.2.attention.layers.1.norm2.weight\n",
      "input_blocks.10.2.attention.layers.1.norm2.bias\n",
      "input_blocks.11.0.in_layers.0.weight\n",
      "input_blocks.11.0.in_layers.0.bias\n",
      "input_blocks.11.0.in_layers.2.weight\n",
      "input_blocks.11.0.in_layers.2.bias\n",
      "input_blocks.11.0.emb_layers.1.weight\n",
      "input_blocks.11.0.emb_layers.1.bias\n",
      "input_blocks.11.0.out_layers.0.weight\n",
      "input_blocks.11.0.out_layers.0.bias\n",
      "input_blocks.11.0.out_layers.3.weight\n",
      "input_blocks.11.0.out_layers.3.bias\n",
      "input_blocks.11.1.norm.weight\n",
      "input_blocks.11.1.norm.bias\n",
      "input_blocks.11.1.proj_in.weight\n",
      "input_blocks.11.1.proj_in.bias\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_q.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_k.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_v.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "input_blocks.11.1.transformer_blocks.0.norm1.weight\n",
      "input_blocks.11.1.transformer_blocks.0.norm1.bias\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_q.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_k.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_v.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "input_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "input_blocks.11.1.transformer_blocks.0.norm2.weight\n",
      "input_blocks.11.1.transformer_blocks.0.norm2.bias\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.2.weight\n",
      "input_blocks.11.1.transformer_blocks.0.ff.net.2.bias\n",
      "input_blocks.11.1.transformer_blocks.0.norm3.weight\n",
      "input_blocks.11.1.transformer_blocks.0.norm3.bias\n",
      "input_blocks.11.1.proj_out.weight\n",
      "input_blocks.11.1.proj_out.bias\n",
      "input_blocks.11.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.11.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.11.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.11.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.11.2.attention.layers.0.linear1.weight\n",
      "input_blocks.11.2.attention.layers.0.linear1.bias\n",
      "input_blocks.11.2.attention.layers.0.linear2.weight\n",
      "input_blocks.11.2.attention.layers.0.linear2.bias\n",
      "input_blocks.11.2.attention.layers.0.norm1.weight\n",
      "input_blocks.11.2.attention.layers.0.norm1.bias\n",
      "input_blocks.11.2.attention.layers.0.norm2.weight\n",
      "input_blocks.11.2.attention.layers.0.norm2.bias\n",
      "input_blocks.11.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.11.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.11.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.11.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.11.2.attention.layers.1.linear1.weight\n",
      "input_blocks.11.2.attention.layers.1.linear1.bias\n",
      "input_blocks.11.2.attention.layers.1.linear2.weight\n",
      "input_blocks.11.2.attention.layers.1.linear2.bias\n",
      "input_blocks.11.2.attention.layers.1.norm1.weight\n",
      "input_blocks.11.2.attention.layers.1.norm1.bias\n",
      "input_blocks.11.2.attention.layers.1.norm2.weight\n",
      "input_blocks.11.2.attention.layers.1.norm2.bias\n",
      "middle_block.0.in_layers.0.weight\n",
      "middle_block.0.in_layers.0.bias\n",
      "middle_block.0.in_layers.2.weight\n",
      "middle_block.0.in_layers.2.bias\n",
      "middle_block.0.emb_layers.1.weight\n",
      "middle_block.0.emb_layers.1.bias\n",
      "middle_block.0.out_layers.0.weight\n",
      "middle_block.0.out_layers.0.bias\n",
      "middle_block.0.out_layers.3.weight\n",
      "middle_block.0.out_layers.3.bias\n",
      "middle_block.1.norm.weight\n",
      "middle_block.1.norm.bias\n",
      "middle_block.1.proj_in.weight\n",
      "middle_block.1.proj_in.bias\n",
      "middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "middle_block.1.transformer_blocks.0.norm1.weight\n",
      "middle_block.1.transformer_blocks.0.norm1.bias\n",
      "middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "middle_block.1.transformer_blocks.0.norm2.weight\n",
      "middle_block.1.transformer_blocks.0.norm2.bias\n",
      "middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "middle_block.1.transformer_blocks.0.norm3.weight\n",
      "middle_block.1.transformer_blocks.0.norm3.bias\n",
      "middle_block.1.proj_out.weight\n",
      "middle_block.1.proj_out.bias\n",
      "middle_block.2.in_layers.0.weight\n",
      "middle_block.2.in_layers.0.bias\n",
      "middle_block.2.in_layers.2.weight\n",
      "middle_block.2.in_layers.2.bias\n",
      "middle_block.2.emb_layers.1.weight\n",
      "middle_block.2.emb_layers.1.bias\n",
      "middle_block.2.out_layers.0.weight\n",
      "middle_block.2.out_layers.0.bias\n",
      "middle_block.2.out_layers.3.weight\n",
      "middle_block.2.out_layers.3.bias\n",
      "output_blocks.0.0.in_layers.0.weight\n",
      "output_blocks.0.0.in_layers.0.bias\n",
      "output_blocks.0.0.in_layers.2.weight\n",
      "output_blocks.0.0.in_layers.2.bias\n",
      "output_blocks.0.0.emb_layers.1.weight\n",
      "output_blocks.0.0.emb_layers.1.bias\n",
      "output_blocks.0.0.out_layers.0.weight\n",
      "output_blocks.0.0.out_layers.0.bias\n",
      "output_blocks.0.0.out_layers.3.weight\n",
      "output_blocks.0.0.out_layers.3.bias\n",
      "output_blocks.0.0.skip_connection.weight\n",
      "output_blocks.0.0.skip_connection.bias\n",
      "output_blocks.0.1.norm.weight\n",
      "output_blocks.0.1.norm.bias\n",
      "output_blocks.0.1.proj_in.weight\n",
      "output_blocks.0.1.proj_in.bias\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.0.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.0.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.0.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.0.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.0.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.0.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.0.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.0.1.proj_out.weight\n",
      "output_blocks.0.1.proj_out.bias\n",
      "output_blocks.0.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.0.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.0.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.0.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.0.2.attention.layers.0.linear1.weight\n",
      "output_blocks.0.2.attention.layers.0.linear1.bias\n",
      "output_blocks.0.2.attention.layers.0.linear2.weight\n",
      "output_blocks.0.2.attention.layers.0.linear2.bias\n",
      "output_blocks.0.2.attention.layers.0.norm1.weight\n",
      "output_blocks.0.2.attention.layers.0.norm1.bias\n",
      "output_blocks.0.2.attention.layers.0.norm2.weight\n",
      "output_blocks.0.2.attention.layers.0.norm2.bias\n",
      "output_blocks.0.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.0.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.0.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.0.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.0.2.attention.layers.1.linear1.weight\n",
      "output_blocks.0.2.attention.layers.1.linear1.bias\n",
      "output_blocks.0.2.attention.layers.1.linear2.weight\n",
      "output_blocks.0.2.attention.layers.1.linear2.bias\n",
      "output_blocks.0.2.attention.layers.1.norm1.weight\n",
      "output_blocks.0.2.attention.layers.1.norm1.bias\n",
      "output_blocks.0.2.attention.layers.1.norm2.weight\n",
      "output_blocks.0.2.attention.layers.1.norm2.bias\n",
      "output_blocks.1.0.in_layers.0.weight\n",
      "output_blocks.1.0.in_layers.0.bias\n",
      "output_blocks.1.0.in_layers.2.weight\n",
      "output_blocks.1.0.in_layers.2.bias\n",
      "output_blocks.1.0.emb_layers.1.weight\n",
      "output_blocks.1.0.emb_layers.1.bias\n",
      "output_blocks.1.0.out_layers.0.weight\n",
      "output_blocks.1.0.out_layers.0.bias\n",
      "output_blocks.1.0.out_layers.3.weight\n",
      "output_blocks.1.0.out_layers.3.bias\n",
      "output_blocks.1.0.skip_connection.weight\n",
      "output_blocks.1.0.skip_connection.bias\n",
      "output_blocks.1.1.norm.weight\n",
      "output_blocks.1.1.norm.bias\n",
      "output_blocks.1.1.proj_in.weight\n",
      "output_blocks.1.1.proj_in.bias\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.1.1.proj_out.weight\n",
      "output_blocks.1.1.proj_out.bias\n",
      "output_blocks.1.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.1.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.1.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.1.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.1.2.attention.layers.0.linear1.weight\n",
      "output_blocks.1.2.attention.layers.0.linear1.bias\n",
      "output_blocks.1.2.attention.layers.0.linear2.weight\n",
      "output_blocks.1.2.attention.layers.0.linear2.bias\n",
      "output_blocks.1.2.attention.layers.0.norm1.weight\n",
      "output_blocks.1.2.attention.layers.0.norm1.bias\n",
      "output_blocks.1.2.attention.layers.0.norm2.weight\n",
      "output_blocks.1.2.attention.layers.0.norm2.bias\n",
      "output_blocks.1.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.1.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.1.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.1.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.1.2.attention.layers.1.linear1.weight\n",
      "output_blocks.1.2.attention.layers.1.linear1.bias\n",
      "output_blocks.1.2.attention.layers.1.linear2.weight\n",
      "output_blocks.1.2.attention.layers.1.linear2.bias\n",
      "output_blocks.1.2.attention.layers.1.norm1.weight\n",
      "output_blocks.1.2.attention.layers.1.norm1.bias\n",
      "output_blocks.1.2.attention.layers.1.norm2.weight\n",
      "output_blocks.1.2.attention.layers.1.norm2.bias\n",
      "output_blocks.2.0.in_layers.0.weight\n",
      "output_blocks.2.0.in_layers.0.bias\n",
      "output_blocks.2.0.in_layers.2.weight\n",
      "output_blocks.2.0.in_layers.2.bias\n",
      "output_blocks.2.0.emb_layers.1.weight\n",
      "output_blocks.2.0.emb_layers.1.bias\n",
      "output_blocks.2.0.out_layers.0.weight\n",
      "output_blocks.2.0.out_layers.0.bias\n",
      "output_blocks.2.0.out_layers.3.weight\n",
      "output_blocks.2.0.out_layers.3.bias\n",
      "output_blocks.2.0.skip_connection.weight\n",
      "output_blocks.2.0.skip_connection.bias\n",
      "output_blocks.2.1.norm.weight\n",
      "output_blocks.2.1.norm.bias\n",
      "output_blocks.2.1.proj_in.weight\n",
      "output_blocks.2.1.proj_in.bias\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.2.1.proj_out.weight\n",
      "output_blocks.2.1.proj_out.bias\n",
      "output_blocks.2.2.conv.weight\n",
      "output_blocks.2.2.conv.bias\n",
      "output_blocks.2.3.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.2.3.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.2.3.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.2.3.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.2.3.attention.layers.0.linear1.weight\n",
      "output_blocks.2.3.attention.layers.0.linear1.bias\n",
      "output_blocks.2.3.attention.layers.0.linear2.weight\n",
      "output_blocks.2.3.attention.layers.0.linear2.bias\n",
      "output_blocks.2.3.attention.layers.0.norm1.weight\n",
      "output_blocks.2.3.attention.layers.0.norm1.bias\n",
      "output_blocks.2.3.attention.layers.0.norm2.weight\n",
      "output_blocks.2.3.attention.layers.0.norm2.bias\n",
      "output_blocks.2.3.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.2.3.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.2.3.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.2.3.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.2.3.attention.layers.1.linear1.weight\n",
      "output_blocks.2.3.attention.layers.1.linear1.bias\n",
      "output_blocks.2.3.attention.layers.1.linear2.weight\n",
      "output_blocks.2.3.attention.layers.1.linear2.bias\n",
      "output_blocks.2.3.attention.layers.1.norm1.weight\n",
      "output_blocks.2.3.attention.layers.1.norm1.bias\n",
      "output_blocks.2.3.attention.layers.1.norm2.weight\n",
      "output_blocks.2.3.attention.layers.1.norm2.bias\n",
      "output_blocks.3.0.in_layers.0.weight\n",
      "output_blocks.3.0.in_layers.0.bias\n",
      "output_blocks.3.0.in_layers.2.weight\n",
      "output_blocks.3.0.in_layers.2.bias\n",
      "output_blocks.3.0.emb_layers.1.weight\n",
      "output_blocks.3.0.emb_layers.1.bias\n",
      "output_blocks.3.0.out_layers.0.weight\n",
      "output_blocks.3.0.out_layers.0.bias\n",
      "output_blocks.3.0.out_layers.3.weight\n",
      "output_blocks.3.0.out_layers.3.bias\n",
      "output_blocks.3.0.skip_connection.weight\n",
      "output_blocks.3.0.skip_connection.bias\n",
      "output_blocks.3.1.norm.weight\n",
      "output_blocks.3.1.norm.bias\n",
      "output_blocks.3.1.proj_in.weight\n",
      "output_blocks.3.1.proj_in.bias\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.3.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.3.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.3.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.3.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.3.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.3.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.3.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.3.1.proj_out.weight\n",
      "output_blocks.3.1.proj_out.bias\n",
      "output_blocks.3.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.3.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.3.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.3.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.3.2.attention.layers.0.linear1.weight\n",
      "output_blocks.3.2.attention.layers.0.linear1.bias\n",
      "output_blocks.3.2.attention.layers.0.linear2.weight\n",
      "output_blocks.3.2.attention.layers.0.linear2.bias\n",
      "output_blocks.3.2.attention.layers.0.norm1.weight\n",
      "output_blocks.3.2.attention.layers.0.norm1.bias\n",
      "output_blocks.3.2.attention.layers.0.norm2.weight\n",
      "output_blocks.3.2.attention.layers.0.norm2.bias\n",
      "output_blocks.3.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.3.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.3.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.3.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.3.2.attention.layers.1.linear1.weight\n",
      "output_blocks.3.2.attention.layers.1.linear1.bias\n",
      "output_blocks.3.2.attention.layers.1.linear2.weight\n",
      "output_blocks.3.2.attention.layers.1.linear2.bias\n",
      "output_blocks.3.2.attention.layers.1.norm1.weight\n",
      "output_blocks.3.2.attention.layers.1.norm1.bias\n",
      "output_blocks.3.2.attention.layers.1.norm2.weight\n",
      "output_blocks.3.2.attention.layers.1.norm2.bias\n",
      "output_blocks.4.0.in_layers.0.weight\n",
      "output_blocks.4.0.in_layers.0.bias\n",
      "output_blocks.4.0.in_layers.2.weight\n",
      "output_blocks.4.0.in_layers.2.bias\n",
      "output_blocks.4.0.emb_layers.1.weight\n",
      "output_blocks.4.0.emb_layers.1.bias\n",
      "output_blocks.4.0.out_layers.0.weight\n",
      "output_blocks.4.0.out_layers.0.bias\n",
      "output_blocks.4.0.out_layers.3.weight\n",
      "output_blocks.4.0.out_layers.3.bias\n",
      "output_blocks.4.0.skip_connection.weight\n",
      "output_blocks.4.0.skip_connection.bias\n",
      "output_blocks.4.1.norm.weight\n",
      "output_blocks.4.1.norm.bias\n",
      "output_blocks.4.1.proj_in.weight\n",
      "output_blocks.4.1.proj_in.bias\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.4.1.proj_out.weight\n",
      "output_blocks.4.1.proj_out.bias\n",
      "output_blocks.4.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.4.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.4.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.4.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.4.2.attention.layers.0.linear1.weight\n",
      "output_blocks.4.2.attention.layers.0.linear1.bias\n",
      "output_blocks.4.2.attention.layers.0.linear2.weight\n",
      "output_blocks.4.2.attention.layers.0.linear2.bias\n",
      "output_blocks.4.2.attention.layers.0.norm1.weight\n",
      "output_blocks.4.2.attention.layers.0.norm1.bias\n",
      "output_blocks.4.2.attention.layers.0.norm2.weight\n",
      "output_blocks.4.2.attention.layers.0.norm2.bias\n",
      "output_blocks.4.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.4.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.4.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.4.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.4.2.attention.layers.1.linear1.weight\n",
      "output_blocks.4.2.attention.layers.1.linear1.bias\n",
      "output_blocks.4.2.attention.layers.1.linear2.weight\n",
      "output_blocks.4.2.attention.layers.1.linear2.bias\n",
      "output_blocks.4.2.attention.layers.1.norm1.weight\n",
      "output_blocks.4.2.attention.layers.1.norm1.bias\n",
      "output_blocks.4.2.attention.layers.1.norm2.weight\n",
      "output_blocks.4.2.attention.layers.1.norm2.bias\n",
      "output_blocks.5.0.in_layers.0.weight\n",
      "output_blocks.5.0.in_layers.0.bias\n",
      "output_blocks.5.0.in_layers.2.weight\n",
      "output_blocks.5.0.in_layers.2.bias\n",
      "output_blocks.5.0.emb_layers.1.weight\n",
      "output_blocks.5.0.emb_layers.1.bias\n",
      "output_blocks.5.0.out_layers.0.weight\n",
      "output_blocks.5.0.out_layers.0.bias\n",
      "output_blocks.5.0.out_layers.3.weight\n",
      "output_blocks.5.0.out_layers.3.bias\n",
      "output_blocks.5.0.skip_connection.weight\n",
      "output_blocks.5.0.skip_connection.bias\n",
      "output_blocks.5.1.norm.weight\n",
      "output_blocks.5.1.norm.bias\n",
      "output_blocks.5.1.proj_in.weight\n",
      "output_blocks.5.1.proj_in.bias\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "output_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "output_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "output_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "output_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "output_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "output_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "output_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "output_blocks.5.1.proj_out.weight\n",
      "output_blocks.5.1.proj_out.bias\n",
      "output_blocks.5.2.conv.weight\n",
      "output_blocks.5.2.conv.bias\n",
      "output_blocks.5.3.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.5.3.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.5.3.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.5.3.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.5.3.attention.layers.0.linear1.weight\n",
      "output_blocks.5.3.attention.layers.0.linear1.bias\n",
      "output_blocks.5.3.attention.layers.0.linear2.weight\n",
      "output_blocks.5.3.attention.layers.0.linear2.bias\n",
      "output_blocks.5.3.attention.layers.0.norm1.weight\n",
      "output_blocks.5.3.attention.layers.0.norm1.bias\n",
      "output_blocks.5.3.attention.layers.0.norm2.weight\n",
      "output_blocks.5.3.attention.layers.0.norm2.bias\n",
      "output_blocks.5.3.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.5.3.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.5.3.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.5.3.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.5.3.attention.layers.1.linear1.weight\n",
      "output_blocks.5.3.attention.layers.1.linear1.bias\n",
      "output_blocks.5.3.attention.layers.1.linear2.weight\n",
      "output_blocks.5.3.attention.layers.1.linear2.bias\n",
      "output_blocks.5.3.attention.layers.1.norm1.weight\n",
      "output_blocks.5.3.attention.layers.1.norm1.bias\n",
      "output_blocks.5.3.attention.layers.1.norm2.weight\n",
      "output_blocks.5.3.attention.layers.1.norm2.bias\n",
      "output_blocks.6.0.in_layers.0.weight\n",
      "output_blocks.6.0.in_layers.0.bias\n",
      "output_blocks.6.0.in_layers.2.weight\n",
      "output_blocks.6.0.in_layers.2.bias\n",
      "output_blocks.6.0.emb_layers.1.weight\n",
      "output_blocks.6.0.emb_layers.1.bias\n",
      "output_blocks.6.0.out_layers.0.weight\n",
      "output_blocks.6.0.out_layers.0.bias\n",
      "output_blocks.6.0.out_layers.3.weight\n",
      "output_blocks.6.0.out_layers.3.bias\n",
      "output_blocks.6.0.skip_connection.weight\n",
      "output_blocks.6.0.skip_connection.bias\n",
      "output_blocks.7.0.in_layers.0.weight\n",
      "output_blocks.7.0.in_layers.0.bias\n",
      "output_blocks.7.0.in_layers.2.weight\n",
      "output_blocks.7.0.in_layers.2.bias\n",
      "output_blocks.7.0.emb_layers.1.weight\n",
      "output_blocks.7.0.emb_layers.1.bias\n",
      "output_blocks.7.0.out_layers.0.weight\n",
      "output_blocks.7.0.out_layers.0.bias\n",
      "output_blocks.7.0.out_layers.3.weight\n",
      "output_blocks.7.0.out_layers.3.bias\n",
      "output_blocks.7.0.skip_connection.weight\n",
      "output_blocks.7.0.skip_connection.bias\n",
      "output_blocks.8.0.in_layers.0.weight\n",
      "output_blocks.8.0.in_layers.0.bias\n",
      "output_blocks.8.0.in_layers.2.weight\n",
      "output_blocks.8.0.in_layers.2.bias\n",
      "output_blocks.8.0.emb_layers.1.weight\n",
      "output_blocks.8.0.emb_layers.1.bias\n",
      "output_blocks.8.0.out_layers.0.weight\n",
      "output_blocks.8.0.out_layers.0.bias\n",
      "output_blocks.8.0.out_layers.3.weight\n",
      "output_blocks.8.0.out_layers.3.bias\n",
      "output_blocks.8.0.skip_connection.weight\n",
      "output_blocks.8.0.skip_connection.bias\n",
      "output_blocks.8.1.conv.weight\n",
      "output_blocks.8.1.conv.bias\n",
      "output_blocks.9.0.in_layers.0.weight\n",
      "output_blocks.9.0.in_layers.0.bias\n",
      "output_blocks.9.0.in_layers.2.weight\n",
      "output_blocks.9.0.in_layers.2.bias\n",
      "output_blocks.9.0.emb_layers.1.weight\n",
      "output_blocks.9.0.emb_layers.1.bias\n",
      "output_blocks.9.0.out_layers.0.weight\n",
      "output_blocks.9.0.out_layers.0.bias\n",
      "output_blocks.9.0.out_layers.3.weight\n",
      "output_blocks.9.0.out_layers.3.bias\n",
      "output_blocks.9.0.skip_connection.weight\n",
      "output_blocks.9.0.skip_connection.bias\n",
      "output_blocks.10.0.in_layers.0.weight\n",
      "output_blocks.10.0.in_layers.0.bias\n",
      "output_blocks.10.0.in_layers.2.weight\n",
      "output_blocks.10.0.in_layers.2.bias\n",
      "output_blocks.10.0.emb_layers.1.weight\n",
      "output_blocks.10.0.emb_layers.1.bias\n",
      "output_blocks.10.0.out_layers.0.weight\n",
      "output_blocks.10.0.out_layers.0.bias\n",
      "output_blocks.10.0.out_layers.3.weight\n",
      "output_blocks.10.0.out_layers.3.bias\n",
      "output_blocks.10.0.skip_connection.weight\n",
      "output_blocks.10.0.skip_connection.bias\n",
      "output_blocks.11.0.in_layers.0.weight\n",
      "output_blocks.11.0.in_layers.0.bias\n",
      "output_blocks.11.0.in_layers.2.weight\n",
      "output_blocks.11.0.in_layers.2.bias\n",
      "output_blocks.11.0.emb_layers.1.weight\n",
      "output_blocks.11.0.emb_layers.1.bias\n",
      "output_blocks.11.0.out_layers.0.weight\n",
      "output_blocks.11.0.out_layers.0.bias\n",
      "output_blocks.11.0.out_layers.3.weight\n",
      "output_blocks.11.0.out_layers.3.bias\n",
      "output_blocks.11.0.skip_connection.weight\n",
      "output_blocks.11.0.skip_connection.bias\n",
      "out.0.weight\n",
      "out.0.bias\n",
      "out.2.weight\n",
      "out.2.bias\n"
     ]
    }
   ],
   "source": [
    "multipoly_unet_params = inspect.signature(MultipolyUNet.__init__).parameters\n",
    "multipoly_unet_params_dict = {key:params[key] for key in params if key in multipoly_unet_params}\n",
    "multipoly_unet_params_dict[\"n_intertrack_head\"] = 4\n",
    "multipoly_unet_params_dict[\"num_intertrack_encoder_layers\"] = 2\n",
    "multipoly_unet_params_dict[\"intertrack_attention_levels\"] = [2,3]\n",
    "multipoly_unet = MultipolyUNet(**multipoly_unet_params_dict)\n",
    "\n",
    "for key in multipoly_unet.state_dict().keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------loading polyffusion weights-------------------\n",
      "input_blocks.7.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.7.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.7.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.7.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.7.2.attention.layers.0.linear1.weight\n",
      "input_blocks.7.2.attention.layers.0.linear1.bias\n",
      "input_blocks.7.2.attention.layers.0.linear2.weight\n",
      "input_blocks.7.2.attention.layers.0.linear2.bias\n",
      "input_blocks.7.2.attention.layers.0.norm1.weight\n",
      "input_blocks.7.2.attention.layers.0.norm1.bias\n",
      "input_blocks.7.2.attention.layers.0.norm2.weight\n",
      "input_blocks.7.2.attention.layers.0.norm2.bias\n",
      "input_blocks.7.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.7.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.7.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.7.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.7.2.attention.layers.1.linear1.weight\n",
      "input_blocks.7.2.attention.layers.1.linear1.bias\n",
      "input_blocks.7.2.attention.layers.1.linear2.weight\n",
      "input_blocks.7.2.attention.layers.1.linear2.bias\n",
      "input_blocks.7.2.attention.layers.1.norm1.weight\n",
      "input_blocks.7.2.attention.layers.1.norm1.bias\n",
      "input_blocks.7.2.attention.layers.1.norm2.weight\n",
      "input_blocks.7.2.attention.layers.1.norm2.bias\n",
      "input_blocks.8.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.8.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.8.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.8.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.8.2.attention.layers.0.linear1.weight\n",
      "input_blocks.8.2.attention.layers.0.linear1.bias\n",
      "input_blocks.8.2.attention.layers.0.linear2.weight\n",
      "input_blocks.8.2.attention.layers.0.linear2.bias\n",
      "input_blocks.8.2.attention.layers.0.norm1.weight\n",
      "input_blocks.8.2.attention.layers.0.norm1.bias\n",
      "input_blocks.8.2.attention.layers.0.norm2.weight\n",
      "input_blocks.8.2.attention.layers.0.norm2.bias\n",
      "input_blocks.8.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.8.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.8.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.8.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.8.2.attention.layers.1.linear1.weight\n",
      "input_blocks.8.2.attention.layers.1.linear1.bias\n",
      "input_blocks.8.2.attention.layers.1.linear2.weight\n",
      "input_blocks.8.2.attention.layers.1.linear2.bias\n",
      "input_blocks.8.2.attention.layers.1.norm1.weight\n",
      "input_blocks.8.2.attention.layers.1.norm1.bias\n",
      "input_blocks.8.2.attention.layers.1.norm2.weight\n",
      "input_blocks.8.2.attention.layers.1.norm2.bias\n",
      "input_blocks.10.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.10.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.10.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.10.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.10.2.attention.layers.0.linear1.weight\n",
      "input_blocks.10.2.attention.layers.0.linear1.bias\n",
      "input_blocks.10.2.attention.layers.0.linear2.weight\n",
      "input_blocks.10.2.attention.layers.0.linear2.bias\n",
      "input_blocks.10.2.attention.layers.0.norm1.weight\n",
      "input_blocks.10.2.attention.layers.0.norm1.bias\n",
      "input_blocks.10.2.attention.layers.0.norm2.weight\n",
      "input_blocks.10.2.attention.layers.0.norm2.bias\n",
      "input_blocks.10.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.10.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.10.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.10.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.10.2.attention.layers.1.linear1.weight\n",
      "input_blocks.10.2.attention.layers.1.linear1.bias\n",
      "input_blocks.10.2.attention.layers.1.linear2.weight\n",
      "input_blocks.10.2.attention.layers.1.linear2.bias\n",
      "input_blocks.10.2.attention.layers.1.norm1.weight\n",
      "input_blocks.10.2.attention.layers.1.norm1.bias\n",
      "input_blocks.10.2.attention.layers.1.norm2.weight\n",
      "input_blocks.10.2.attention.layers.1.norm2.bias\n",
      "input_blocks.11.2.attention.layers.0.self_attn.in_proj_weight\n",
      "input_blocks.11.2.attention.layers.0.self_attn.in_proj_bias\n",
      "input_blocks.11.2.attention.layers.0.self_attn.out_proj.weight\n",
      "input_blocks.11.2.attention.layers.0.self_attn.out_proj.bias\n",
      "input_blocks.11.2.attention.layers.0.linear1.weight\n",
      "input_blocks.11.2.attention.layers.0.linear1.bias\n",
      "input_blocks.11.2.attention.layers.0.linear2.weight\n",
      "input_blocks.11.2.attention.layers.0.linear2.bias\n",
      "input_blocks.11.2.attention.layers.0.norm1.weight\n",
      "input_blocks.11.2.attention.layers.0.norm1.bias\n",
      "input_blocks.11.2.attention.layers.0.norm2.weight\n",
      "input_blocks.11.2.attention.layers.0.norm2.bias\n",
      "input_blocks.11.2.attention.layers.1.self_attn.in_proj_weight\n",
      "input_blocks.11.2.attention.layers.1.self_attn.in_proj_bias\n",
      "input_blocks.11.2.attention.layers.1.self_attn.out_proj.weight\n",
      "input_blocks.11.2.attention.layers.1.self_attn.out_proj.bias\n",
      "input_blocks.11.2.attention.layers.1.linear1.weight\n",
      "input_blocks.11.2.attention.layers.1.linear1.bias\n",
      "input_blocks.11.2.attention.layers.1.linear2.weight\n",
      "input_blocks.11.2.attention.layers.1.linear2.bias\n",
      "input_blocks.11.2.attention.layers.1.norm1.weight\n",
      "input_blocks.11.2.attention.layers.1.norm1.bias\n",
      "input_blocks.11.2.attention.layers.1.norm2.weight\n",
      "input_blocks.11.2.attention.layers.1.norm2.bias\n",
      "output_blocks.0.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.0.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.0.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.0.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.0.2.attention.layers.0.linear1.weight\n",
      "output_blocks.0.2.attention.layers.0.linear1.bias\n",
      "output_blocks.0.2.attention.layers.0.linear2.weight\n",
      "output_blocks.0.2.attention.layers.0.linear2.bias\n",
      "output_blocks.0.2.attention.layers.0.norm1.weight\n",
      "output_blocks.0.2.attention.layers.0.norm1.bias\n",
      "output_blocks.0.2.attention.layers.0.norm2.weight\n",
      "output_blocks.0.2.attention.layers.0.norm2.bias\n",
      "output_blocks.0.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.0.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.0.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.0.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.0.2.attention.layers.1.linear1.weight\n",
      "output_blocks.0.2.attention.layers.1.linear1.bias\n",
      "output_blocks.0.2.attention.layers.1.linear2.weight\n",
      "output_blocks.0.2.attention.layers.1.linear2.bias\n",
      "output_blocks.0.2.attention.layers.1.norm1.weight\n",
      "output_blocks.0.2.attention.layers.1.norm1.bias\n",
      "output_blocks.0.2.attention.layers.1.norm2.weight\n",
      "output_blocks.0.2.attention.layers.1.norm2.bias\n",
      "output_blocks.1.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.1.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.1.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.1.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.1.2.attention.layers.0.linear1.weight\n",
      "output_blocks.1.2.attention.layers.0.linear1.bias\n",
      "output_blocks.1.2.attention.layers.0.linear2.weight\n",
      "output_blocks.1.2.attention.layers.0.linear2.bias\n",
      "output_blocks.1.2.attention.layers.0.norm1.weight\n",
      "output_blocks.1.2.attention.layers.0.norm1.bias\n",
      "output_blocks.1.2.attention.layers.0.norm2.weight\n",
      "output_blocks.1.2.attention.layers.0.norm2.bias\n",
      "output_blocks.1.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.1.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.1.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.1.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.1.2.attention.layers.1.linear1.weight\n",
      "output_blocks.1.2.attention.layers.1.linear1.bias\n",
      "output_blocks.1.2.attention.layers.1.linear2.weight\n",
      "output_blocks.1.2.attention.layers.1.linear2.bias\n",
      "output_blocks.1.2.attention.layers.1.norm1.weight\n",
      "output_blocks.1.2.attention.layers.1.norm1.bias\n",
      "output_blocks.1.2.attention.layers.1.norm2.weight\n",
      "output_blocks.1.2.attention.layers.1.norm2.bias\n",
      "output_blocks.2.3.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.2.3.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.2.3.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.2.3.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.2.3.attention.layers.0.linear1.weight\n",
      "output_blocks.2.3.attention.layers.0.linear1.bias\n",
      "output_blocks.2.3.attention.layers.0.linear2.weight\n",
      "output_blocks.2.3.attention.layers.0.linear2.bias\n",
      "output_blocks.2.3.attention.layers.0.norm1.weight\n",
      "output_blocks.2.3.attention.layers.0.norm1.bias\n",
      "output_blocks.2.3.attention.layers.0.norm2.weight\n",
      "output_blocks.2.3.attention.layers.0.norm2.bias\n",
      "output_blocks.2.3.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.2.3.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.2.3.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.2.3.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.2.3.attention.layers.1.linear1.weight\n",
      "output_blocks.2.3.attention.layers.1.linear1.bias\n",
      "output_blocks.2.3.attention.layers.1.linear2.weight\n",
      "output_blocks.2.3.attention.layers.1.linear2.bias\n",
      "output_blocks.2.3.attention.layers.1.norm1.weight\n",
      "output_blocks.2.3.attention.layers.1.norm1.bias\n",
      "output_blocks.2.3.attention.layers.1.norm2.weight\n",
      "output_blocks.2.3.attention.layers.1.norm2.bias\n",
      "output_blocks.3.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.3.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.3.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.3.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.3.2.attention.layers.0.linear1.weight\n",
      "output_blocks.3.2.attention.layers.0.linear1.bias\n",
      "output_blocks.3.2.attention.layers.0.linear2.weight\n",
      "output_blocks.3.2.attention.layers.0.linear2.bias\n",
      "output_blocks.3.2.attention.layers.0.norm1.weight\n",
      "output_blocks.3.2.attention.layers.0.norm1.bias\n",
      "output_blocks.3.2.attention.layers.0.norm2.weight\n",
      "output_blocks.3.2.attention.layers.0.norm2.bias\n",
      "output_blocks.3.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.3.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.3.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.3.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.3.2.attention.layers.1.linear1.weight\n",
      "output_blocks.3.2.attention.layers.1.linear1.bias\n",
      "output_blocks.3.2.attention.layers.1.linear2.weight\n",
      "output_blocks.3.2.attention.layers.1.linear2.bias\n",
      "output_blocks.3.2.attention.layers.1.norm1.weight\n",
      "output_blocks.3.2.attention.layers.1.norm1.bias\n",
      "output_blocks.3.2.attention.layers.1.norm2.weight\n",
      "output_blocks.3.2.attention.layers.1.norm2.bias\n",
      "output_blocks.4.2.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.4.2.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.4.2.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.4.2.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.4.2.attention.layers.0.linear1.weight\n",
      "output_blocks.4.2.attention.layers.0.linear1.bias\n",
      "output_blocks.4.2.attention.layers.0.linear2.weight\n",
      "output_blocks.4.2.attention.layers.0.linear2.bias\n",
      "output_blocks.4.2.attention.layers.0.norm1.weight\n",
      "output_blocks.4.2.attention.layers.0.norm1.bias\n",
      "output_blocks.4.2.attention.layers.0.norm2.weight\n",
      "output_blocks.4.2.attention.layers.0.norm2.bias\n",
      "output_blocks.4.2.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.4.2.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.4.2.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.4.2.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.4.2.attention.layers.1.linear1.weight\n",
      "output_blocks.4.2.attention.layers.1.linear1.bias\n",
      "output_blocks.4.2.attention.layers.1.linear2.weight\n",
      "output_blocks.4.2.attention.layers.1.linear2.bias\n",
      "output_blocks.4.2.attention.layers.1.norm1.weight\n",
      "output_blocks.4.2.attention.layers.1.norm1.bias\n",
      "output_blocks.4.2.attention.layers.1.norm2.weight\n",
      "output_blocks.4.2.attention.layers.1.norm2.bias\n",
      "output_blocks.5.3.attention.layers.0.self_attn.in_proj_weight\n",
      "output_blocks.5.3.attention.layers.0.self_attn.in_proj_bias\n",
      "output_blocks.5.3.attention.layers.0.self_attn.out_proj.weight\n",
      "output_blocks.5.3.attention.layers.0.self_attn.out_proj.bias\n",
      "output_blocks.5.3.attention.layers.0.linear1.weight\n",
      "output_blocks.5.3.attention.layers.0.linear1.bias\n",
      "output_blocks.5.3.attention.layers.0.linear2.weight\n",
      "output_blocks.5.3.attention.layers.0.linear2.bias\n",
      "output_blocks.5.3.attention.layers.0.norm1.weight\n",
      "output_blocks.5.3.attention.layers.0.norm1.bias\n",
      "output_blocks.5.3.attention.layers.0.norm2.weight\n",
      "output_blocks.5.3.attention.layers.0.norm2.bias\n",
      "output_blocks.5.3.attention.layers.1.self_attn.in_proj_weight\n",
      "output_blocks.5.3.attention.layers.1.self_attn.in_proj_bias\n",
      "output_blocks.5.3.attention.layers.1.self_attn.out_proj.weight\n",
      "output_blocks.5.3.attention.layers.1.self_attn.out_proj.bias\n",
      "output_blocks.5.3.attention.layers.1.linear1.weight\n",
      "output_blocks.5.3.attention.layers.1.linear1.bias\n",
      "output_blocks.5.3.attention.layers.1.linear2.weight\n",
      "output_blocks.5.3.attention.layers.1.linear2.bias\n",
      "output_blocks.5.3.attention.layers.1.norm1.weight\n",
      "output_blocks.5.3.attention.layers.1.norm1.bias\n",
      "output_blocks.5.3.attention.layers.1.norm2.weight\n",
      "output_blocks.5.3.attention.layers.1.norm2.bias\n",
      "---------------polyffusion weights loaded with the above missing keys-------------------\n",
      "It is expected that missing keys are all intertrack attention modules\n"
     ]
    }
   ],
   "source": [
    "\n",
    "multipoly_unet.load_polyffusion_checkpoints(polyffusion_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-4.2153e-01,  2.2796e+00,  8.9296e-01,  ...,  1.6508e+00,\n",
      "            1.0649e+00,  7.7144e-02],\n",
      "          [-1.2095e+00,  2.2932e+00,  3.1827e+00,  ..., -7.6543e-01,\n",
      "           -2.3157e+00, -5.4130e-02],\n",
      "          [ 3.3175e-01, -2.8370e+00, -2.8006e-01,  ..., -1.3234e+00,\n",
      "            1.4200e+00,  1.0681e+00],\n",
      "          ...,\n",
      "          [-2.2961e+00,  1.3349e+00,  1.2207e+00,  ..., -4.4104e-01,\n",
      "           -2.3516e+00,  8.0565e-01],\n",
      "          [ 8.2355e-02, -5.0008e-01,  3.7398e-01,  ..., -1.5104e+00,\n",
      "            2.0714e+00,  1.8730e+00],\n",
      "          [ 5.5025e-01,  2.6173e-02,  6.0537e-01,  ..., -1.1773e+00,\n",
      "           -5.3282e-01,  8.9610e-01]],\n",
      "\n",
      "         [[-3.1474e-01, -2.5832e+00,  9.3191e-01,  ...,  1.3512e-01,\n",
      "           -1.4860e+00, -5.6623e-01],\n",
      "          [-1.1148e+00,  1.5074e+00,  4.2294e-01,  ...,  2.1198e-01,\n",
      "            2.6903e-01, -1.3249e+00],\n",
      "          [ 6.1299e-01, -2.0538e+00, -3.5259e-03,  ...,  2.6435e+00,\n",
      "            2.3889e+00, -2.4689e-01],\n",
      "          ...,\n",
      "          [-7.8349e-01, -1.4713e+00, -2.1782e-01,  ...,  1.4823e+00,\n",
      "           -3.0286e-01,  1.8678e+00],\n",
      "          [ 1.0544e+00, -2.3216e+00, -2.1782e+00,  ...,  2.3096e+00,\n",
      "           -1.0683e+00, -2.3270e+00],\n",
      "          [-5.1973e-01,  7.9948e-01,  4.5371e-01,  ...,  2.3276e+00,\n",
      "            8.7067e-01, -8.7801e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 6.6532e-01, -6.7563e-01,  3.4914e-02,  ...,  2.0070e+00,\n",
      "            8.7474e-01,  3.9407e-01],\n",
      "          [-1.2792e-02, -2.4063e+00, -9.1093e-03,  ...,  2.7024e-01,\n",
      "           -1.0909e+00,  2.0243e-01],\n",
      "          [ 2.5713e+00,  6.6365e-01,  1.0200e+00,  ...,  9.9461e-01,\n",
      "            1.4915e+00, -4.0353e-01],\n",
      "          ...,\n",
      "          [-1.0036e-01, -8.3106e-01, -1.0109e+00,  ...,  2.2309e+00,\n",
      "           -2.8278e+00, -1.3320e+00],\n",
      "          [-1.9089e-02,  1.9340e+00, -4.5467e-01,  ...,  2.3310e+00,\n",
      "           -5.1094e-01, -1.4375e+00],\n",
      "          [ 6.6190e-01, -1.9041e+00,  2.3391e+00,  ..., -1.5213e+00,\n",
      "            1.0363e+00,  1.4342e-01]],\n",
      "\n",
      "         [[ 7.2457e-01, -1.7441e+00, -3.5750e-01,  ..., -2.9265e-01,\n",
      "           -1.9639e+00,  1.4585e+00],\n",
      "          [-7.9997e-02, -6.1600e-01,  1.0898e+00,  ...,  6.6297e-01,\n",
      "           -1.1204e+00,  1.5432e+00],\n",
      "          [ 1.6876e-01, -8.1672e-01, -4.1577e-01,  ..., -8.1909e-01,\n",
      "           -7.9514e-01,  9.2817e-01],\n",
      "          ...,\n",
      "          [ 1.1710e+00,  1.5536e+00,  1.9047e+00,  ..., -1.5658e+00,\n",
      "            1.1350e+00, -1.1588e+00],\n",
      "          [-4.5732e-01, -5.0128e-01,  4.4865e-01,  ...,  3.1237e-01,\n",
      "           -2.4265e+00,  1.2704e+00],\n",
      "          [-4.6133e-01, -3.4090e-01, -2.5988e+00,  ..., -1.4377e+00,\n",
      "            1.5442e+00,  2.9059e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.8629e-01, -3.6404e-01, -1.4542e-02,  ...,  2.0103e+00,\n",
      "            1.4834e+00,  7.5092e-01],\n",
      "          [ 6.8322e-01, -2.0050e+00, -1.5408e+00,  ..., -2.6196e+00,\n",
      "           -9.4531e-01,  2.1471e+00],\n",
      "          [ 1.8362e+00,  1.3835e-01,  6.8327e-01,  ..., -4.0760e-01,\n",
      "            2.0491e+00,  1.5257e+00],\n",
      "          ...,\n",
      "          [ 9.8596e-02, -6.3507e-01,  1.9847e+00,  ..., -1.3782e+00,\n",
      "           -6.3155e-01,  1.9917e+00],\n",
      "          [-4.9829e-01,  1.3275e+00,  1.8293e+00,  ...,  2.2463e+00,\n",
      "            1.3611e+00,  8.5874e-03],\n",
      "          [-1.3457e+00, -2.2195e+00, -6.5423e-01,  ..., -1.6594e-01,\n",
      "           -1.7990e+00, -1.0387e+00]],\n",
      "\n",
      "         [[ 9.5748e-01, -1.3605e+00,  1.1551e+00,  ..., -9.6744e-01,\n",
      "           -5.4597e-01,  8.2417e-01],\n",
      "          [-6.1605e-01, -1.6014e+00, -1.3979e+00,  ...,  1.0967e+00,\n",
      "           -6.9959e-01, -7.5096e-01],\n",
      "          [ 3.0501e-01,  9.2882e-01, -8.2838e-02,  ..., -1.4583e-01,\n",
      "           -6.0547e-01,  1.1193e+00],\n",
      "          ...,\n",
      "          [-9.2240e-01, -1.5947e+00,  9.4295e-01,  ..., -1.3848e+00,\n",
      "            1.3875e+00, -2.7350e-02],\n",
      "          [ 8.1638e-01, -3.1448e+00, -8.0527e-01,  ...,  1.6207e+00,\n",
      "            2.3565e+00, -1.4831e+00],\n",
      "          [ 2.7233e-01,  7.5108e-01,  5.6980e-01,  ..., -9.4370e-01,\n",
      "           -1.0831e-01, -8.1984e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.4106e+00,  1.8709e-01, -5.3481e-01,  ...,  5.7468e-01,\n",
      "           -4.7846e-01, -8.6454e-01],\n",
      "          [ 6.6917e-01, -1.8665e+00,  5.0995e-01,  ...,  4.8518e-01,\n",
      "           -1.9284e+00,  2.6000e-02],\n",
      "          [-3.2729e-01,  1.7561e+00, -1.6997e+00,  ...,  2.9663e+00,\n",
      "           -1.7800e+00,  1.2896e+00],\n",
      "          ...,\n",
      "          [-9.5328e-01,  2.4847e+00, -1.6791e+00,  ..., -1.8241e+00,\n",
      "           -1.0208e+00,  8.0405e-01],\n",
      "          [-6.1729e-01,  1.3523e+00, -5.9486e-01,  ..., -1.6793e+00,\n",
      "            7.5778e-01,  9.0986e-01],\n",
      "          [-6.2441e-01, -1.9963e-01, -6.3566e-02,  ...,  6.7997e-01,\n",
      "           -8.9120e-01, -1.7871e-01]],\n",
      "\n",
      "         [[ 1.3875e-02,  1.8729e+00,  4.3724e-01,  ...,  2.0513e+00,\n",
      "            3.2008e-01, -8.7242e-01],\n",
      "          [-7.6745e-01,  1.3118e+00, -1.0757e+00,  ...,  1.3771e+00,\n",
      "            5.8037e-01, -1.6136e+00],\n",
      "          [-9.2149e-01,  8.2203e-01, -2.0429e+00,  ...,  1.5823e+00,\n",
      "           -6.8246e-01,  1.3350e+00],\n",
      "          ...,\n",
      "          [-8.8264e-01, -1.7617e+00,  1.1691e+00,  ...,  3.6705e-01,\n",
      "           -9.1727e-01,  1.4409e+00],\n",
      "          [-1.6137e-01, -8.6129e-01, -6.9174e-01,  ..., -2.9195e-01,\n",
      "           -2.5190e+00, -1.0519e+00],\n",
      "          [-4.0317e-02, -3.5691e-01, -6.6409e-01,  ...,  1.9784e+00,\n",
      "           -1.2367e+00,  8.8458e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.1206e-01, -1.7130e-01, -5.9969e-01,  ...,  4.8146e-01,\n",
      "           -1.0053e+00,  4.3825e-01],\n",
      "          [-9.0747e-01,  4.2086e-01, -9.7313e-01,  ..., -8.4188e-01,\n",
      "            2.5163e+00,  1.0550e-01],\n",
      "          [ 4.9164e-01,  1.2895e+00, -1.3710e+00,  ...,  1.9149e+00,\n",
      "           -2.7242e+00, -1.2753e+00],\n",
      "          ...,\n",
      "          [ 6.9434e-01, -2.2436e+00, -7.3325e-01,  ..., -6.1087e-01,\n",
      "           -1.1566e+00,  2.1172e+00],\n",
      "          [ 1.0752e+00,  3.5464e-01,  2.9422e-01,  ...,  3.8752e-02,\n",
      "           -1.6442e+00, -2.0040e+00],\n",
      "          [ 4.7597e-01, -3.2494e+00,  2.1524e+00,  ..., -1.1157e+00,\n",
      "           -6.6461e-01,  9.4764e-03]],\n",
      "\n",
      "         [[-5.0718e-01, -1.2395e+00, -9.7388e-01,  ...,  7.4810e-01,\n",
      "           -1.1505e+00, -4.1061e-01],\n",
      "          [-8.5548e-01,  2.0476e-01,  1.3548e-01,  ...,  4.0606e-01,\n",
      "           -3.3954e-01, -2.1172e+00],\n",
      "          [ 7.7779e-02,  3.5758e-01,  2.9704e+00,  ...,  2.8849e+00,\n",
      "           -1.4023e+00, -3.9661e-01],\n",
      "          ...,\n",
      "          [-1.2765e+00,  3.8669e-01,  6.9674e-01,  ...,  2.3502e-01,\n",
      "           -1.6229e+00,  6.5597e-03],\n",
      "          [-2.1261e+00, -8.4606e-01, -2.2120e+00,  ...,  6.7842e-01,\n",
      "            4.9576e-01,  7.4101e-01],\n",
      "          [-4.9743e-02, -1.7091e+00, -4.2058e-01,  ..., -1.1583e+00,\n",
      "            1.0734e+00, -6.0733e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0039e-01, -5.1796e-01, -1.8647e+00,  ..., -4.8061e-01,\n",
      "            1.0431e+00,  7.1642e-01],\n",
      "          [ 4.0398e-01,  2.1576e+00,  1.6262e+00,  ..., -1.5319e+00,\n",
      "            1.5505e+00,  1.6206e+00],\n",
      "          [-5.6344e-01,  2.3572e+00, -2.5357e+00,  ...,  1.9022e+00,\n",
      "           -1.9750e+00, -9.6439e-01],\n",
      "          ...,\n",
      "          [-2.9027e+00,  1.1865e-01, -3.6326e-01,  ...,  3.8471e+00,\n",
      "           -1.2015e+00,  2.3020e+00],\n",
      "          [-1.5640e+00, -1.2028e+00,  1.6386e-01,  ..., -2.3441e+00,\n",
      "            1.1794e+00, -6.3272e-01],\n",
      "          [-2.9949e-01,  1.8390e+00, -8.8321e-01,  ..., -6.5018e-02,\n",
      "           -8.5761e-01,  1.3524e+00]],\n",
      "\n",
      "         [[-1.2030e+00, -3.2756e-01, -1.2502e+00,  ...,  2.6886e-01,\n",
      "           -1.3406e+00,  1.2882e-02],\n",
      "          [-1.2019e+00,  6.3372e-01,  9.2388e-01,  ..., -7.8868e-01,\n",
      "            2.0550e+00, -1.2939e+00],\n",
      "          [-1.3293e+00,  5.3898e-01,  5.6174e-01,  ..., -1.8938e+00,\n",
      "            6.1487e-01, -4.0592e-01],\n",
      "          ...,\n",
      "          [ 3.3003e-01, -5.2033e-01,  2.1741e+00,  ...,  3.2296e-01,\n",
      "            1.8029e+00,  2.4259e+00],\n",
      "          [ 1.8093e-01,  8.8285e-01, -1.0113e+00,  ..., -1.4636e+00,\n",
      "           -6.4887e-02,  5.6515e-01],\n",
      "          [-5.7765e-01, -5.7547e-01, -8.6752e-01,  ...,  1.3921e+00,\n",
      "           -1.1428e+00,  8.0906e-01]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "track_num = 5\n",
    "image_width = 128\n",
    "image_height = 128\n",
    "channels = 2\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "input_tensors = torch.randn(batch_size, track_num, channels, image_width, image_height).to(device)\n",
    "cond = torch.randn(batch_size*track_num, 1, 512).to(device)\n",
    "t = torch.randn(batch_size*track_num, ).to(device)\n",
    "\n",
    "polyffusion_unet = polyffusion_unet.to(device)\n",
    "polyffusion_unet.eval()\n",
    "with torch.no_grad():\n",
    "    poly_output = polyffusion_unet(input_tensors.reshape(batch_size*track_num, channels, image_width, image_height),t,  cond)\n",
    "print(poly_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[-4.2145e-01,  2.2796e+00,  8.9285e-01,  ...,  1.6507e+00,\n",
      "             1.0647e+00,  7.7178e-02],\n",
      "           [-1.2095e+00,  2.2932e+00,  3.1828e+00,  ..., -7.6538e-01,\n",
      "            -2.3158e+00, -5.4117e-02],\n",
      "           [ 3.3190e-01, -2.8370e+00, -2.8012e-01,  ..., -1.3235e+00,\n",
      "             1.4201e+00,  1.0681e+00],\n",
      "           ...,\n",
      "           [-2.2961e+00,  1.3349e+00,  1.2207e+00,  ..., -4.4104e-01,\n",
      "            -2.3515e+00,  8.0562e-01],\n",
      "           [ 8.2380e-02, -5.0005e-01,  3.7395e-01,  ..., -1.5103e+00,\n",
      "             2.0713e+00,  1.8730e+00],\n",
      "           [ 5.5017e-01,  2.6166e-02,  6.0540e-01,  ..., -1.1774e+00,\n",
      "            -5.3280e-01,  8.9613e-01]],\n",
      "\n",
      "          [[-3.1481e-01, -2.5831e+00,  9.3184e-01,  ...,  1.3507e-01,\n",
      "            -1.4860e+00, -5.6623e-01],\n",
      "           [-1.1148e+00,  1.5074e+00,  4.2303e-01,  ...,  2.1185e-01,\n",
      "             2.6898e-01, -1.3248e+00],\n",
      "           [ 6.1301e-01, -2.0538e+00, -3.5293e-03,  ...,  2.6434e+00,\n",
      "             2.3890e+00, -2.4687e-01],\n",
      "           ...,\n",
      "           [-7.8349e-01, -1.4713e+00, -2.1779e-01,  ...,  1.4824e+00,\n",
      "            -3.0284e-01,  1.8678e+00],\n",
      "           [ 1.0544e+00, -2.3216e+00, -2.1781e+00,  ...,  2.3096e+00,\n",
      "            -1.0684e+00, -2.3270e+00],\n",
      "           [-5.1961e-01,  7.9952e-01,  4.5372e-01,  ...,  2.3277e+00,\n",
      "             8.7066e-01, -8.7808e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.6535e-01, -6.7561e-01,  3.4894e-02,  ...,  2.0070e+00,\n",
      "             8.7475e-01,  3.9410e-01],\n",
      "           [-1.2794e-02, -2.4064e+00, -9.1526e-03,  ...,  2.7023e-01,\n",
      "            -1.0909e+00,  2.0243e-01],\n",
      "           [ 2.5713e+00,  6.6366e-01,  1.0198e+00,  ...,  9.9460e-01,\n",
      "             1.4917e+00, -4.0353e-01],\n",
      "           ...,\n",
      "           [-1.0032e-01, -8.3115e-01, -1.0108e+00,  ...,  2.2306e+00,\n",
      "            -2.8280e+00, -1.3320e+00],\n",
      "           [-1.9095e-02,  1.9337e+00, -4.5468e-01,  ...,  2.3309e+00,\n",
      "            -5.1102e-01, -1.4375e+00],\n",
      "           [ 6.6197e-01, -1.9041e+00,  2.3392e+00,  ..., -1.5214e+00,\n",
      "             1.0364e+00,  1.4339e-01]],\n",
      "\n",
      "          [[ 7.2462e-01, -1.7441e+00, -3.5754e-01,  ..., -2.9260e-01,\n",
      "            -1.9639e+00,  1.4586e+00],\n",
      "           [-8.0000e-02, -6.1605e-01,  1.0898e+00,  ...,  6.6295e-01,\n",
      "            -1.1204e+00,  1.5432e+00],\n",
      "           [ 1.6868e-01, -8.1671e-01, -4.1577e-01,  ..., -8.1909e-01,\n",
      "            -7.9520e-01,  9.2819e-01],\n",
      "           ...,\n",
      "           [ 1.1711e+00,  1.5537e+00,  1.9045e+00,  ..., -1.5659e+00,\n",
      "             1.1351e+00, -1.1588e+00],\n",
      "           [-4.5733e-01, -5.0118e-01,  4.4865e-01,  ...,  3.1248e-01,\n",
      "            -2.4265e+00,  1.2704e+00],\n",
      "           [-4.6137e-01, -3.4097e-01, -2.5988e+00,  ..., -1.4379e+00,\n",
      "             1.5443e+00,  2.9057e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.8624e-01, -3.6402e-01, -1.4526e-02,  ...,  2.0102e+00,\n",
      "             1.4835e+00,  7.5096e-01],\n",
      "           [ 6.8323e-01, -2.0049e+00, -1.5409e+00,  ..., -2.6196e+00,\n",
      "            -9.4527e-01,  2.1472e+00],\n",
      "           [ 1.8362e+00,  1.3836e-01,  6.8327e-01,  ..., -4.0757e-01,\n",
      "             2.0491e+00,  1.5256e+00],\n",
      "           ...,\n",
      "           [ 9.8615e-02, -6.3510e-01,  1.9847e+00,  ..., -1.3782e+00,\n",
      "            -6.3154e-01,  1.9917e+00],\n",
      "           [-4.9829e-01,  1.3275e+00,  1.8292e+00,  ...,  2.2463e+00,\n",
      "             1.3612e+00,  8.5715e-03],\n",
      "           [-1.3457e+00, -2.2195e+00, -6.5425e-01,  ..., -1.6595e-01,\n",
      "            -1.7991e+00, -1.0387e+00]],\n",
      "\n",
      "          [[ 9.5754e-01, -1.3604e+00,  1.1550e+00,  ..., -9.6743e-01,\n",
      "            -5.4603e-01,  8.2419e-01],\n",
      "           [-6.1606e-01, -1.6014e+00, -1.3979e+00,  ...,  1.0967e+00,\n",
      "            -6.9957e-01, -7.5101e-01],\n",
      "           [ 3.0507e-01,  9.2886e-01, -8.2766e-02,  ..., -1.4584e-01,\n",
      "            -6.0547e-01,  1.1193e+00],\n",
      "           ...,\n",
      "           [-9.2241e-01, -1.5948e+00,  9.4292e-01,  ..., -1.3848e+00,\n",
      "             1.3876e+00, -2.7337e-02],\n",
      "           [ 8.1637e-01, -3.1447e+00, -8.0516e-01,  ...,  1.6207e+00,\n",
      "             2.3566e+00, -1.4831e+00],\n",
      "           [ 2.7229e-01,  7.5111e-01,  5.6981e-01,  ..., -9.4368e-01,\n",
      "            -1.0830e-01, -8.1980e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 7.9042e-01, -3.8363e-02,  1.1057e+00,  ...,  1.1363e+00,\n",
      "            -1.9386e-01, -3.4961e-01],\n",
      "           [-1.1482e+00, -3.5606e+00,  2.7184e+00,  ..., -2.6735e+00,\n",
      "            -2.2727e-01,  8.0447e-01],\n",
      "           [-9.8601e-01, -1.8036e+00,  2.4799e-01,  ...,  1.0628e+00,\n",
      "             1.2405e+00, -5.6075e-01],\n",
      "           ...,\n",
      "           [ 1.8379e+00,  6.8995e-01, -1.3824e+00,  ..., -1.5929e+00,\n",
      "             1.0844e+00,  7.8113e-01],\n",
      "           [-1.7796e+00,  9.6539e-01,  8.7679e-01,  ...,  4.2817e-01,\n",
      "             1.8675e+00, -4.7974e-01],\n",
      "           [ 1.8432e+00,  2.0852e-01,  7.3806e-01,  ...,  1.5428e-01,\n",
      "            -2.1076e+00,  1.5532e+00]],\n",
      "\n",
      "          [[-1.3659e+00,  4.0004e-01,  1.8023e-01,  ..., -2.0515e-01,\n",
      "            -2.4926e-01,  5.5215e-01],\n",
      "           [-8.7500e-01,  1.1583e+00,  1.7001e+00,  ..., -3.8968e-01,\n",
      "             7.5889e-02, -1.9894e+00],\n",
      "           [ 2.7959e-01,  6.7474e-01,  6.6105e-03,  ..., -2.5334e-02,\n",
      "             3.6671e-01,  2.5220e+00],\n",
      "           ...,\n",
      "           [ 5.5339e-01, -5.8352e-01,  7.8268e-02,  ..., -2.5417e-01,\n",
      "             3.1856e+00,  1.4605e+00],\n",
      "           [-6.9203e-01, -1.0560e+00,  2.1507e+00,  ..., -7.0086e-01,\n",
      "             1.1692e+00,  9.3927e-01],\n",
      "           [ 1.6533e-01, -8.4199e-01, -3.5772e+00,  ...,  3.3263e-01,\n",
      "            -3.2499e-02, -1.4050e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 2.4872e-01, -1.3800e-02, -6.8495e-01,  ...,  1.0645e+00,\n",
      "             1.7350e+00, -9.6086e-02],\n",
      "           [-2.2658e-01, -7.5967e-02,  1.5833e+00,  ..., -1.1337e-01,\n",
      "            -9.0022e-01, -2.6478e+00],\n",
      "           [-3.0685e-01,  1.9149e+00, -2.0473e+00,  ..., -2.8951e+00,\n",
      "            -2.5444e+00,  4.3891e-01],\n",
      "           ...,\n",
      "           [ 8.0421e-01, -2.7458e-01,  4.8443e-01,  ...,  1.9802e-01,\n",
      "            -7.5655e-01,  8.5887e-01],\n",
      "           [-7.3154e-01,  2.2088e+00, -2.7679e+00,  ...,  4.6573e-02,\n",
      "            -5.4423e-01, -4.3331e-01],\n",
      "           [ 4.8871e-01, -1.1529e+00, -1.1475e+00,  ..., -5.0817e-01,\n",
      "            -3.1455e-01,  5.2203e-01]],\n",
      "\n",
      "          [[-5.5011e-01, -5.5920e-01,  7.1890e-01,  ...,  1.4042e+00,\n",
      "             1.8054e-01,  5.5268e-01],\n",
      "           [-7.6345e-01,  1.1738e+00, -8.9329e-01,  ..., -1.4368e+00,\n",
      "             2.1959e+00, -1.2349e+00],\n",
      "           [ 2.1700e+00,  3.0633e+00,  1.8596e+00,  ..., -3.0562e+00,\n",
      "             3.2827e-01,  1.4484e+00],\n",
      "           ...,\n",
      "           [-1.4100e-01, -6.6833e-01,  1.8621e-01,  ..., -1.8126e+00,\n",
      "             1.1127e+00, -2.1665e+00],\n",
      "           [-4.5626e-02, -8.3713e-01, -8.9454e-01,  ..., -2.0516e-01,\n",
      "             5.2909e-01, -1.9012e+00],\n",
      "           [ 1.8888e-01,  1.5463e+00,  3.4352e-01,  ...,  1.8555e-01,\n",
      "             2.7412e-01, -6.3007e-01]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-2.0135e+00,  7.8703e-01, -2.2907e+00,  ..., -1.2053e+00,\n",
      "            -1.7365e-01, -1.5181e+00],\n",
      "           [-3.0306e-02, -3.2981e-01,  1.1306e+00,  ..., -6.1315e-01,\n",
      "             1.7880e+00,  7.0892e-01],\n",
      "           [-1.7434e+00, -5.2195e-01,  1.0552e+00,  ..., -2.0250e-01,\n",
      "             2.7481e+00, -5.7523e-01],\n",
      "           ...,\n",
      "           [-2.7576e-01,  9.4877e-01, -3.6947e+00,  ..., -2.8225e+00,\n",
      "             1.1414e+00,  1.1849e+00],\n",
      "           [-2.7132e-01, -1.2120e-01, -3.8354e-02,  ...,  6.7288e-02,\n",
      "            -7.6250e-01, -6.1289e-01],\n",
      "           [ 6.9788e-01,  6.1829e-01,  1.4510e+00,  ..., -1.3623e+00,\n",
      "            -5.9499e-01,  4.3081e-01]],\n",
      "\n",
      "          [[-4.0849e-01,  1.0869e+00, -8.3403e-01,  ..., -2.1099e+00,\n",
      "             1.6797e+00, -6.0189e-01],\n",
      "           [-1.0949e+00,  4.1272e+00, -2.1991e+00,  ..., -9.5402e-01,\n",
      "             1.0841e+00,  1.1935e+00],\n",
      "           [ 9.7583e-02,  9.3110e-01, -2.3828e+00,  ..., -2.9252e+00,\n",
      "            -4.2233e-01, -1.6434e+00],\n",
      "           ...,\n",
      "           [ 1.4520e-01, -6.2370e-01, -3.0420e-01,  ..., -4.9613e-01,\n",
      "            -2.0519e-01,  6.7182e-01],\n",
      "           [ 1.0189e+00,  1.4065e+00, -6.8404e-01,  ..., -9.7989e-01,\n",
      "             6.5487e-01, -1.9980e+00],\n",
      "           [ 9.7306e-02,  1.3287e+00,  4.0682e-01,  ..., -2.4999e+00,\n",
      "             4.2291e-02, -5.1827e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 1.2247e+00,  7.4716e-01,  5.5395e-01,  ..., -7.1345e-01,\n",
      "            -1.3753e+00, -1.1840e+00],\n",
      "           [-2.6944e+00,  1.5912e+00, -1.1756e+00,  ...,  2.2846e+00,\n",
      "            -1.3632e+00, -7.7170e-02],\n",
      "           [ 4.6927e-01, -1.2540e-01, -4.3416e-02,  ...,  1.6997e+00,\n",
      "            -2.3911e+00,  2.1130e+00],\n",
      "           ...,\n",
      "           [-9.2726e-01, -2.5909e+00,  2.6825e+00,  ..., -8.5156e-01,\n",
      "             5.8612e-01,  1.5784e+00],\n",
      "           [ 4.4830e-01,  7.2907e-01, -1.0234e+00,  ...,  7.2531e-01,\n",
      "             3.0015e-01,  5.0649e-01],\n",
      "           [ 7.1751e-01, -8.4384e-01, -6.8565e-01,  ...,  1.3831e-01,\n",
      "             8.6776e-01, -8.6774e-01]],\n",
      "\n",
      "          [[ 2.1613e-01, -2.8465e-01, -1.8650e+00,  ..., -1.4273e+00,\n",
      "             5.2722e-01,  6.6582e-01],\n",
      "           [ 5.3727e-01, -9.4197e-01,  7.5287e-02,  ..., -2.0383e+00,\n",
      "            -1.6069e+00,  7.8648e-02],\n",
      "           [-1.2522e+00,  9.0400e-01,  1.2393e+00,  ..., -1.4176e+00,\n",
      "             5.9404e-01, -1.1994e+00],\n",
      "           ...,\n",
      "           [-2.6293e-01, -4.0941e-01, -1.2012e+00,  ..., -1.6281e+00,\n",
      "            -9.0636e-01, -3.7404e-01],\n",
      "           [-1.1207e+00,  2.5025e+00,  1.3825e+00,  ...,  1.5516e-01,\n",
      "             9.0100e-01, -2.0623e-01],\n",
      "           [-6.1003e-01, -1.9210e-01,  3.4749e-01,  ...,  2.8626e-01,\n",
      "             1.1400e-01, -6.4670e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.4106e+00,  1.8710e-01, -5.3486e-01,  ...,  5.7470e-01,\n",
      "            -4.7847e-01, -8.6454e-01],\n",
      "           [ 6.6911e-01, -1.8665e+00,  5.0992e-01,  ...,  4.8528e-01,\n",
      "            -1.9284e+00,  2.5946e-02],\n",
      "           [-3.2731e-01,  1.7560e+00, -1.6996e+00,  ...,  2.9662e+00,\n",
      "            -1.7800e+00,  1.2896e+00],\n",
      "           ...,\n",
      "           [-9.5324e-01,  2.4846e+00, -1.6791e+00,  ..., -1.8240e+00,\n",
      "            -1.0208e+00,  8.0402e-01],\n",
      "           [-6.1740e-01,  1.3523e+00, -5.9484e-01,  ..., -1.6794e+00,\n",
      "             7.5785e-01,  9.0984e-01],\n",
      "           [-6.2436e-01, -1.9964e-01, -6.3606e-02,  ...,  6.7999e-01,\n",
      "            -8.9119e-01, -1.7874e-01]],\n",
      "\n",
      "          [[ 1.3910e-02,  1.8730e+00,  4.3722e-01,  ...,  2.0513e+00,\n",
      "             3.2004e-01, -8.7242e-01],\n",
      "           [-7.6745e-01,  1.3118e+00, -1.0757e+00,  ...,  1.3770e+00,\n",
      "             5.8038e-01, -1.6136e+00],\n",
      "           [-9.2147e-01,  8.2196e-01, -2.0429e+00,  ...,  1.5822e+00,\n",
      "            -6.8248e-01,  1.3351e+00],\n",
      "           ...,\n",
      "           [-8.8261e-01, -1.7616e+00,  1.1691e+00,  ...,  3.6704e-01,\n",
      "            -9.1724e-01,  1.4408e+00],\n",
      "           [-1.6139e-01, -8.6132e-01, -6.9172e-01,  ..., -2.9184e-01,\n",
      "            -2.5190e+00, -1.0518e+00],\n",
      "           [-4.0255e-02, -3.5699e-01, -6.6400e-01,  ...,  1.9785e+00,\n",
      "            -1.2366e+00,  8.8453e-01]]],\n",
      "\n",
      "\n",
      "         [[[-4.1206e-01, -1.7125e-01, -5.9971e-01,  ...,  4.8140e-01,\n",
      "            -1.0053e+00,  4.3820e-01],\n",
      "           [-9.0748e-01,  4.2084e-01, -9.7317e-01,  ..., -8.4194e-01,\n",
      "             2.5160e+00,  1.0554e-01],\n",
      "           [ 4.9166e-01,  1.2895e+00, -1.3710e+00,  ...,  1.9148e+00,\n",
      "            -2.7244e+00, -1.2753e+00],\n",
      "           ...,\n",
      "           [ 6.9431e-01, -2.2436e+00, -7.3330e-01,  ..., -6.1096e-01,\n",
      "            -1.1565e+00,  2.1171e+00],\n",
      "           [ 1.0752e+00,  3.5462e-01,  2.9416e-01,  ...,  3.8757e-02,\n",
      "            -1.6442e+00, -2.0039e+00],\n",
      "           [ 4.7598e-01, -3.2495e+00,  2.1523e+00,  ..., -1.1158e+00,\n",
      "            -6.6461e-01,  9.4013e-03]],\n",
      "\n",
      "          [[-5.0724e-01, -1.2394e+00, -9.7384e-01,  ...,  7.4809e-01,\n",
      "            -1.1506e+00, -4.1060e-01],\n",
      "           [-8.5548e-01,  2.0478e-01,  1.3544e-01,  ...,  4.0596e-01,\n",
      "            -3.3954e-01, -2.1172e+00],\n",
      "           [ 7.7746e-02,  3.5764e-01,  2.9703e+00,  ...,  2.8846e+00,\n",
      "            -1.4023e+00, -3.9656e-01],\n",
      "           ...,\n",
      "           [-1.2766e+00,  3.8686e-01,  6.9673e-01,  ...,  2.3502e-01,\n",
      "            -1.6230e+00,  6.5201e-03],\n",
      "           [-2.1260e+00, -8.4603e-01, -2.2119e+00,  ...,  6.7839e-01,\n",
      "             4.9582e-01,  7.4089e-01],\n",
      "           [-4.9776e-02, -1.7091e+00, -4.2056e-01,  ..., -1.1584e+00,\n",
      "             1.0734e+00, -6.0725e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 5.0038e-01, -5.1799e-01, -1.8647e+00,  ..., -4.8060e-01,\n",
      "             1.0433e+00,  7.1646e-01],\n",
      "           [ 4.0398e-01,  2.1578e+00,  1.6262e+00,  ..., -1.5319e+00,\n",
      "             1.5507e+00,  1.6206e+00],\n",
      "           [-5.6343e-01,  2.3575e+00, -2.5357e+00,  ...,  1.9022e+00,\n",
      "            -1.9749e+00, -9.6436e-01],\n",
      "           ...,\n",
      "           [-2.9027e+00,  1.1875e-01, -3.6332e-01,  ...,  3.8470e+00,\n",
      "            -1.2015e+00,  2.3021e+00],\n",
      "           [-1.5640e+00, -1.2028e+00,  1.6391e-01,  ..., -2.3441e+00,\n",
      "             1.1794e+00, -6.3280e-01],\n",
      "           [-2.9946e-01,  1.8389e+00, -8.8323e-01,  ..., -6.4980e-02,\n",
      "            -8.5765e-01,  1.3524e+00]],\n",
      "\n",
      "          [[-1.2030e+00, -3.2753e-01, -1.2502e+00,  ...,  2.6886e-01,\n",
      "            -1.3406e+00,  1.2874e-02],\n",
      "           [-1.2019e+00,  6.3375e-01,  9.2387e-01,  ..., -7.8867e-01,\n",
      "             2.0552e+00, -1.2940e+00],\n",
      "           [-1.3293e+00,  5.3989e-01,  5.6175e-01,  ..., -1.8937e+00,\n",
      "             6.1473e-01, -4.0587e-01],\n",
      "           ...,\n",
      "           [ 3.3010e-01, -5.2037e-01,  2.1740e+00,  ...,  3.2306e-01,\n",
      "             1.8029e+00,  2.4258e+00],\n",
      "           [ 1.8099e-01,  8.8293e-01, -1.0113e+00,  ..., -1.4635e+00,\n",
      "            -6.4870e-02,  5.6519e-01],\n",
      "           [-5.7766e-01, -5.7546e-01, -8.6751e-01,  ...,  1.3921e+00,\n",
      "            -1.1429e+00,  8.0912e-01]]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "multipoly_unet = multipoly_unet.to(device)\n",
    "multipoly_unet.eval()\n",
    "with torch.no_grad():\n",
    "    multi_output = multipoly_unet(input_tensors, t, cond)\n",
    "print(multi_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8036e-05, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "poly_output = poly_output.flatten()\n",
    "multi_output = multi_output.flatten()\n",
    "avg_diff = (poly_output-multi_output).abs().mean()\n",
    "print(avg_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
